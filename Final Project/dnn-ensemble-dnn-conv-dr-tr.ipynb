{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Model Ensemble\n> Ensemble of baseline work","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport gc\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import initializers\nfrom tensorflow import keras\nimport mlcrate as mlc\nimport pickle as pkl\nfrom tensorflow.keras.layers import BatchNormalization\nfrom keras.models import Sequential, Model\nfrom keras.layers import Input, Embedding, Dense, Flatten, Concatenate, Dot, Reshape, Add, Subtract\nfrom keras import backend as K\nfrom keras import regularizers \nfrom tensorflow.keras.optimizers import Adam\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom keras.regularizers import l2\nfrom sklearn.base import clone\nfrom typing import Dict\nfrom scipy import stats\nfrom tensorflow.keras.losses import Loss\nfrom tensorflow.keras import backend as K\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import TimeSeriesSplit, StratifiedKFold, KFold, GroupKFold\nfrom tqdm import tqdm\nfrom tensorflow.python.ops import math_ops","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-20T22:23:08.704848Z","iopub.execute_input":"2022-04-20T22:23:08.705178Z","iopub.status.idle":"2022-04-20T22:23:14.465898Z","shell.execute_reply.started":"2022-04-20T22:23:08.705106Z","shell.execute_reply":"2022-04-20T22:23:14.46518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1. Data Exploration & Features layers","metadata":{}},{"cell_type":"code","source":"%%time\nn_features = 300\nfeatures = [f'f_{i}' for i in range(n_features)]\nfeature_columns = ['investment_id', 'time_id'] + features\ntrain = pd.read_pickle('../input/ubiquant-market-prediction-half-precision-pickle/train.pkl')\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-20T22:23:14.467514Z","iopub.execute_input":"2022-04-20T22:23:14.467785Z","iopub.status.idle":"2022-04-20T22:23:31.511376Z","shell.execute_reply.started":"2022-04-20T22:23:14.46775Z","shell.execute_reply":"2022-04-20T22:23:31.510671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"investment_id = train.pop(\"investment_id\")\ninvestment_id.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-20T22:23:31.512616Z","iopub.execute_input":"2022-04-20T22:23:31.513049Z","iopub.status.idle":"2022-04-20T22:23:31.53077Z","shell.execute_reply.started":"2022-04-20T22:23:31.513011Z","shell.execute_reply":"2022-04-20T22:23:31.530102Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"_ = train.pop(\"time_id\")\ny = train.pop(\"target\")\ny.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-20T22:23:31.533696Z","iopub.execute_input":"2022-04-20T22:23:31.534139Z","iopub.status.idle":"2022-04-20T22:23:31.554881Z","shell.execute_reply.started":"2022-04-20T22:23:31.5341Z","shell.execute_reply":"2022-04-20T22:23:31.554247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ninvestment_ids = list(investment_id.unique())\ninvestment_id_size = len(investment_ids) + 1\ninvestment_id_lookup_layer = layers.IntegerLookup(max_tokens=investment_id_size)\nwith tf.device(\"cpu\"):\n    investment_id_lookup_layer.adapt(investment_id)","metadata":{"execution":{"iopub.status.busy":"2022-04-20T22:23:31.556082Z","iopub.execute_input":"2022-04-20T22:23:31.556298Z","iopub.status.idle":"2022-04-20T22:24:55.817716Z","shell.execute_reply.started":"2022-04-20T22:23:31.556268Z","shell.execute_reply":"2022-04-20T22:24:55.816848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"investment_id2 = investment_id[~investment_id.isin([85, 905, 2558, 3662, 2800, 1415])]\n\ninvestment_ids2 = list(investment_id2.unique())\ninvestment_id_size2 = len(investment_ids2) + 1\ninvestment_id_lookup_layer2 = layers.IntegerLookup(max_tokens=investment_id_size2)\ninvestment_id_lookup_layer2.adapt(pd.DataFrame({\"investment_ids\":investment_ids}))","metadata":{"execution":{"iopub.status.busy":"2022-04-20T22:24:55.819411Z","iopub.execute_input":"2022-04-20T22:24:55.820173Z","iopub.status.idle":"2022-04-20T22:24:56.070839Z","shell.execute_reply.started":"2022-04-20T22:24:55.820128Z","shell.execute_reply":"2022-04-20T22:24:56.070132Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess(X, y):\n    print(X)\n    print(y)\n    return X, y\ndef make_dataset(feature, investment_id, y, batch_size=1024, mode=\"train\"):\n    ds = tf.data.Dataset.from_tensor_slices(((investment_id, feature), y))\n    ds = ds.map(preprocess)\n    if mode == \"train\":\n        ds = ds.shuffle(256)\n    ds = ds.batch(batch_size).cache().prefetch(tf.data.experimental.AUTOTUNE)\n    return ds","metadata":{"execution":{"iopub.status.busy":"2022-04-20T22:24:56.072375Z","iopub.execute_input":"2022-04-20T22:24:56.072861Z","iopub.status.idle":"2022-04-20T22:24:56.080258Z","shell.execute_reply.started":"2022-04-20T22:24:56.072823Z","shell.execute_reply":"2022-04-20T22:24:56.07941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### feature_time_ds","metadata":{}},{"cell_type":"code","source":"def make_ft_dataset(investment_id, feature, time_id, y=None, batch_size=1024):\n    if y is not None:\n        slices = ((investment_id, feature, time_id), y)\n    else:\n        slices = ((investment_id, feature, time_id))\n        \n    ds = tf.data.Dataset.from_tensor_slices(slices)\n    ds = ds.batch(batch_size).cache().prefetch(tf.data.experimental.AUTOTUNE)\n    return ds","metadata":{"execution":{"iopub.status.busy":"2022-04-20T22:24:56.081237Z","iopub.execute_input":"2022-04-20T22:24:56.08348Z","iopub.status.idle":"2022-04-20T22:24:56.09068Z","shell.execute_reply.started":"2022-04-20T22:24:56.083427Z","shell.execute_reply":"2022-04-20T22:24:56.089927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. DNN Architecture","metadata":{}},{"cell_type":"code","source":"class MyModel(keras.Model):\n    \n    def __init__(self, investment_id, device='gpu'):\n        super().__init__()\n        \n        with tf.device(device):\n            self.inv_embedding = layers.Embedding(investment_id_size, 32)\n            self.inv_fc = keras.Sequential([\n                layers.Dense(64, activation='swish', kernel_initializer='he_normal', bias_initializer='zeros'),\n                layers.Dropout(0.5),\n                layers.Dense(32, activation='swish', kernel_initializer='he_normal', bias_initializer='zeros'),\n                layers.Dropout(0.5),\n            ])\n\n            self.fea_fc = keras.Sequential([\n                layers.Dense(256, activation='swish', kernel_initializer='he_normal', bias_initializer='zeros'),\n                keras.layers.BatchNormalization(axis=1),\n                layers.Dropout(0.5),\n                layers.Dense(128, activation='swish', kernel_initializer='he_normal', bias_initializer='zeros'),\n                keras.layers.BatchNormalization(axis=1),\n                layers.Dropout(0.5),\n                layers.Dense(64, activation='swish', kernel_initializer='he_normal', bias_initializer='zeros')\n            ])\n            \n            self.fc = keras.Sequential([\n                layers.Dropout(0.5),\n                layers.Dense(128, activation='swish', kernel_initializer='he_normal', bias_initializer='zeros', kernel_regularizer=\"l2\"),\n                layers.Dropout(0.5),\n                layers.Dense(32, activation='swish', kernel_initializer='he_normal', bias_initializer='zeros',  kernel_regularizer=\"l2\"),\n                layers.Dropout(0.5),\n                layers.Dense(16, activation='swish', kernel_initializer='he_normal', bias_initializer='zeros', kernel_regularizer=\"l2\"),\n                layers.Dense(1)\n            ])\n    \n    def call(self, inputs):\n        inv_id, fea = inputs\n        \n        inv = investment_id_lookup_layer(inv_id)\n        inv = self.inv_embedding(inv)\n        inv = self.inv_fc(inv)\n        inv = tf.squeeze(inv, axis=1)\n        \n        fea = self.fea_fc(fea)\n        \n        concat = tf.concat([inv, fea], axis=1)\n        output = self.fc(concat)\n        \n        return output","metadata":{"execution":{"iopub.status.busy":"2022-04-20T22:24:56.091923Z","iopub.execute_input":"2022-04-20T22:24:56.092277Z","iopub.status.idle":"2022-04-20T22:24:56.10689Z","shell.execute_reply.started":"2022-04-20T22:24:56.092243Z","shell.execute_reply":"2022-04-20T22:24:56.106106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_model():\n    investment_id_inputs = tf.keras.Input((1, ), dtype=tf.uint16)\n    features_inputs = tf.keras.Input((300, ), dtype=tf.float16)\n    \n    investment_id_x = investment_id_lookup_layer(investment_id_inputs)\n    investment_id_x = layers.Embedding(investment_id_size, 32, input_length=1)(investment_id_x)\n    investment_id_x = layers.Reshape((-1, ))(investment_id_x)\n    investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)\n    investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)\n    investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)\n    \n    feature_x = layers.Dense(256, activation='swish')(features_inputs)\n    feature_x = layers.Dense(256, activation='swish')(feature_x)\n    feature_x = layers.Dense(256, activation='swish')(feature_x)\n    \n    x = layers.Concatenate(axis=1)([investment_id_x, feature_x])\n    x = layers.Dense(512, activation='swish', kernel_regularizer=\"l2\")(x)\n    x = layers.Dense(128, activation='swish', kernel_regularizer=\"l2\")(x)\n    x = layers.Dense(32, activation='swish', kernel_regularizer=\"l2\")(x)\n    output = layers.Dense(1)(x)\n    rmse = keras.metrics.RootMeanSquaredError(name=\"rmse\")\n    model = tf.keras.Model(inputs=[investment_id_inputs, features_inputs], outputs=[output])\n    model.compile(optimizer=tf.optimizers.Adam(0.001), loss='mse', metrics=['mse', \"mae\", \"mape\", rmse])\n    return model\n\n\ndef get_model2():\n    investment_id_inputs = tf.keras.Input((1, ), dtype=tf.uint16)\n    features_inputs = tf.keras.Input((300, ), dtype=tf.float16)\n    \n    investment_id_x = investment_id_lookup_layer(investment_id_inputs)\n    investment_id_x = layers.Embedding(investment_id_size, 32, input_length=1)(investment_id_x)\n    investment_id_x = layers.Reshape((-1, ))(investment_id_x)\n    investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)    \n    investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)\n    investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)\n    investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)\n   # investment_id_x = layers.Dropout(0.65)(investment_id_x)\n   \n    \n    feature_x = layers.Dense(256, activation='swish')(features_inputs)\n    feature_x = layers.Dense(256, activation='swish')(feature_x)\n    feature_x = layers.Dense(256, activation='swish')(feature_x)\n    feature_x = layers.Dense(256, activation='swish')(feature_x)\n    feature_x = layers.Dropout(0.65)(feature_x)\n    \n    x = layers.Concatenate(axis=1)([investment_id_x, feature_x])\n    x = layers.Dense(512, activation='swish', kernel_regularizer=\"l2\")(x)\n   # x = layers.Dropout(0.2)(x)\n    x = layers.Dense(128, activation='swish', kernel_regularizer=\"l2\")(x)\n  #  x = layers.Dropout(0.4)(x)\n    x = layers.Dense(32, activation='swish', kernel_regularizer=\"l2\")(x)\n    x = layers.Dense(32, activation='swish', kernel_regularizer=\"l2\")(x)\n    x = layers.Dropout(0.75)(x)\n    output = layers.Dense(1)(x)\n    rmse = keras.metrics.RootMeanSquaredError(name=\"rmse\")\n    model = tf.keras.Model(inputs=[investment_id_inputs, features_inputs], outputs=[output])\n    model.compile(optimizer=tf.optimizers.Adam(0.001), loss='mse', metrics=['mse', \"mae\", \"mape\", rmse])\n    return model\n\n\ndef get_model5():\n    features_inputs = tf.keras.Input((300, ), dtype=tf.float16)\n    \n    ## feature ##\n    feature_x = layers.Dense(256, activation='swish')(features_inputs)\n    feature_x = layers.Dropout(0.1)(feature_x)\n    ## convolution 1 ##\n    feature_x = layers.Reshape((-1,1))(feature_x)\n    feature_x = layers.Conv1D(filters=16, kernel_size=4, strides=1, padding='same')(feature_x)\n    feature_x = layers.BatchNormalization()(feature_x)\n    feature_x = layers.LeakyReLU()(feature_x)\n    ## convolution 2 ##\n    feature_x = layers.Conv1D(filters=16, kernel_size=4, strides=4, padding='same')(feature_x)\n    feature_x = layers.BatchNormalization()(feature_x)\n    feature_x = layers.LeakyReLU()(feature_x)\n    ## convolution 3 ##\n    feature_x = layers.Conv1D(filters=64, kernel_size=4, strides=1, padding='same')(feature_x)\n    feature_x = layers.BatchNormalization()(feature_x)\n    feature_x = layers.LeakyReLU()(feature_x)\n    ## convolution 4 ##\n    feature_x = layers.Conv1D(filters=64, kernel_size=4, strides=4, padding='same')(feature_x)\n    feature_x = layers.BatchNormalization()(feature_x)\n    feature_x = layers.LeakyReLU()(feature_x)\n    ## convolution 5 ##\n    feature_x = layers.Conv1D(filters=64, kernel_size=4, strides=2, padding='same')(feature_x)\n    feature_x = layers.BatchNormalization()(feature_x)\n    feature_x = layers.LeakyReLU()(feature_x)\n    ## flatten ##\n    feature_x = layers.Flatten()(feature_x)\n    \n    x = layers.Dense(512, activation='swish', kernel_regularizer=\"l2\")(feature_x)\n    \n    x = layers.Dropout(0.1)(x)\n    x = layers.Dense(128, activation='swish', kernel_regularizer=\"l2\")(x)\n    x = layers.Dropout(0.1)(x)\n    x = layers.Dense(32, activation='swish', kernel_regularizer=\"l2\")(x)\n    x = layers.Dropout(0.1)(x)\n    output = layers.Dense(1)(x)\n    rmse = keras.metrics.RootMeanSquaredError(name=\"rmse\")\n    model = tf.keras.Model(inputs=[features_inputs], outputs=[output])\n    model.compile(optimizer=tf.optimizers.Adam(0.001), loss='mse', metrics=['mse', \"mae\", \"mape\", rmse])\n    return model\ndel train\n# del investment_id\ndel y\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-04-20T22:24:56.110311Z","iopub.execute_input":"2022-04-20T22:24:56.111107Z","iopub.status.idle":"2022-04-20T22:24:56.304509Z","shell.execute_reply.started":"2022-04-20T22:24:56.110752Z","shell.execute_reply":"2022-04-20T22:24:56.30373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.0 Model_Dropout_10_RMSE","metadata":{}},{"cell_type":"code","source":"def get_model_dr04():\n    features_inputs = tf.keras.Input((300, ), dtype=tf.float32)\n    \n    feature_x = layers.Dense(256, activation='swish')(features_inputs)\n    feature_x = layers.Dropout(0.4)(feature_x)\n    feature_x = layers.Dense(128, activation='swish')(feature_x)\n    feature_x = layers.Dropout(0.4)(feature_x)\n    feature_x = layers.Dense(64, activation='swish')(feature_x)\n    \n    x = layers.Concatenate(axis=1)([feature_x])\n    x = layers.Dropout(0.4)(x)\n    x = layers.Dense(64, activation='swish', kernel_regularizer=\"l2\")(x)\n    x = layers.Dropout(0.4)(x)\n    x = layers.Dense(32, activation='swish', kernel_regularizer=\"l2\")(x)\n    x = layers.Dropout(0.4)(x)\n    x = layers.Dense(16, activation='swish', kernel_regularizer=\"l2\")(x)\n    x = layers.Dropout(0.4)(x)\n    output = layers.Dense(1)(x)\n    output = tf.keras.layers.BatchNormalization(axis=1)(output)\n    rmse = keras.metrics.RootMeanSquaredError(name=\"rmse\")\n    model = tf.keras.Model(inputs=[features_inputs], outputs=[output])\n    model.compile(optimizer=tf.optimizers.Adam(0.001),  loss = correlationLoss, metrics=[correlationMetric])\n    return model\n\ndr=0.3\n\ngpus = tf.config.experimental.list_physical_devices('GPU')\nfor gpu in gpus:\n    print(\"Name:\", gpu.name, \"  Type:\", gpu.device_type)\n\nn_features = 300\nfeatures = [f'f_{i}' for i in range(n_features)]\n\n# def preprocess(X, y):\n#     return X, y\n# def make_dataset(feature, y, batch_size=1024, mode=\"train\"):\n#     ds = tf.data.Dataset.from_tensor_slices((feature, y))\n#     ds = ds.map(preprocess)\n#     if mode == \"train\":\n#         ds = ds.shuffle(512)\n# #     ds = ds.batch(batch_size).cache().prefetch(tf.data.experimental.AUTOTUNE)\n#     ds = ds.batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n#     return ds\n\ndef correlationMetric(x, y, axis=-2):\n  \"\"\"Metric returning the Pearson correlation coefficient of two tensors over some axis, default -2.\"\"\"\n  x = tf.convert_to_tensor(x)\n  y = math_ops.cast(y, x.dtype)\n  n = tf.cast(tf.shape(x)[axis], x.dtype)\n  xsum = tf.reduce_sum(x, axis=axis)\n  ysum = tf.reduce_sum(y, axis=axis)\n  xmean = xsum / n\n  ymean = ysum / n\n  xvar = tf.reduce_sum( tf.math.squared_difference(x, xmean), axis=axis)\n  yvar = tf.reduce_sum( tf.math.squared_difference(y, ymean), axis=axis)\n  cov = tf.reduce_sum( (x - xmean) * (y - ymean), axis=axis)\n  corr = cov / tf.sqrt(xvar * yvar)\n  return tf.constant(1.0, dtype=x.dtype) - corr\n\n\ndef correlationLoss(x,y, axis=-2):\n  \"\"\"Loss function that maximizes the pearson correlation coefficient between the predicted values and the labels,\n  while trying to have the same mean and variance\"\"\"\n  x = tf.convert_to_tensor(x)\n  y = math_ops.cast(y, x.dtype)\n  n = tf.cast(tf.shape(x)[axis], x.dtype)\n  xsum = tf.reduce_sum(x, axis=axis)\n  ysum = tf.reduce_sum(y, axis=axis)\n  xmean = xsum / n\n  ymean = ysum / n\n  xsqsum = tf.reduce_sum( tf.math.squared_difference(x, xmean), axis=axis)\n  ysqsum = tf.reduce_sum( tf.math.squared_difference(y, ymean), axis=axis)\n  cov = tf.reduce_sum( (x - xmean) * (y - ymean), axis=axis)\n  corr = cov / tf.sqrt(xsqsum * ysqsum)\n  return tf.convert_to_tensor( K.mean(tf.constant(1.0, dtype=x.dtype) - corr ) , dtype=tf.float32 )\ndef correlationMetric_01mse(x, y, axis=-2):\n  \"\"\"Metric returning the Pearson correlation coefficient of two tensors over some axis, default -2.\"\"\"\n  x = tf.convert_to_tensor(x)\n  y = math_ops.cast(y, x.dtype)\n  n = tf.cast(tf.shape(x)[axis], x.dtype)\n  xsum = tf.reduce_sum(x, axis=axis)\n  ysum = tf.reduce_sum(y, axis=axis)\n  xmean = xsum / n\n  ymean = ysum / n\n  xvar = tf.reduce_sum( tf.math.squared_difference(x, xmean), axis=axis)\n  yvar = tf.reduce_sum( tf.math.squared_difference(y, ymean), axis=axis)\n  cov = tf.reduce_sum( (x - xmean) * (y - ymean), axis=axis)\n  corr = cov / tf.sqrt(xvar * yvar)\n  return tf.constant(1.0, dtype=x.dtype) - corr\n\ndef correlation(x, y, axis=-2):\n    xmean = tf.reduce_mean(x, axis=axis)\n    ymean = tf.reduce_mean(y, axis=axis)\n    cossim = keras.losses.cosine_similarity(x - xmean, y - ymean, axis=axis)\n    return 1 + cossim\n\ngc.collect()\n\n# list(GroupKFold(5).split(train , groups = train.index))[0]\ndef pearson_coef(data):\n    return data.corr()['target']['preds']\n\ndef evaluate_metric(valid_df):\n    return np.mean(valid_df[['time_id_', 'target', 'preds']].groupby('time_id').apply(pearson_coef))","metadata":{"execution":{"iopub.status.busy":"2022-04-20T22:24:56.305884Z","iopub.execute_input":"2022-04-20T22:24:56.306288Z","iopub.status.idle":"2022-04-20T22:24:56.468737Z","shell.execute_reply.started":"2022-04-20T22:24:56.306235Z","shell.execute_reply":"2022-04-20T22:24:56.468012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_model_best(ft_units, x_units, x_dropout):\n    investment_id_inputs = tf.keras.Input((1, ), dtype=tf.uint16)\n    features_inputs = tf.keras.Input((300, ), dtype=tf.float16)\n    \n    investment_id_x = investment_id_lookup_layer2(investment_id_inputs)\n    investment_id_x = layers.Embedding(investment_id_size2, 32, input_length=1)(investment_id_x)\n    investment_id_x = layers.Reshape((-1, ))(investment_id_x)\n    investment_id_x = layers.Dense(128, activation='swish')(investment_id_x)\n    investment_id_x = layers.Dense(128, activation='swish')(investment_id_x)\n    investment_id_x = layers.Dense(128, activation='swish')(investment_id_x)\n    \n    feature_x = layers.Dense(256, activation='swish')(features_inputs)\n    for hu in ft_units:\n        feature_x = layers.Dense(hu, activation='swish')(feature_x)\n    \n    x = layers.Concatenate(axis=1)([investment_id_x, feature_x])\n    \n    for i in range(len(x_units)):\n        x = tf.keras.layers.Dense(x_units[i], kernel_regularizer=\"l2\")(x) #v8\n        x = tf.keras.layers.BatchNormalization()(x) #v7\n        x = tf.keras.layers.Activation('swish')(x) #v7\n        x = tf.keras.layers.Dropout(x_dropout[i])(x) #v8\n        \n    output = layers.Dense(1)(x)\n    rmse = keras.metrics.RootMeanSquaredError(name=\"rmse\")\n    model = tf.keras.Model(inputs=[investment_id_inputs, features_inputs], outputs=[output])\n    model.compile(optimizer=tf.optimizers.Adam(0.0001), loss='mse', metrics=['mse', \"mae\", \"mape\", rmse])\n    return model\n\nparams = {\n    'ft_units': [256,256],\n    'x_units': [512, 256, 128, 32],\n    'x_dropout': [0.4, 0.3, 0.2, 0.1]\n#           'lr':1e-3, \n         }\n\nmodels_best = []\nscores = []\nfor i in range(7):\n    model = get_model_best(**params)\n    model.load_weights(f\"../input/wmodels/best/model_{i}.tf\")\n    models_best.append(model)","metadata":{"execution":{"iopub.status.busy":"2022-04-20T22:24:56.469931Z","iopub.execute_input":"2022-04-20T22:24:56.470644Z","iopub.status.idle":"2022-04-20T22:24:59.835897Z","shell.execute_reply.started":"2022-04-20T22:24:56.470606Z","shell.execute_reply":"2022-04-20T22:24:59.834499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.1 Augment Model: Gaussian_Conv1 + Conv2d Model: Account for Spatial/Area Relationship","metadata":{}},{"cell_type":"code","source":"def get_model6():\n    features_inputs = tf.keras.Input((300, ), dtype=tf.float16)\n    \n    features_x = layers.GaussianNoise(0.1)(features_inputs)\n    ## feature ##\n    feature_x = layers.Dense(256, activation='swish')(features_x)\n    feature_x = layers.Dropout(0.1)(feature_x)\n    ## convolution 1 ##\n    feature_x = layers.Reshape((-1,1))(feature_x)\n    feature_x = layers.Conv1D(filters=16, kernel_size=4, strides=1, padding='same')(feature_x)\n    feature_x = layers.BatchNormalization()(feature_x)\n    feature_x = layers.LeakyReLU()(feature_x)\n    ## convolution 2 ##\n    feature_x = layers.Conv1D(filters=16, kernel_size=4, strides=4, padding='same')(feature_x)\n    feature_x = layers.BatchNormalization()(feature_x)\n    feature_x = layers.LeakyReLU()(feature_x)\n    ## convolution 3 ##\n    feature_x = layers.Conv1D(filters=64, kernel_size=4, strides=1, padding='same')(feature_x)\n    feature_x = layers.BatchNormalization()(feature_x)\n    feature_x = layers.LeakyReLU()(feature_x)\n    ## convolution 4 ##\n    feature_x = layers.Conv1D(filters=64, kernel_size=4, strides=4, padding='same')(feature_x)\n    feature_x = layers.BatchNormalization()(feature_x)\n    feature_x = layers.LeakyReLU()(feature_x)\n    ## convolution 5 ##\n    feature_x = layers.Conv1D(filters=64, kernel_size=4, strides=2, padding='same')(feature_x)\n    feature_x = layers.BatchNormalization()(feature_x)\n    feature_x = layers.LeakyReLU()(feature_x)\n    ## flatten ##\n    feature_x = layers.Flatten()(feature_x)\n \n    x = layers.Dense(512, activation='swish', kernel_regularizer=\"l2\")(feature_x)\n    \n    x = layers.Dropout(0.1)(x)\n    x = layers.Dense(128, activation='swish', kernel_regularizer=\"l2\")(x)\n    x = layers.Dropout(0.1)(x)\n    x = layers.Dense(32, activation='swish', kernel_regularizer=\"l2\")(x)\n    x = layers.Dropout(0.1)(x)\n    output = layers.Dense(1)(x)\n    rmse = keras.metrics.RootMeanSquaredError(name=\"rmse\")\n    model = tf.keras.Model(inputs=[features_inputs], outputs=[output])\n    model.compile(optimizer=tf.optimizers.Adam(0.001), loss='mse', metrics=['mse', \"mae\", \"mape\", rmse])\n    return model\n\ndef get_model7():\n    features_inputs = tf.keras.Input((300, ), dtype=tf.float16)\n    \n    ## Dense 1 ##\n    feature_x = layers.Dense(256, activation='swish')(features_inputs)\n    feature_x = layers.Dropout(0.1)(feature_x)\n    ## convolution 1 ##\n    feature_x = layers.Reshape((-1,1))(feature_x)\n    feature_x = layers.Conv1D(filters=16, kernel_size=4, strides=1, padding='same')(feature_x)\n    feature_x = layers.BatchNormalization()(feature_x)\n    feature_x = layers.LeakyReLU()(feature_x)\n    ## convolution 2 ##\n    feature_x = layers.Conv1D(filters=16, kernel_size=4, strides=4, padding='same')(feature_x)\n    feature_x = layers.BatchNormalization()(feature_x)\n    feature_x = layers.LeakyReLU()(feature_x)\n    ## convolution 3 ##\n    feature_x = layers.Conv1D(filters=64, kernel_size=4, strides=1, padding='same')(feature_x)\n    feature_x = layers.BatchNormalization()(feature_x)\n    feature_x = layers.LeakyReLU()(feature_x)\n\n    ## convolution2D 1 ##\n    feature_x = layers.Reshape((64,64,1))(feature_x)\n    feature_x = layers.Conv2D(filters=32, kernel_size=4, strides=1, padding='same')(feature_x)\n    feature_x = layers.BatchNormalization()(feature_x)\n    feature_x = layers.LeakyReLU()(feature_x)\n    ## convolution2D 2 ##\n    feature_x = layers.Conv2D(filters=32, kernel_size=4, strides=4, padding='same')(feature_x)\n    feature_x = layers.BatchNormalization()(feature_x)\n    feature_x = layers.LeakyReLU()(feature_x)\n    ## convolution2D 3 ##\n    feature_x = layers.Conv2D(filters=32, kernel_size=4, strides=4, padding='same')(feature_x)\n    feature_x = layers.BatchNormalization()(feature_x)\n    feature_x = layers.LeakyReLU()(feature_x)\n\n    ## flatten ##\n    feature_x = layers.Flatten()(feature_x)\n    ## Dense 3 ##\n    x = layers.Dense(512, activation='swish', kernel_regularizer=\"l2\")(feature_x)\n    ## Dense 4 ##\n    x = layers.Dropout(0.1)(x)\n    ## Dense 5 ##    \n    x = layers.Dense(128, activation='swish', kernel_regularizer=\"l2\")(x)\n    x = layers.Dropout(0.1)(x)\n    ## Dense 6 ##\n    x = layers.Dense(32, activation='swish', kernel_regularizer=\"l2\")(x)\n    x = layers.Dropout(0.1)(x)\n    ## Dense 7 ##\n    output = layers.Dense(1)(x)\n    rmse = keras.metrics.RootMeanSquaredError(name=\"rmse\")\n    model = tf.keras.Model(inputs=[features_inputs], outputs=[output])\n    model.compile(optimizer=tf.optimizers.Adam(0.001), loss='mse', metrics=['mse', \"mae\", \"mape\", rmse])\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-04-20T22:24:59.838098Z","iopub.execute_input":"2022-04-20T22:24:59.838553Z","iopub.status.idle":"2022-04-20T22:24:59.884838Z","shell.execute_reply.started":"2022-04-20T22:24:59.838515Z","shell.execute_reply":"2022-04-20T22:24:59.883739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.2 Augment2: Feature_Time Model","metadata":{}},{"cell_type":"code","source":"def get_model_ft():\n    investment_id_input = tf.keras.Input(shape=(1,), dtype=tf.uint16, name='investment_id')\n    inv_x = layers.Dense(64, activation='relu')(investment_id_input)\n    inv_x = layers.Dropout(0.2)(inv_x)\n\n    features_input = tf.keras.Input(shape=(300,), dtype=tf.float16, name='features')\n    f_x = layers.Dense(512, activation='relu')(features_input)\n    f_x = layers.Dropout(0.25)(f_x)\n    f_x = layers.Dense(256, activation='relu')(f_x)\n    f_x = layers.Dropout(0.2)(f_x)\n\n    time_id_input = tf.keras.Input(shape=(1,), dtype=tf.uint16, name='time_id')\n    time_x = layers.Dense(64, activation='relu')(time_id_input)\n    time_x = layers.Dropout(0.2)(time_x)\n\n    concatenated = layers.concatenate([inv_x, f_x, time_x], axis=-1)\n    output = layers.Dense(1)(concatenated)\n\n    model = tf.keras.models.Model([investment_id_input, features_input, time_id_input], output, name='model_with_time_id')\n    \n    model.compile(optimizer='rmsprop', loss='mse', metrics=['mse', 'mae', 'mape'])\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-04-20T22:24:59.886413Z","iopub.execute_input":"2022-04-20T22:24:59.887013Z","iopub.status.idle":"2022-04-20T22:24:59.909089Z","shell.execute_reply.started":"2022-04-20T22:24:59.886971Z","shell.execute_reply":"2022-04-20T22:24:59.908231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()\nmodel_ft = get_model_ft()\nmodel_ft.summary()","metadata":{"execution":{"iopub.status.busy":"2022-04-20T22:24:59.910698Z","iopub.execute_input":"2022-04-20T22:24:59.91121Z","iopub.status.idle":"2022-04-20T22:25:00.210974Z","shell.execute_reply.started":"2022-04-20T22:24:59.911171Z","shell.execute_reply":"2022-04-20T22:25:00.210263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"models = []\n\nfor i in range(5):\n    model = get_model()\n    model.load_weights(f'../input/dnn-base/model_{i}')\n    models.append(model)\n\nfor i in range(10):\n    model = get_model2()\n    model.load_weights(f'../input/train-dnn-v2-10fold/model_{i}')\n    models.append(model)\n\nfor i in range(5):\n    model = MyModel(investment_id=investment_id, device='cpu')\n    model.load_weights(f'../input/masked-model-weights/random_mask_DNN_ensemble_weights/model{i}/model_{i}.tf')\n    models.append(model)\n    \n    \nmodels2 = []\n    \nfor i in range(5):\n    model = get_model5()\n    model.load_weights(f'../input/prediction-including-spatial-info-with-conv1d/model_{i}.tf')\n    models2.append(model)\n    \nfor i in range(5):\n    model = get_model6()\n    model.load_weights(f'../input/gaussian-noise-model-weights/model_{i}.tf')\n    models2.append(model)\n\nfor i in range(5):\n    model = get_model7()\n    model.load_weights(f'../input/altconv2dweights/model_{i}.tf')\n    models2.append(model)\n    \nmodels3 = []\n    \nfor i in range(10):\n    model = get_model_dr04()\n    model.load_weights(f'../input/mse10-model-weights/model_{i}')\n    models3.append(model)\n\nmodel_ft = get_model_ft()\nmodel_ft.load_weights(f'../input/feature-time-model/ns_model_with_time_id.tf')","metadata":{"execution":{"iopub.status.busy":"2022-04-20T22:25:00.212416Z","iopub.execute_input":"2022-04-20T22:25:00.21292Z","iopub.status.idle":"2022-04-20T22:25:11.719085Z","shell.execute_reply.started":"2022-04-20T22:25:00.212884Z","shell.execute_reply":"2022-04-20T22:25:11.718404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Transformer_masked_model","metadata":{}},{"cell_type":"code","source":"def gen_ids_and_skf_idxs():\n    train = pd.read_pickle('../input/ubiquant-market-prediction-half-precision-pickle/train.pkl')\n    investment_id = train[[\"investment_id\"]].astype('int64')\n    train.pop(\"investment_id\")\n    train.pop(\"time_id\")\n    train.pop(\"target\")\n    skf = StratifiedKFold(5, shuffle=True, random_state=42)\n    idxs = list(enumerate(skf.split(train, investment_id)))\n    del train\n    gc.collect()\n    return investment_id, idxs","metadata":{"execution":{"iopub.status.busy":"2022-04-20T22:25:11.720368Z","iopub.execute_input":"2022-04-20T22:25:11.720623Z","iopub.status.idle":"2022-04-20T22:25:11.726987Z","shell.execute_reply.started":"2022-04-20T22:25:11.72059Z","shell.execute_reply":"2022-04-20T22:25:11.726324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''investment_id, idxs = gen_ids_and_skf_idxs()\n\ninvestment_ids = list(np.unique(investment_id.values))\ninvestment_id_size = len(investment_ids) + 1\nid_lookup_layer = layers.IntegerLookup(max_tokens=investment_id_size)\nid_lookup_layer.adapt(investment_id)'''","metadata":{"execution":{"iopub.status.busy":"2022-04-20T22:25:11.728231Z","iopub.execute_input":"2022-04-20T22:25:11.72865Z","iopub.status.idle":"2022-04-20T22:25:11.740211Z","shell.execute_reply.started":"2022-04-20T22:25:11.728618Z","shell.execute_reply":"2022-04-20T22:25:11.739459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TransformerBlock(layers.Layer):\n    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n        super(TransformerBlock, self).__init__()\n        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n        self.ffn = keras.Sequential(\n            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n        )\n        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n        self.dropout1 = layers.Dropout(rate)\n        self.dropout2 = layers.Dropout(rate)\n\n    def call(self, inputs, training):\n        attn_output = self.att(inputs, inputs)\n        attn_output = self.dropout1(attn_output, training=training)\n        out1 = self.layernorm1(inputs + attn_output)\n        ffn_output = self.ffn(out1)\n        ffn_output = self.dropout2(ffn_output, training=training)\n        return self.layernorm2(out1 + ffn_output)","metadata":{"execution":{"iopub.status.busy":"2022-04-20T22:25:11.741574Z","iopub.execute_input":"2022-04-20T22:25:11.741986Z","iopub.status.idle":"2022-04-20T22:25:11.751341Z","shell.execute_reply.started":"2022-04-20T22:25:11.741949Z","shell.execute_reply":"2022-04-20T22:25:11.750662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_model_transformer():\n  embed_dim = 300  # Embedding size for each token\n  num_heads = 8  # Number of attention heads\n  ff_dim = 512  # Hidden layer size in feed forward network inside transformer\n\n  transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n  features_input = tf.keras.Input((300,), dtype=tf.float32)\n  investment_id_input = tf.keras.Input((1,))\n\n  investment_id_x = investment_id_lookup_layer(investment_id_input)\n  investment_id_x = layers.Embedding(investment_id_size, 32)(investment_id_x)\n  investment_id_x = layers.Dense(64, activation='swish', kernel_initializer='he_normal', bias_initializer='zeros')(investment_id_x)\n  investment_id_x = layers.Dropout(0.5)(investment_id_x)\n  investment_id_x = layers.Dense(32, activation='swish', kernel_initializer='he_normal', bias_initializer='zeros')(investment_id_x)\n  investment_id_x = layers.Dropout(0.5)(investment_id_x)\n  investment_id_x = tf.squeeze(investment_id_x, axis=1)\n\n  features_inputs = tf.expand_dims(investment_id_x, axis = -1) + tf.expand_dims(features_input, axis = 1)\n  # (None, 32, 300)\n\n# 6 transformer blocks\n  features_x = transformer_block(features_inputs)\n\n  features_x = layers.Reshape((-1,))(features_x)\n# MLP for output\n  output_x = layers.Dense(512, activation='swish',\n                          kernel_initializer=initializers.RandomNormal(stddev=1))(features_x)\n  output_x = layers.Dropout(0.6)(output_x)\n  output_x = layers.Dense(256, activation='swish',\n                          kernel_initializer=initializers.RandomNormal(stddev=1))(output_x)\n  output_x = layers.Dropout(0.6)(output_x)\n  output_x = layers.Dense(128, activation='swish',\n                          kernel_initializer=initializers.RandomNormal(stddev=1))(output_x)\n  output_x = layers.Dense(64, activation='swish',\n                          kernel_initializer=initializers.RandomNormal(stddev=1),\n                          kernel_regularizer=\"l2\")(output_x)\n  output_x = layers.Dropout(0.6)(output_x)\n  output_x = layers.Dense(32, activation='swish',\n                          kernel_initializer=initializers.RandomNormal(stddev=1),\n                          kernel_regularizer=\"l2\")(output_x)\n  output_x = layers.Dropout(0.6)(output_x)\n  output_x = layers.Dense(16, activation='swish',\n                          kernel_initializer=initializers.RandomNormal(stddev=1),\n                          kernel_regularizer=\"l2\")(output_x)\n  output_x = layers.Dropout(0.6)(output_x)\n  output = layers.Dense(1)(output_x)\n  output = tf.keras.layers.BatchNormalization(axis=1)(output)\n\n# Model compilation\n  rmse = keras.metrics.RootMeanSquaredError(name=\"rmse\")\n  model = tf.keras.Model(inputs=[investment_id_input, features_input], outputs=[output])\n  # learning rate decay in Adam\n  initial_learning_rate = 5e-4\n  lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n        initial_learning_rate,\n        decay_steps=10000,\n        decay_rate=0.9,\n        staircase=True)\n  model.compile(optimizer=tf.optimizers.Adam(learning_rate=lr_schedule), \n                loss='mse', \n                metrics=[rmse, correlation])\n  return model","metadata":{"execution":{"iopub.status.busy":"2022-04-20T22:25:11.752654Z","iopub.execute_input":"2022-04-20T22:25:11.75307Z","iopub.status.idle":"2022-04-20T22:25:11.771293Z","shell.execute_reply.started":"2022-04-20T22:25:11.753033Z","shell.execute_reply":"2022-04-20T22:25:11.770584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# End of transformer_masked model","metadata":{}},{"cell_type":"code","source":"def get_model_corr(ft_units, x_units, x_dropout):\n    \n    # investment_id\n    investment_id_inputs = tf.keras.Input((1, ), dtype=tf.uint16)\n    investment_id_x = investment_id_lookup_layer(investment_id_inputs)\n    investment_id_x = layers.Embedding(investment_id_size, 32, input_length=1)(investment_id_x)\n    investment_id_x = layers.Reshape((-1, ))(investment_id_x)\n    investment_id_x = layers.Dense(128, activation='swish')(investment_id_x)\n    investment_id_x = layers.Dense(128, activation='swish')(investment_id_x) \n    investment_id_x = layers.Dense(128, activation='swish')(investment_id_x)\n    \n    # features_inputs\n    features_inputs = tf.keras.Input((300, ), dtype=tf.float16)\n    bn = tf.keras.layers.BatchNormalization()(features_inputs)\n    gn = tf.keras.layers.GaussianNoise(0.035)(bn)\n    feature_x = layers.Dense(300, activation='swish')(gn)\n    feature_x = tf.keras.layers.Dropout(0.5)(feature_x)\n    \n    for hu in ft_units:\n        feature_x = layers.Dense(hu, activation='swish')(feature_x)\n#         feature_x = tf.keras.layers.Activation('swish')(feature_x)\n        feature_x = tf.keras.layers.Dropout(0.35)(feature_x)\n    \n    x = layers.Concatenate(axis=1)([investment_id_x, feature_x])\n    \n    for i in range(len(x_units)):\n        x = tf.keras.layers.Dense(x_units[i], kernel_regularizer=\"l2\")(x) \n        x = tf.keras.layers.Activation('swish')(x)\n        x = tf.keras.layers.Dropout(x_dropout[i])(x)\n        \n    output = layers.Dense(1)(x)\n    model = tf.keras.Model(inputs=[investment_id_inputs, features_inputs], outputs=[output])\n    model.compile(optimizer=tf.optimizers.Adam(0.0001), loss=correlationLoss, \n                  metrics=['mse', \"mae\", correlation])\n    return model\n\n\nparams = {\n#     'num_columns': len(features), \n    'ft_units': [150, 75, 150 ,200],\n    'x_units': [512, 256, 128, 32],\n    'x_dropout': [0.44, 0.4, 0.33, 0.2] #4, 3, 2, 1\n#           'lr':1e-3, \n         }","metadata":{"execution":{"iopub.status.busy":"2022-04-20T22:25:11.772219Z","iopub.execute_input":"2022-04-20T22:25:11.7724Z","iopub.status.idle":"2022-04-20T22:25:11.786534Z","shell.execute_reply.started":"2022-04-20T22:25:11.772378Z","shell.execute_reply":"2022-04-20T22:25:11.785583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Validation","metadata":{}},{"cell_type":"code","source":"def preprocess_test(investment_id, feature):\n    return (investment_id, feature), 0\n\ndef preprocess_test_s(feature):\n    return (feature), 0\n\ndef make_test_dataset(feature, investment_id, batch_size=1024):\n    ds = tf.data.Dataset.from_tensor_slices(((investment_id, feature)))\n    ds = ds.map(preprocess_test)\n    ds = ds.batch(batch_size).cache().prefetch(tf.data.experimental.AUTOTUNE)\n    return ds\n\ndef make_test_dataset2(feature, batch_size=1024):\n    ds = tf.data.Dataset.from_tensor_slices(((feature)))\n    ds = ds.batch(batch_size).cache().prefetch(tf.data.AUTOTUNE)\n    return ds\n\ndef inference(models, ds):\n    y_preds = []\n    for model in models:\n        y_pred = model.predict(ds)\n        y_preds.append(y_pred)\n    return np.mean(y_preds, axis=0)\n\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=1)\n\n\ndef pca_inference(models, ds):\n    y_preds = []\n    for model in models:\n        y_pred = model.predict(ds)\n        y_preds.append(y_pred)\n    res = np.hstack(y_preds)\n    print(len(res))\n    if len(res)>1:\n        res = pca.fit_transform(res)\n    else:\n        res = np.mean(res, axis=1)\n    return res\n\ndef make_test_dataset3(feature, batch_size=1024):\n    ds = tf.data.Dataset.from_tensor_slices((feature))\n    ds = ds.map(preprocess_test_s)\n    ds = ds.batch(batch_size).cache().prefetch(tf.data.experimental.AUTOTUNE)\n    return ds\n\ndef infer(models, ds):\n    y_preds = []\n    for model in models:\n        y_pred = model.predict(ds)\n        y_preds.append((y_pred-y_pred.mean())/y_pred.std())\n    \n    return np.mean(y_preds, axis=0)\n","metadata":{"execution":{"iopub.status.busy":"2022-04-20T22:25:11.787757Z","iopub.execute_input":"2022-04-20T22:25:11.788228Z","iopub.status.idle":"2022-04-20T22:25:11.820795Z","shell.execute_reply.started":"2022-04-20T22:25:11.788178Z","shell.execute_reply":"2022-04-20T22:25:11.820186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def inference_transformer(model, ds):\n    y_preds = []\n    for i in range(5):\n        model.load_weights(f\"../input/transformer-mask-weights/Transformer_mask_weights/model{i}/model_{i}.tf\")  # private models' weights saved in training cell\n        y_pred = model.predict(ds)\n        y_preds.append(y_pred)\n    return np.mean(y_preds, axis=0)","metadata":{"execution":{"iopub.status.busy":"2022-04-20T22:25:11.821963Z","iopub.execute_input":"2022-04-20T22:25:11.822207Z","iopub.status.idle":"2022-04-20T22:25:11.827413Z","shell.execute_reply.started":"2022-04-20T22:25:11.822174Z","shell.execute_reply":"2022-04-20T22:25:11.826345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import ubiquant\nenv = ubiquant.make_env()\niter_test = env.iter_test()","metadata":{"execution":{"iopub.status.busy":"2022-04-20T22:25:11.829034Z","iopub.execute_input":"2022-04-20T22:25:11.829376Z","iopub.status.idle":"2022-04-20T22:25:11.851325Z","shell.execute_reply.started":"2022-04-20T22:25:11.829338Z","shell.execute_reply":"2022-04-20T22:25:11.850709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" for (test_df, sample_prediction_df) in iter_test:\n    ds = make_test_dataset(test_df[features], test_df[\"investment_id\"])\n    p1 = inference(models, ds) # DNN + masked_DNN combined\n    ds2 = make_test_dataset2(test_df[features])\n    p2 = inference(models2, ds2) # Conv2D\n    ds3 = make_test_dataset3(test_df[features])\n    p3 = infer(models3, ds3) # dropout = 0.4 for MLP, not used for this version\n    p4 = inference(models_best, ds) # grid_search\n    \n    # feature_time_augment\n    test_time_id = test_df['row_id'].str.split('_', expand=True).get(key=0).astype(int)\n    ds5 = make_ft_dataset(investment_id=test_df['investment_id'], feature=test_df[features], time_id=test_time_id)\n    p5 = model_ft.predict([test_df['investment_id'], test_df[features], test_time_id])[:, 0]\n    \n    # Transformer model\n    ds_transformer = make_test_dataset(test_df[features], test_df[\"investment_id\"].astype('int64'))\n    transformer_model = get_model_transformer()\n    p_t = inference_transformer(transformer_model, ds_transformer)\n    \n    sample_prediction_df['target'] = p1 * 0.18 + p2 * 0.57 + p3 * 0.07 + p_t * 0.07 + p5 * 0.11\n    env.predict(sample_prediction_df) \n    display(sample_prediction_df)","metadata":{"execution":{"iopub.status.busy":"2022-04-20T22:30:16.070132Z","iopub.execute_input":"2022-04-20T22:30:16.070412Z","iopub.status.idle":"2022-04-20T22:30:16.080767Z","shell.execute_reply.started":"2022-04-20T22:30:16.070381Z","shell.execute_reply":"2022-04-20T22:30:16.079872Z"},"trusted":true},"execution_count":null,"outputs":[]}]}