{"cells":[{"cell_type":"markdown","metadata":{"id":"ImLCXm8IsSS2"},"source":["# Download the Cora data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xRN47p1SKRgP","executionInfo":{"status":"ok","timestamp":1649122552438,"user_tz":240,"elapsed":2059,"user":{"displayName":"TONGFEI ZHOU","userId":"15125844894166682257"}},"outputId":"726de66f-6eae-4f22-a5c9-8d95da37f2d4","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["--2022-04-05 01:35:50--  https://linqs-data.soe.ucsc.edu/public/lbc/cora.tgz\n","Resolving linqs-data.soe.ucsc.edu (linqs-data.soe.ucsc.edu)... 128.114.47.74\n","Connecting to linqs-data.soe.ucsc.edu (linqs-data.soe.ucsc.edu)|128.114.47.74|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 168052 (164K) [application/x-gzip]\n","Saving to: ‘cora.tgz’\n","\n","cora.tgz            100%[===================>] 164.11K   404KB/s    in 0.4s    \n","\n","2022-04-05 01:35:51 (404 KB/s) - ‘cora.tgz’ saved [168052/168052]\n","\n","cora/\n","cora/README\n","cora/cora.cites\n","cora/cora.content\n"]}],"source":["! wget https://linqs-data.soe.ucsc.edu/public/lbc/cora.tgz\n","! tar -zxvf cora.tgz"]},{"cell_type":"markdown","metadata":{"id":"rXIYzURA4OKg"},"source":["# import modules and set random seed"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uJQYMX02_z0M"},"outputs":[],"source":["import numpy as np\n","import scipy.sparse as sp\n","import torch\n","import pandas as pd\n","import math\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import time\n","\n","seed = 0\n","\n","np.random.seed(seed)\n","torch.manual_seed(seed)\n","torch.cuda.manual_seed(seed)\n","torch.cuda.manual_seed_all(seed)\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"]},{"cell_type":"markdown","metadata":{"id":"dgOv1h7YsK-5"},"source":["# Loading and preprocessing the data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kXPHN61i9keB"},"outputs":[],"source":["def encode_onehot(labels):\n","    # The classes must be sorted before encoding to enable static class encoding.\n","    # In other words, make sure the first class always maps to index 0.\n","    classes = sorted(list(set(labels)))\n","    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in\n","                    enumerate(classes)}\n","    labels_onehot = np.array(list(map(classes_dict.get, labels)),\n","                             dtype=np.int32)\n","    return labels_onehot\n","\n","\n","def load_data(path=\"/content/cora/\", dataset=\"cora\", training_samples=140):\n","    \"\"\"Load citation network dataset (cora only for now)\"\"\"\n","    print('Loading {} dataset...'.format(dataset))\n","\n","    idx_features_labels = np.genfromtxt(\"{}{}.content\".format(path, dataset),\n","                                        dtype=np.dtype(str))\n","    features = sp.csr_matrix(idx_features_labels[:, 1:-1], dtype=np.float32)\n","    labels = encode_onehot(idx_features_labels[:, -1])\n","\n","    # build graph\n","    idx = np.array(idx_features_labels[:, 0], dtype=np.int32)\n","    idx_map = {j: i for i, j in enumerate(idx)}\n","    edges_unordered = np.genfromtxt(\"{}{}.cites\".format(path, dataset),\n","                                    dtype=np.int32)\n","    edges = np.array(list(map(idx_map.get, edges_unordered.flatten())),\n","                     dtype=np.int32).reshape(edges_unordered.shape)\n","    adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])),\n","                        shape=(labels.shape[0], labels.shape[0]),\n","                        dtype=np.float32)\n","\n","    # build symmetric adjacency matrix\n","    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n","\n","    features = normalize(features)\n","    adj = adj + sp.eye(adj.shape[0])\n","    adj = normalize_adj(adj)\n","\n","    # Random indexes\n","    idx_rand = torch.randperm(len(labels))\n","    # Nodes for training\n","    idx_train = idx_rand[:training_samples]\n","    # Nodes for validation\n","    idx_val= idx_rand[training_samples:]\n","\n","    adj = torch.FloatTensor(np.array(adj.todense()))\n","    features = torch.FloatTensor(np.array(features.todense()))\n","    labels = torch.LongTensor(np.where(labels)[1])\n","\n","    idx_train = torch.LongTensor(idx_train)\n","    idx_val = torch.LongTensor(idx_val)\n","\n","    return adj, features, labels, idx_train, idx_val\n","\n","def normalize_adj(mx):\n","    \"\"\"symmetric normalization\"\"\"\n","    rowsum = np.array(mx.sum(1))\n","    r_inv_sqrt = np.power(rowsum, -0.5).flatten()\n","    r_inv_sqrt[np.isinf(r_inv_sqrt)] = 0.\n","    r_mat_inv_sqrt = sp.diags(r_inv_sqrt)\n","    return mx.dot(r_mat_inv_sqrt).transpose().dot(r_mat_inv_sqrt)\n","\n","def normalize(mx):\n","    \"\"\"Row-normalize sparse matrix\"\"\"\n","    rowsum = np.array(mx.sum(1))\n","    r_inv = np.power(rowsum, -1).flatten()\n","    r_inv[np.isinf(r_inv)] = 0.\n","    r_mat_inv = sp.diags(r_inv)\n","    mx = r_mat_inv.dot(mx)\n","    return mx\n","\n","\n","def accuracy(output, labels):\n","    preds = output.max(1)[1].type_as(labels)\n","    correct = preds.eq(labels).double()\n","    correct = correct.sum()\n","    return correct / len(labels)"]},{"cell_type":"markdown","metadata":{"id":"WzCZVd1JsbHr"},"source":["## check the data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KlsKjMKx8_b7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1649122563352,"user_tz":240,"elapsed":4919,"user":{"displayName":"TONGFEI ZHOU","userId":"15125844894166682257"}},"outputId":"2e7e3bce-5258-458a-d2cf-6dcac08dd80f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Loading cora dataset...\n"]}],"source":["adj, features, labels, idx_train, idx_val = load_data()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mxrv21rLnpiZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1648955658155,"user_tz":240,"elapsed":132,"user":{"displayName":"TONGFEI ZHOU","userId":"15125844894166682257"}},"outputId":"48ce93a1-8bf5-4d37-eabd-0503ae9e85a4"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0.1667, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n","        [0.0000, 0.5000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n","        [0.0000, 0.0000, 0.2000,  ..., 0.0000, 0.0000, 0.0000],\n","        ...,\n","        [0.0000, 0.0000, 0.0000,  ..., 0.2000, 0.0000, 0.0000],\n","        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2000, 0.0000],\n","        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.2500]])\n","torch.Size([2708, 2708])\n"]}],"source":["print(adj)\n","print(adj.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lWrDf0iWnpqV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1648955658155,"user_tz":240,"elapsed":6,"user":{"displayName":"TONGFEI ZHOU","userId":"15125844894166682257"}},"outputId":"154d9b40-1006-4cc9-e309-f7314dea31e4"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n","        [0., 0., 0.,  ..., 0., 0., 0.],\n","        [0., 0., 0.,  ..., 0., 0., 0.],\n","        ...,\n","        [0., 0., 0.,  ..., 0., 0., 0.],\n","        [0., 0., 0.,  ..., 0., 0., 0.],\n","        [0., 0., 0.,  ..., 0., 0., 0.]])\n","torch.Size([2708, 1433])\n"]}],"source":["print(features)\n","print(features.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TUkt2JJdsuA2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1648955658155,"user_tz":240,"elapsed":4,"user":{"displayName":"TONGFEI ZHOU","userId":"15125844894166682257"}},"outputId":"905873bd-8750-4f01-8003-94f30b432533"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([2, 5, 4,  ..., 1, 0, 2])\n","tensor([0, 1, 2, 3, 4, 5, 6])\n","2708\n"]}],"source":["print(labels)\n","print(labels.unique())\n","print(len(labels))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iGP18jNAs1Gp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1648955658156,"user_tz":240,"elapsed":5,"user":{"displayName":"TONGFEI ZHOU","userId":"15125844894166682257"}},"outputId":"381f18fd-7dfe-46b6-f6f5-26fb0503065a"},"outputs":[{"output_type":"stream","name":"stdout","text":["140\n","2568\n"]}],"source":["print(len(idx_train))\n","print(len(idx_val))"]},{"cell_type":"markdown","metadata":{"id":"vHqIcfH-vIic"},"source":["# Vanilla GCN for node classification\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","source":["## Define Graph Convolution layer (Your Task)\n","\n","This module takes $\\mathbf{h} = \\{ \\overrightarrow{h_1}, \\overrightarrow{h_2}, \\dots, \\overrightarrow{h_N} \\}$ where $\\overrightarrow{h_i} \\in \\mathbb{R}^F$ as input and outputs $\\mathbf{h'} = \\{ \\overrightarrow{h'_1}, \\overrightarrow{h'_2}, \\dots, \\overrightarrow{h'_N} \\}$, where $\\overrightarrow{h'_i} \\in \\mathbb{R}^{F'}$.\n","1.   perform initial transformation: $\\mathbf{s} = \\mathbf{W} \\times \\mathbf{h} ^{(l)}$\n","2.   multiply $\\mathbf{s}$ by normalized adjacency matrix: $\\mathbf{h'} = \\mathbf{A} \\times \\mathbf{s}$"],"metadata":{"id":"f48tylWyjLPE"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"M-fU8L7f41VZ"},"outputs":[],"source":["class GraphConvolution(nn.Module):\n","    \"\"\"\n","    A Graph Convolution Layer (GCN)\n","    \"\"\"\n","\n","    def __init__(self, in_features, out_features, bias=True):\n","        \"\"\"\n","        * `in_features`, $F$, is the number of input features per node\n","        * `out_features`, $F'$, is the number of output features per node\n","        * `bias`, whether to include the bias term in the linear layer. Default=True\n","        \"\"\"\n","        super(GraphConvolution, self).__init__()\n","        # TODO: initialize the weight W that maps the input feature (dim F ) to output feature (dim F')\n","        # hint: use nn.Linear()\n","        ############ Your code here ###################################\n","        self.initial_layer = nn.Linear(in_features=in_features, \n","                                       out_features=out_features, \n","                                       bias=bias)\n","\n","\n","        ###############################################################\n","\n","    def forward(self, input, adj):\n","        # TODO: transform input feature to output (don't forget to use the adjacency matrix \n","        # to sum over neighbouring nodes )\n","        # hint: use the linear layer you declared above. \n","        # hint: you can use torch.spmm() sparse matrix multiplication to handle the \n","        #       adjacency matrix\n","        ############ Your code here ###################################\n","        return torch.spmm(adj, self.initial_layer(input))\n","        ###############################################################\n"]},{"cell_type":"markdown","source":["## Define GCN (Your Task)\n","\n","you will implement a two-layer GCN with ReLU activation function and Dropout after the first Conv layer."],"metadata":{"id":"RxBELCxkjF6F"}},{"cell_type":"code","source":["class GCN(nn.Module):\n","    '''\n","    A two-layer GCN\n","    '''\n","    def __init__(self, nfeat, n_hidden, n_classes, dropout, bias=True):\n","        \"\"\"\n","        * `nfeat`, is the number of input features per node of the first layer\n","        * `n_hidden`, number of hidden units\n","        * `n_classes`, total number of classes for classification\n","        * `dropout`, the dropout ratio\n","        * `bias`, whether to include the bias term in the linear layer. Default=True\n","        \"\"\"\n","\n","        super(GCN, self).__init__()\n","        # TODO: Initialization\n","        # (1) 2 GraphConvolution() layers. \n","        # (2) 1 Dropout layer\n","        # (3) 1 activation function: ReLU()\n","        ############ Your code here ###################################\n","        self.GCNlayer1 = GraphConvolution(nfeat, n_hidden, bias = bias)\n","        self.GCNlayer2 = GraphConvolution(n_hidden, n_classes, bias = bias)\n","        self.dropout_layer = nn.Dropout(p = dropout)\n","        self.activation_layer = nn.ReLU()\n","\n","\n","        ###############################################################\n","\n","    def forward(self, x, adj):\n","        # TODO: the input will pass through the first graph convolution layer, \n","        # the activation function, the dropout layer, then the second graph \n","        # convolution layer. No activation function for the \n","        # last layer. Return the logits. \n","        ############ Your code here ###################################\n","        GCN1 = self.dropout_layer(self.activation_layer(self.GCNlayer1(x, adj)))\n","        output = self.GCNlayer2(GCN1, adj)\n","        return output\n","\n","        ###############################################################"],"metadata":{"id":"HtVr2cN8jD5t"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IX1d9F1G508r"},"source":["## define loss function"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HyhqJ39OCzNN"},"outputs":[],"source":["criterion = nn.CrossEntropyLoss()"]},{"cell_type":"markdown","metadata":{"id":"vXsdid6C5K1c"},"source":["## training loop"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bjlYeoFPFAWm"},"outputs":[],"source":["args = {\"training_samples\": 140,\n","        \"epochs\": 100,\n","        \"lr\": 0.01,\n","        \"weight_decay\": 5e-4,\n","        \"hidden\": 16,\n","        \"dropout\": 0.5,\n","        \"bias\": True, \n","        }\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qbx0uc-9G5vs"},"outputs":[],"source":["def train(epoch):\n","    t = time.time()\n","    model.train()\n","    optimizer.zero_grad()\n","    output = model(features, adj)\n","    loss_train = criterion(output[idx_train], labels[idx_train])\n","    acc_train = accuracy(output[idx_train], labels[idx_train])\n","    loss_train.backward()\n","    optimizer.step()\n","\n","    model.eval()\n","    output = model(features, adj)\n","\n","    loss_val = criterion(output[idx_val], labels[idx_val])\n","    acc_val = accuracy(output[idx_val], labels[idx_val])\n","    print('Epoch: {:04d}'.format(epoch+1),\n","          'loss_train: {:.4f}'.format(loss_train.item()),\n","          'acc_train: {:.4f}'.format(acc_train.item()),\n","          'loss_val: {:.4f}'.format(loss_val.item()),\n","          'acc_val: {:.4f}'.format(acc_val.item()),\n","          'time: {:.4f}s'.format(time.time() - t))\n","\n","\n","def test():\n","    model.eval()\n","    output = model(features, adj)\n","    loss_test = criterion(output[idx_val], labels[idx_val])\n","    acc_test = accuracy(output[idx_val], labels[idx_val])\n","    print(\"Test set results:\",\n","          \"loss= {:.4f}\".format(loss_test.item()),\n","          \"accuracy= {:.4f}\".format(acc_test.item()))\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TjNiui83FYBr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1648939931433,"user_tz":240,"elapsed":5609,"user":{"displayName":"TONGFEI ZHOU","userId":"15125844894166682257"}},"outputId":"6f952ca1-bcf5-4b09-c50e-4dcf75f4bb5f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Loading cora dataset...\n"]}],"source":["model = GCN(nfeat=features.shape[1],\n","            n_hidden=args[\"hidden\"],\n","            n_classes=labels.max().item() + 1,\n","            dropout=args[\"dropout\"]).to(device)\n","optimizer = optim.Adam(model.parameters(),\n","                       lr=args[\"lr\"], weight_decay=args[\"weight_decay\"])\n","\n","\n","adj, features, labels, idx_train, idx_val = load_data(training_samples=args[\"training_samples\"])\n","adj, features, labels, idx_train, idx_val = adj.to(device), features.to(device), labels.to(device), idx_train.to(device), idx_val.to(device)"]},{"cell_type":"markdown","source":["## training Vanilla GCN"],"metadata":{"id":"1W6tqqj16iz-"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"WSjUYJPSlnOU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1648939934524,"user_tz":240,"elapsed":658,"user":{"displayName":"TONGFEI ZHOU","userId":"15125844894166682257"}},"outputId":"c19b836f-e69f-4d4c-b34c-2132c257631a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 0001 loss_train: 1.9467 acc_train: 0.0571 loss_val: 1.9334 acc_val: 0.0814 time: 0.0627s\n","Epoch: 0002 loss_train: 1.9407 acc_train: 0.0571 loss_val: 1.9285 acc_val: 0.1305 time: 0.0056s\n","Epoch: 0003 loss_train: 1.9355 acc_train: 0.2500 loss_val: 1.9238 acc_val: 0.2083 time: 0.0043s\n","Epoch: 0004 loss_train: 1.9301 acc_train: 0.2786 loss_val: 1.9181 acc_val: 0.3548 time: 0.0052s\n","Epoch: 0005 loss_train: 1.9246 acc_train: 0.2929 loss_val: 1.9123 acc_val: 0.3205 time: 0.0044s\n","Epoch: 0006 loss_train: 1.9184 acc_train: 0.3000 loss_val: 1.9066 acc_val: 0.3092 time: 0.0045s\n","Epoch: 0007 loss_train: 1.9113 acc_train: 0.3214 loss_val: 1.9004 acc_val: 0.3076 time: 0.0046s\n","Epoch: 0008 loss_train: 1.9046 acc_train: 0.3214 loss_val: 1.8939 acc_val: 0.3076 time: 0.0052s\n","Epoch: 0009 loss_train: 1.8970 acc_train: 0.2714 loss_val: 1.8873 acc_val: 0.3072 time: 0.0045s\n","Epoch: 0010 loss_train: 1.8904 acc_train: 0.2714 loss_val: 1.8806 acc_val: 0.3080 time: 0.0046s\n","Epoch: 0011 loss_train: 1.8808 acc_train: 0.3214 loss_val: 1.8739 acc_val: 0.3088 time: 0.0056s\n","Epoch: 0012 loss_train: 1.8752 acc_train: 0.2857 loss_val: 1.8672 acc_val: 0.3119 time: 0.0053s\n","Epoch: 0013 loss_train: 1.8661 acc_train: 0.3214 loss_val: 1.8605 acc_val: 0.3181 time: 0.0049s\n","Epoch: 0014 loss_train: 1.8565 acc_train: 0.3071 loss_val: 1.8538 acc_val: 0.3228 time: 0.0055s\n","Epoch: 0015 loss_train: 1.8436 acc_train: 0.3500 loss_val: 1.8472 acc_val: 0.3287 time: 0.0053s\n","Epoch: 0016 loss_train: 1.8399 acc_train: 0.3357 loss_val: 1.8405 acc_val: 0.3302 time: 0.0047s\n","Epoch: 0017 loss_train: 1.8279 acc_train: 0.3143 loss_val: 1.8338 acc_val: 0.3290 time: 0.0051s\n","Epoch: 0018 loss_train: 1.8249 acc_train: 0.2929 loss_val: 1.8271 acc_val: 0.3248 time: 0.0054s\n","Epoch: 0019 loss_train: 1.8072 acc_train: 0.3429 loss_val: 1.8204 acc_val: 0.3213 time: 0.0050s\n","Epoch: 0020 loss_train: 1.8036 acc_train: 0.3143 loss_val: 1.8139 acc_val: 0.3201 time: 0.0053s\n","Epoch: 0021 loss_train: 1.7977 acc_train: 0.3214 loss_val: 1.8076 acc_val: 0.3189 time: 0.0059s\n","Epoch: 0022 loss_train: 1.7892 acc_train: 0.3714 loss_val: 1.8014 acc_val: 0.3193 time: 0.0049s\n","Epoch: 0023 loss_train: 1.7686 acc_train: 0.3571 loss_val: 1.7952 acc_val: 0.3197 time: 0.0053s\n","Epoch: 0024 loss_train: 1.7684 acc_train: 0.3071 loss_val: 1.7892 acc_val: 0.3228 time: 0.0046s\n","Epoch: 0025 loss_train: 1.7597 acc_train: 0.3357 loss_val: 1.7833 acc_val: 0.3267 time: 0.0052s\n","Epoch: 0026 loss_train: 1.7465 acc_train: 0.3429 loss_val: 1.7772 acc_val: 0.3349 time: 0.0050s\n","Epoch: 0027 loss_train: 1.7340 acc_train: 0.3357 loss_val: 1.7710 acc_val: 0.3411 time: 0.0036s\n","Epoch: 0028 loss_train: 1.7293 acc_train: 0.3357 loss_val: 1.7648 acc_val: 0.3501 time: 0.0046s\n","Epoch: 0029 loss_train: 1.7097 acc_train: 0.3714 loss_val: 1.7584 acc_val: 0.3621 time: 0.0052s\n","Epoch: 0030 loss_train: 1.6897 acc_train: 0.3857 loss_val: 1.7517 acc_val: 0.3711 time: 0.0052s\n","Epoch: 0031 loss_train: 1.7138 acc_train: 0.3571 loss_val: 1.7448 acc_val: 0.3851 time: 0.0053s\n","Epoch: 0032 loss_train: 1.6784 acc_train: 0.4000 loss_val: 1.7376 acc_val: 0.3984 time: 0.0052s\n","Epoch: 0033 loss_train: 1.6838 acc_train: 0.4000 loss_val: 1.7299 acc_val: 0.4120 time: 0.0060s\n","Epoch: 0034 loss_train: 1.6478 acc_train: 0.5000 loss_val: 1.7222 acc_val: 0.4225 time: 0.0057s\n","Epoch: 0035 loss_train: 1.6296 acc_train: 0.4500 loss_val: 1.7141 acc_val: 0.4377 time: 0.0069s\n","Epoch: 0036 loss_train: 1.6311 acc_train: 0.4714 loss_val: 1.7059 acc_val: 0.4533 time: 0.0048s\n","Epoch: 0037 loss_train: 1.5951 acc_train: 0.5000 loss_val: 1.6975 acc_val: 0.4716 time: 0.0074s\n","Epoch: 0038 loss_train: 1.6047 acc_train: 0.5214 loss_val: 1.6885 acc_val: 0.4864 time: 0.0049s\n","Epoch: 0039 loss_train: 1.5695 acc_train: 0.5429 loss_val: 1.6789 acc_val: 0.4965 time: 0.0040s\n","Epoch: 0040 loss_train: 1.5621 acc_train: 0.5000 loss_val: 1.6692 acc_val: 0.5047 time: 0.0061s\n","Epoch: 0041 loss_train: 1.5523 acc_train: 0.5429 loss_val: 1.6587 acc_val: 0.5090 time: 0.0042s\n","Epoch: 0042 loss_train: 1.5370 acc_train: 0.5714 loss_val: 1.6481 acc_val: 0.5125 time: 0.0047s\n","Epoch: 0043 loss_train: 1.4917 acc_train: 0.6000 loss_val: 1.6372 acc_val: 0.5171 time: 0.0042s\n","Epoch: 0044 loss_train: 1.4968 acc_train: 0.6214 loss_val: 1.6265 acc_val: 0.5245 time: 0.0047s\n","Epoch: 0045 loss_train: 1.5022 acc_train: 0.5929 loss_val: 1.6154 acc_val: 0.5319 time: 0.0036s\n","Epoch: 0046 loss_train: 1.4720 acc_train: 0.6143 loss_val: 1.6038 acc_val: 0.5378 time: 0.0048s\n","Epoch: 0047 loss_train: 1.4636 acc_train: 0.6143 loss_val: 1.5922 acc_val: 0.5452 time: 0.0045s\n","Epoch: 0048 loss_train: 1.4289 acc_train: 0.6786 loss_val: 1.5804 acc_val: 0.5549 time: 0.0047s\n","Epoch: 0049 loss_train: 1.4334 acc_train: 0.6286 loss_val: 1.5686 acc_val: 0.5646 time: 0.0046s\n","Epoch: 0050 loss_train: 1.4011 acc_train: 0.6857 loss_val: 1.5563 acc_val: 0.5717 time: 0.0041s\n","Epoch: 0051 loss_train: 1.3635 acc_train: 0.7071 loss_val: 1.5439 acc_val: 0.5857 time: 0.0035s\n","Epoch: 0052 loss_train: 1.3996 acc_train: 0.7000 loss_val: 1.5316 acc_val: 0.5919 time: 0.0040s\n","Epoch: 0053 loss_train: 1.3595 acc_train: 0.7286 loss_val: 1.5194 acc_val: 0.5966 time: 0.0041s\n","Epoch: 0054 loss_train: 1.3452 acc_train: 0.7286 loss_val: 1.5073 acc_val: 0.6032 time: 0.0048s\n","Epoch: 0055 loss_train: 1.3109 acc_train: 0.7143 loss_val: 1.4951 acc_val: 0.6075 time: 0.0046s\n","Epoch: 0056 loss_train: 1.3068 acc_train: 0.7500 loss_val: 1.4827 acc_val: 0.6125 time: 0.0039s\n","Epoch: 0057 loss_train: 1.2916 acc_train: 0.7286 loss_val: 1.4705 acc_val: 0.6168 time: 0.0048s\n","Epoch: 0058 loss_train: 1.2600 acc_train: 0.7071 loss_val: 1.4586 acc_val: 0.6231 time: 0.0040s\n","Epoch: 0059 loss_train: 1.2592 acc_train: 0.7357 loss_val: 1.4471 acc_val: 0.6312 time: 0.0073s\n","Epoch: 0060 loss_train: 1.2407 acc_train: 0.7500 loss_val: 1.4357 acc_val: 0.6398 time: 0.0038s\n","Epoch: 0061 loss_train: 1.1830 acc_train: 0.8143 loss_val: 1.4244 acc_val: 0.6472 time: 0.0047s\n","Epoch: 0062 loss_train: 1.1894 acc_train: 0.7643 loss_val: 1.4138 acc_val: 0.6604 time: 0.0038s\n","Epoch: 0063 loss_train: 1.1512 acc_train: 0.8000 loss_val: 1.4030 acc_val: 0.6729 time: 0.0033s\n","Epoch: 0064 loss_train: 1.1822 acc_train: 0.7857 loss_val: 1.3922 acc_val: 0.6776 time: 0.0042s\n","Epoch: 0065 loss_train: 1.1304 acc_train: 0.7857 loss_val: 1.3811 acc_val: 0.6807 time: 0.0041s\n","Epoch: 0066 loss_train: 1.1508 acc_train: 0.7929 loss_val: 1.3697 acc_val: 0.6826 time: 0.0036s\n","Epoch: 0067 loss_train: 1.0988 acc_train: 0.7786 loss_val: 1.3581 acc_val: 0.6838 time: 0.0042s\n","Epoch: 0068 loss_train: 1.0986 acc_train: 0.7857 loss_val: 1.3465 acc_val: 0.6896 time: 0.0037s\n","Epoch: 0069 loss_train: 1.0639 acc_train: 0.8071 loss_val: 1.3349 acc_val: 0.6970 time: 0.0042s\n","Epoch: 0070 loss_train: 1.0817 acc_train: 0.8143 loss_val: 1.3238 acc_val: 0.7005 time: 0.0036s\n","Epoch: 0071 loss_train: 1.0855 acc_train: 0.8143 loss_val: 1.3132 acc_val: 0.7087 time: 0.0047s\n","Epoch: 0072 loss_train: 1.0570 acc_train: 0.7929 loss_val: 1.3027 acc_val: 0.7122 time: 0.0044s\n","Epoch: 0073 loss_train: 1.0354 acc_train: 0.7714 loss_val: 1.2924 acc_val: 0.7235 time: 0.0048s\n","Epoch: 0074 loss_train: 0.9938 acc_train: 0.8071 loss_val: 1.2827 acc_val: 0.7301 time: 0.0080s\n","Epoch: 0075 loss_train: 0.9770 acc_train: 0.8357 loss_val: 1.2728 acc_val: 0.7336 time: 0.0091s\n","Epoch: 0076 loss_train: 0.9715 acc_train: 0.8357 loss_val: 1.2631 acc_val: 0.7344 time: 0.0050s\n","Epoch: 0077 loss_train: 0.9366 acc_train: 0.8857 loss_val: 1.2534 acc_val: 0.7356 time: 0.0034s\n","Epoch: 0078 loss_train: 0.9232 acc_train: 0.8714 loss_val: 1.2438 acc_val: 0.7383 time: 0.0037s\n","Epoch: 0079 loss_train: 0.9731 acc_train: 0.8643 loss_val: 1.2343 acc_val: 0.7395 time: 0.0047s\n","Epoch: 0080 loss_train: 0.9706 acc_train: 0.8429 loss_val: 1.2250 acc_val: 0.7430 time: 0.0031s\n","Epoch: 0081 loss_train: 0.8910 acc_train: 0.8857 loss_val: 1.2160 acc_val: 0.7445 time: 0.0044s\n","Epoch: 0082 loss_train: 0.9118 acc_train: 0.8643 loss_val: 1.2073 acc_val: 0.7453 time: 0.0039s\n","Epoch: 0083 loss_train: 0.8768 acc_train: 0.8571 loss_val: 1.1988 acc_val: 0.7445 time: 0.0040s\n","Epoch: 0084 loss_train: 0.8998 acc_train: 0.8357 loss_val: 1.1906 acc_val: 0.7500 time: 0.0038s\n","Epoch: 0085 loss_train: 0.9014 acc_train: 0.8500 loss_val: 1.1821 acc_val: 0.7461 time: 0.0040s\n","Epoch: 0086 loss_train: 0.8422 acc_train: 0.8714 loss_val: 1.1735 acc_val: 0.7457 time: 0.0048s\n","Epoch: 0087 loss_train: 0.8767 acc_train: 0.8643 loss_val: 1.1651 acc_val: 0.7461 time: 0.0040s\n","Epoch: 0088 loss_train: 0.8603 acc_train: 0.8571 loss_val: 1.1567 acc_val: 0.7473 time: 0.0046s\n","Epoch: 0089 loss_train: 0.8545 acc_train: 0.8929 loss_val: 1.1489 acc_val: 0.7469 time: 0.0046s\n","Epoch: 0090 loss_train: 0.8295 acc_train: 0.8857 loss_val: 1.1414 acc_val: 0.7461 time: 0.0061s\n","Epoch: 0091 loss_train: 0.7786 acc_train: 0.8929 loss_val: 1.1339 acc_val: 0.7516 time: 0.0036s\n","Epoch: 0092 loss_train: 0.7727 acc_train: 0.9000 loss_val: 1.1261 acc_val: 0.7523 time: 0.0035s\n","Epoch: 0093 loss_train: 0.8176 acc_train: 0.8929 loss_val: 1.1187 acc_val: 0.7543 time: 0.0032s\n","Epoch: 0094 loss_train: 0.7994 acc_train: 0.8571 loss_val: 1.1113 acc_val: 0.7551 time: 0.0046s\n","Epoch: 0095 loss_train: 0.7794 acc_train: 0.8571 loss_val: 1.1043 acc_val: 0.7574 time: 0.0045s\n","Epoch: 0096 loss_train: 0.7903 acc_train: 0.8786 loss_val: 1.0979 acc_val: 0.7570 time: 0.0040s\n","Epoch: 0097 loss_train: 0.6926 acc_train: 0.9286 loss_val: 1.0912 acc_val: 0.7558 time: 0.0033s\n","Epoch: 0098 loss_train: 0.7400 acc_train: 0.9143 loss_val: 1.0845 acc_val: 0.7555 time: 0.0033s\n","Epoch: 0099 loss_train: 0.7521 acc_train: 0.8786 loss_val: 1.0782 acc_val: 0.7543 time: 0.0059s\n","Epoch: 0100 loss_train: 0.7869 acc_train: 0.8571 loss_val: 1.0727 acc_val: 0.7558 time: 0.0039s\n","Optimization Finished!\n","Total time elapsed: 0.6020s\n","Test set results: loss= 1.0727 accuracy= 0.7558\n"]}],"source":["# Train model\n","t_total = time.time()\n","for epoch in range(args[\"epochs\"]):\n","    train(epoch)\n","print(\"Optimization Finished!\")\n","print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n","\n","# evaluating\n","test()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZF3eM6DhHfE_"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XCFwzVLmPXnH"},"outputs":[],"source":[""]},{"cell_type":"markdown","source":["# Graph Attention Networks"],"metadata":{"id":"mKHEyXp1EVdo"}},{"cell_type":"markdown","metadata":{"id":"lx15HdotKnt_"},"source":["## Graph attention layer (Your task)\n","A GAT is made up of multiple such layers. In this section, you will implement a single graph attention layer. Similar to the `GraphConvolution()`, this `GraphAttentionLayer()` module takes $\\mathbf{h} = \\{ \\overrightarrow{h_1}, \\overrightarrow{h_2}, \\dots, \\overrightarrow{h_N} \\}$ where $\\overrightarrow{h_i} \\in \\mathbb{R}^F$ as input and outputs $\\mathbf{h'} = \\{ \\overrightarrow{h'_1}, \\overrightarrow{h'_2}, \\dots, \\overrightarrow{h'_N} \\}$, where $\\overrightarrow{h'_i} \\in \\mathbb{R}^{F'}$. However, instead of weighing each neighbouring node based on the adjacency matrix, we will use self attention to learn the relative importance of each neighbouring node. Recall from HW4 where you are asked to write out the equation for single headed attention, here we will implement multi-headed attention, which involves the following steps: \n","\n","\n","### The initial transformation\n","In GCN above, you have completed similar transformation. But here, we need to define a weight matrix and perform this transformation for each head: $\\overrightarrow{s^k_i} = \\mathbf{W}^k \\overrightarrow{h_i}$. We will perform a single linear transformation and then split it up for each head later. Note the input $\\overrightarrow{h}$ has shape `[n_nodes, in_features]` and $\\overrightarrow{s}$ has shape of `[n_nodes, n_heads * n_hidden]`. Remember to reshape $\\overrightarrow{s}$ has shape of `[n_nodes, n_heads, n_hidden]` for later uses. Note: set `bias=False` for this linear transformation. \n","\n","### attention score\n","We calculate these for each head $k$. Here for simplicity of the notation, we omit $k$ in the following equations. The attention scores are defined as the follows: \n","$$e_{ij} = a(\\mathbf{W} \\overrightarrow{h_i}, \\mathbf{W} \\overrightarrow{h_j}) =a(\\overrightarrow{s_i}, \\overrightarrow{s_j})$$, \n","where $e_{ij}$ is the attention score (importance) of node $j$ to node $i$.\n","We will have to calculate this for each head. $a$ is the attention mechanism, that calculates the attention score. The paper concatenates $\\overrightarrow{s_i}$, $\\overrightarrow{s_j}$ and does a linear transformation with a weight vector $\\mathbf{a} \\in \\mathbb{R}^{2 F'}$ followed by a $\\text{LeakyReLU}$. $$e_{ij} = \\text{LeakyReLU} \\Big(\n","\\mathbf{a}^\\top \\Big[ \\overrightarrow{s_i} \\Vert \\overrightarrow{s_j}  \\Big] \\Big)$$\n","\n","#### How to vectorize this? Some hints: \n","1. `tensor.repeat()` gives you $\\{\\overrightarrow{s_1}, \\overrightarrow{s_2}, \\dots, \\overrightarrow{s_N}, \\overrightarrow{s_1}, \\overrightarrow{s_2}, \\dots, \\overrightarrow{s_N}, ...\\}$.\n","\n","2. `tensor.repeat_interleave()` gives you\n","$\\{\\overrightarrow{s_1}, \\overrightarrow{s_1}, \\dots, \\overrightarrow{s_1}, \\overrightarrow{s_2}, \\overrightarrow{s_2}, \\dots, \\overrightarrow{s_2}, ...\\}$.\n","\n","3. concatenate to get $\\Big[\\overrightarrow{s_i} \\Vert \\overrightarrow{s_j} \\Big]$ for all pairs of $i, j$. Reshape $\\overrightarrow{s_i} \\Vert \\overrightarrow{s_j}$ has shape of `[n_nodes, n_nodes, n_heads, 2 * n_hidden]`\n","\n","4. apply the attention layer and non-linear activation function to get $e_{ij} = \\text{LeakyReLU} \\Big( \\mathbf{a}^\\top \\Big[ \\overrightarrow{s_i} \\Vert \\overrightarrow{s_j}  \\Big] \\Big)$, where $\\mathbf{a}^\\top$ is a single linear transformation that maps from dimension `n_hidden * 2` to `1`. Note: set the `bias=False` for this linear transformation. $\\mathbf{e}$ is of shape `[n_nodes, n_nodes, n_heads, 1]`. Remove the last dimension `1` using `squeeze()`. \n","\n","\n","#### Perform softmax \n","First, we need to mask $e_{ij}$ based on adjacency matrix. We only need to sum over the neighbouring nodes for the attention calculation. Set the elements in $e_{ij}$ to $- \\infty$ if there is no edge from $i$ to $j$ for the softmax calculation. We need to do this for all heads and the adjacency matrix is the same for each head. Use `tensor.masked_fill()` to mask $e_{ij}$ based on adjacency matrix for all heads. Hint: reshape the adjacency matrix to `[n_nodes, n_nodes, 1]` using `unsqueeze()`. \n","Now we are ready to normalize attention scores (or coefficients) $$\\alpha_{ij} = \\text{softmax}_j(e_{ij}) =  \\frac{\\exp(e_{ij})}{\\sum_{k \\in \\mathcal{N}_i} \\exp(e_{ik})}$$\n","\n","#### Apply dropout\n","Apply the dropout layer. (this step is easy)\n","\n","#### Calculate final output for each head\n","$$\\overrightarrow{h'^k_i} = \\sum_{j \\in \\mathcal{N}_i} \\alpha^k_{ij} \\overrightarrow{s^k_j}$$\n","\n","\n","#### Concat or Mean\n","Finally we concateneate the transformed features: $\\overrightarrow{h'_i} = \\Bigg\\Vert_{k=1}^{K} \\overrightarrow{h'^k_i}$. In the code, we only need to reshape the tensor to shape of `[n_nodes, n_heads * n_hidden]`. Note that if it is the final layer, then it doesn't make sense to do concatenation anymore. Instead, we sum over the `n_heads` dimension: $\\overrightarrow{h'_i} = \\frac{1}{K} \\sum_{k=1}^{K} \\overrightarrow{h'^k_i}$. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wVu7rcOuAUZz"},"outputs":[],"source":["class GraphAttentionLayer(nn.Module):\n","\n","    def __init__(self, in_features: int, out_features: int, n_heads: int,\n","                 is_concat: bool = True,\n","                 dropout: float = 0.6,\n","                 alpha: float = 0.2):\n","        \"\"\"\n","        in_features: F, the number of input features per node\n","        out_features: F', the number of output features per node\n","        n_heads: K, the number of attention heads\n","        is_concat: whether the multi-head results should be concatenated or averaged\n","        dropout: the dropout probability\n","        alpha: the negative slope for leaky relu activation\n","        \"\"\"\n","        super(GraphAttentionLayer, self).__init__()\n","\n","        self.is_concat = is_concat\n","        self.n_heads = n_heads\n","\n","        if is_concat:\n","            assert out_features % n_heads == 0\n","            self.n_hidden = out_features // n_heads\n","        else:\n","            self.n_hidden = out_features\n","\n","        # TODO: initialize the following modules: \n","        # (1) self.W: Linear layer that transform the input feature before self attention. \n","        # You should NOT use for loops for the multiheaded implementation (set bias = Flase)\n","        # (2) self.attention: Linear layer that compute the attention score (set bias = Flase)\n","        # (3) self.activation: Activation function (LeakyReLU whith negative_slope=alpha)\n","        # (4) self.softmax: Softmax function (what's the dim to compute the summation?)\n","        # (5) self.dropout_layer: Dropout function(with ratio=dropout)\n","        ################ your code here ########################\n","        self.W = nn.Linear(in_features, self.n_heads * self.n_hidden, bias=False)\n","        self.attention = nn.Linear(2 * self.n_hidden, 1, bias=False)\n","        self.activation = nn.LeakyReLU(negative_slope=alpha)\n","        self.softmax = nn.Softmax(dim=1)\n","        self.dropout_layer = nn.Dropout(dropout)\n","\n","        ########################################################\n","\n","    def forward(self, h: torch.Tensor, adj_mat: torch.Tensor):\n","        # Number of nodes\n","        n_nodes = h.shape[0]\n","        \n","        # TODO: \n","        # (1) calculate s = Wh and reshape it to [n_nodes, n_heads, n_hidden] \n","        #     (you can use tensor.view() function)\n","        # (2) get [s_i || s_j] using tensor.repeat(), repeat_interleave(), torch.cat(), tensor.view()  \n","        # (3) apply the attention layer \n","        # (4) apply the activation layer (you will get the attention score e)\n","        # (5) remove the last dimension 1 use tensor.squeeze()\n","        # (6) mask the attention score with the adjacency matrix (if there's no edge, assign it to -inf)\n","        #     note: check the dimensions of e and your adjacency matrix. You may need to use the function unsqueeze()\n","        # (7) apply softmax \n","        # (8) apply dropout_layer \n","        ############## Your code here #########################################\n","        s = self.W(h).view(n_nodes, self.n_heads, self.n_hidden)\n","        concat_s = torch.cat((s.repeat(n_nodes, 1, 1), s.repeat_interleave(n_nodes, dim=0)), \n","                             dim=-1).view(n_nodes, n_nodes, self.n_heads, 2*self.n_hidden)\n","        e = self.activation(self.attention(concat_s)).squeeze(dim=-1).masked_fill_((adj_mat == 0).unsqueeze(dim=-1), -np.inf)\n","        a = self.dropout_layer(self.softmax(e))\n","\n","\n","        #######################################################################\n","\n","        # Summation \n","        h_prime = torch.einsum('ijh,jhf->ihf', a, s) #[n_nodes, n_heads, n_hidden]\n","\n","\n","        # TODO: Concat or Mean\n","        # Concatenate the heads\n","        if self.is_concat:\n","            ############## Your code here #########################################\n","            return h_prime.reshape(n_nodes, -1)\n","            #######################################################################\n","        # Take the mean of the heads (for the last layer)\n","        else:\n","            ############## Your code here #########################################\n","            return torch.mean(h_prime, dim=1)\n","            #######################################################################\n","\n","\n","\n","\n"]},{"cell_type":"markdown","source":["## Define GAT network\n","it's really similar to how we defined GCN. We followed the paper to use two attention layers and ELU() activation function. "],"metadata":{"id":"YOSk_ZShi2nR"}},{"cell_type":"code","source":["class GAT(nn.Module):\n","\n","    def __init__(self, nfeat: int, n_hidden: int, n_classes: int, n_heads: int, dropout: float, alpha: float):\n","        \"\"\"\n","        in_features: the number of features per node\n","        n_hidden: the number of features in the first graph attention layer\n","        n_classes: the number of classes\n","        n_heads: the number of heads in the graph attention layers\n","        dropout: the dropout probability\n","        alpha: the negative input slope for leaky ReLU of the attention layer\n","        \"\"\"\n","        super().__init__()\n","\n","        # First graph attention layer where we concatenate the heads\n","        self.gc1 = GraphAttentionLayer(nfeat, n_hidden, n_heads, is_concat=True, dropout=dropout, alpha=alpha)\n","        self.gc2 = GraphAttentionLayer(n_hidden, n_classes, 1, is_concat=False, dropout=dropout, alpha=alpha)\n","        self.activation = nn.ELU()  \n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x: torch.Tensor, adj_mat: torch.Tensor):\n","        \"\"\"\n","        x: the features vectors\n","        adj_mat: the adjacency matrix\n","        \"\"\"\n","        x = self.dropout(x)\n","        x = self.gc1(x, adj_mat)\n","        x = self.activation(x)\n","        x = self.dropout(x)\n","        x = self.gc2(x, adj_mat)\n","        return x"],"metadata":{"id":"jKNbUtPVi1Vs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## training GAT"],"metadata":{"id":"CtRQ3Ced7RAw"}},{"cell_type":"code","source":["args = {\"training_samples\": 140,\n","        \"epochs\": 100,\n","        \"lr\": 0.01,\n","        \"weight_decay\": 5e-4,\n","        \"hidden\": 16,\n","        \"dropout\": 0.5,\n","        \"bias\": True, \n","        \"alpha\": 0.2,\n","        \"n_heads\": 2 # changed\n","        }"],"metadata":{"id":"b7D5mYXC6zTG"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7MYaK98hDy7u","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1649122693540,"user_tz":240,"elapsed":4625,"user":{"displayName":"TONGFEI ZHOU","userId":"15125844894166682257"}},"outputId":"e497a58c-305c-45bb-b99e-d324cf4ad1c5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Loading cora dataset...\n"]}],"source":["model = GAT(nfeat=features.shape[1],\n","            n_hidden=args[\"hidden\"],\n","            n_classes=labels.max().item() + 1,\n","            dropout=args[\"dropout\"],\n","            alpha=args[\"alpha\"],\n","            n_heads=args[\"n_heads\"]).to(device)\n","optimizer = optim.Adam(model.parameters(),\n","                       lr=args[\"lr\"], weight_decay=args[\"weight_decay\"])\n","\n","adj, features, labels, idx_train, idx_val = load_data(training_samples=args[\"training_samples\"])\n","adj, features, labels, idx_train, idx_val = adj.to(device), features.to(device), labels.to(device), idx_train.to(device), idx_val.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E9FcfXwMDzEt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1649122707915,"user_tz":240,"elapsed":14387,"user":{"displayName":"TONGFEI ZHOU","userId":"15125844894166682257"}},"outputId":"587f2069-ae03-4486-f273-1f7311370d01"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 0001 loss_train: 1.9454 acc_train: 0.1857 loss_val: 1.9425 acc_val: 0.4248 time: 0.2095s\n","Epoch: 0002 loss_train: 1.9405 acc_train: 0.4714 loss_val: 1.9387 acc_val: 0.4023 time: 0.1455s\n","Epoch: 0003 loss_train: 1.9365 acc_train: 0.4500 loss_val: 1.9346 acc_val: 0.3851 time: 0.1397s\n","Epoch: 0004 loss_train: 1.9278 acc_train: 0.5071 loss_val: 1.9302 acc_val: 0.3719 time: 0.1382s\n","Epoch: 0005 loss_train: 1.9257 acc_train: 0.4357 loss_val: 1.9253 acc_val: 0.3625 time: 0.1395s\n","Epoch: 0006 loss_train: 1.9190 acc_train: 0.4000 loss_val: 1.9203 acc_val: 0.3505 time: 0.1393s\n","Epoch: 0007 loss_train: 1.9102 acc_train: 0.4000 loss_val: 1.9148 acc_val: 0.3361 time: 0.1392s\n","Epoch: 0008 loss_train: 1.8999 acc_train: 0.4143 loss_val: 1.9090 acc_val: 0.3306 time: 0.1400s\n","Epoch: 0009 loss_train: 1.8906 acc_train: 0.4071 loss_val: 1.9028 acc_val: 0.3236 time: 0.1408s\n","Epoch: 0010 loss_train: 1.8798 acc_train: 0.3857 loss_val: 1.8963 acc_val: 0.3217 time: 0.1401s\n","Epoch: 0011 loss_train: 1.8735 acc_train: 0.4071 loss_val: 1.8894 acc_val: 0.3193 time: 0.1397s\n","Epoch: 0012 loss_train: 1.8575 acc_train: 0.3571 loss_val: 1.8822 acc_val: 0.3174 time: 0.1396s\n","Epoch: 0013 loss_train: 1.8581 acc_train: 0.3571 loss_val: 1.8747 acc_val: 0.3162 time: 0.1393s\n","Epoch: 0014 loss_train: 1.8511 acc_train: 0.3500 loss_val: 1.8670 acc_val: 0.3166 time: 0.1396s\n","Epoch: 0015 loss_train: 1.8314 acc_train: 0.4000 loss_val: 1.8591 acc_val: 0.3166 time: 0.1399s\n","Epoch: 0016 loss_train: 1.7984 acc_train: 0.3786 loss_val: 1.8507 acc_val: 0.3174 time: 0.1403s\n","Epoch: 0017 loss_train: 1.7994 acc_train: 0.3929 loss_val: 1.8421 acc_val: 0.3178 time: 0.1385s\n","Epoch: 0018 loss_train: 1.8229 acc_train: 0.3429 loss_val: 1.8335 acc_val: 0.3178 time: 0.1390s\n","Epoch: 0019 loss_train: 1.7851 acc_train: 0.3857 loss_val: 1.8247 acc_val: 0.3185 time: 0.1383s\n","Epoch: 0020 loss_train: 1.7710 acc_train: 0.3357 loss_val: 1.8158 acc_val: 0.3205 time: 0.1388s\n","Epoch: 0021 loss_train: 1.7650 acc_train: 0.3643 loss_val: 1.8067 acc_val: 0.3213 time: 0.1389s\n","Epoch: 0022 loss_train: 1.7686 acc_train: 0.3143 loss_val: 1.7976 acc_val: 0.3228 time: 0.1402s\n","Epoch: 0023 loss_train: 1.7527 acc_train: 0.3857 loss_val: 1.7885 acc_val: 0.3228 time: 0.1403s\n","Epoch: 0024 loss_train: 1.6940 acc_train: 0.4143 loss_val: 1.7792 acc_val: 0.3232 time: 0.1391s\n","Epoch: 0025 loss_train: 1.6962 acc_train: 0.3500 loss_val: 1.7697 acc_val: 0.3232 time: 0.1398s\n","Epoch: 0026 loss_train: 1.7037 acc_train: 0.4000 loss_val: 1.7601 acc_val: 0.3244 time: 0.1408s\n","Epoch: 0027 loss_train: 1.6523 acc_train: 0.4000 loss_val: 1.7505 acc_val: 0.3279 time: 0.1403s\n","Epoch: 0028 loss_train: 1.6982 acc_train: 0.3571 loss_val: 1.7408 acc_val: 0.3294 time: 0.1397s\n","Epoch: 0029 loss_train: 1.6346 acc_train: 0.3786 loss_val: 1.7311 acc_val: 0.3329 time: 0.1405s\n","Epoch: 0030 loss_train: 1.6451 acc_train: 0.3500 loss_val: 1.7216 acc_val: 0.3376 time: 0.1410s\n","Epoch: 0031 loss_train: 1.6052 acc_train: 0.4000 loss_val: 1.7121 acc_val: 0.3407 time: 0.1406s\n","Epoch: 0032 loss_train: 1.5992 acc_train: 0.4286 loss_val: 1.7023 acc_val: 0.3454 time: 0.1406s\n","Epoch: 0033 loss_train: 1.5626 acc_train: 0.4286 loss_val: 1.6925 acc_val: 0.3493 time: 0.1400s\n","Epoch: 0034 loss_train: 1.5874 acc_train: 0.3857 loss_val: 1.6828 acc_val: 0.3528 time: 0.1394s\n","Epoch: 0035 loss_train: 1.6308 acc_train: 0.4000 loss_val: 1.6734 acc_val: 0.3590 time: 0.1399s\n","Epoch: 0036 loss_train: 1.6058 acc_train: 0.3929 loss_val: 1.6640 acc_val: 0.3668 time: 0.1399s\n","Epoch: 0037 loss_train: 1.5515 acc_train: 0.4214 loss_val: 1.6544 acc_val: 0.3719 time: 0.1394s\n","Epoch: 0038 loss_train: 1.5116 acc_train: 0.4500 loss_val: 1.6449 acc_val: 0.3836 time: 0.1404s\n","Epoch: 0039 loss_train: 1.5249 acc_train: 0.4286 loss_val: 1.6352 acc_val: 0.3925 time: 0.1383s\n","Epoch: 0040 loss_train: 1.5255 acc_train: 0.4357 loss_val: 1.6256 acc_val: 0.4046 time: 0.1397s\n","Epoch: 0041 loss_train: 1.5686 acc_train: 0.4214 loss_val: 1.6160 acc_val: 0.4151 time: 0.1396s\n","Epoch: 0042 loss_train: 1.4546 acc_train: 0.4929 loss_val: 1.6063 acc_val: 0.4252 time: 0.1396s\n","Epoch: 0043 loss_train: 1.5590 acc_train: 0.5143 loss_val: 1.5973 acc_val: 0.4408 time: 0.1400s\n","Epoch: 0044 loss_train: 1.4620 acc_train: 0.5429 loss_val: 1.5883 acc_val: 0.4560 time: 0.1406s\n","Epoch: 0045 loss_train: 1.4458 acc_train: 0.5643 loss_val: 1.5793 acc_val: 0.4724 time: 0.1388s\n","Epoch: 0046 loss_train: 1.4521 acc_train: 0.5286 loss_val: 1.5705 acc_val: 0.4965 time: 0.1408s\n","Epoch: 0047 loss_train: 1.4234 acc_train: 0.6143 loss_val: 1.5617 acc_val: 0.5249 time: 0.1393s\n","Epoch: 0048 loss_train: 1.3850 acc_train: 0.5786 loss_val: 1.5529 acc_val: 0.5549 time: 0.1391s\n","Epoch: 0049 loss_train: 1.4147 acc_train: 0.5929 loss_val: 1.5440 acc_val: 0.5740 time: 0.1396s\n","Epoch: 0050 loss_train: 1.3793 acc_train: 0.6571 loss_val: 1.5350 acc_val: 0.5927 time: 0.1396s\n","Epoch: 0051 loss_train: 1.4726 acc_train: 0.6286 loss_val: 1.5261 acc_val: 0.6164 time: 0.1395s\n","Epoch: 0052 loss_train: 1.4238 acc_train: 0.6143 loss_val: 1.5174 acc_val: 0.6336 time: 0.1413s\n","Epoch: 0053 loss_train: 1.3603 acc_train: 0.7000 loss_val: 1.5088 acc_val: 0.6472 time: 0.1398s\n","Epoch: 0054 loss_train: 1.3621 acc_train: 0.7071 loss_val: 1.5000 acc_val: 0.6589 time: 0.1394s\n","Epoch: 0055 loss_train: 1.3018 acc_train: 0.7143 loss_val: 1.4908 acc_val: 0.6686 time: 0.1391s\n","Epoch: 0056 loss_train: 1.3022 acc_train: 0.6929 loss_val: 1.4816 acc_val: 0.6764 time: 0.1392s\n","Epoch: 0057 loss_train: 1.3461 acc_train: 0.7071 loss_val: 1.4727 acc_val: 0.6838 time: 0.1389s\n","Epoch: 0058 loss_train: 1.3154 acc_train: 0.6714 loss_val: 1.4643 acc_val: 0.6893 time: 0.1389s\n","Epoch: 0059 loss_train: 1.3955 acc_train: 0.6429 loss_val: 1.4560 acc_val: 0.6939 time: 0.1398s\n","Epoch: 0060 loss_train: 1.3166 acc_train: 0.7429 loss_val: 1.4476 acc_val: 0.7021 time: 0.1397s\n","Epoch: 0061 loss_train: 1.2753 acc_train: 0.7500 loss_val: 1.4392 acc_val: 0.7072 time: 0.1401s\n","Epoch: 0062 loss_train: 1.3629 acc_train: 0.7000 loss_val: 1.4310 acc_val: 0.7103 time: 0.1389s\n","Epoch: 0063 loss_train: 1.2322 acc_train: 0.7714 loss_val: 1.4228 acc_val: 0.7138 time: 0.1386s\n","Epoch: 0064 loss_train: 1.2381 acc_train: 0.7500 loss_val: 1.4146 acc_val: 0.7165 time: 0.1403s\n","Epoch: 0065 loss_train: 1.3434 acc_train: 0.7357 loss_val: 1.4064 acc_val: 0.7161 time: 0.1394s\n","Epoch: 0066 loss_train: 1.2589 acc_train: 0.7143 loss_val: 1.3979 acc_val: 0.7220 time: 0.1403s\n","Epoch: 0067 loss_train: 1.2845 acc_train: 0.7071 loss_val: 1.3894 acc_val: 0.7231 time: 0.1400s\n","Epoch: 0068 loss_train: 1.2082 acc_train: 0.7643 loss_val: 1.3807 acc_val: 0.7251 time: 0.1398s\n","Epoch: 0069 loss_train: 1.2230 acc_train: 0.7429 loss_val: 1.3717 acc_val: 0.7270 time: 0.1394s\n","Epoch: 0070 loss_train: 1.1530 acc_train: 0.7286 loss_val: 1.3624 acc_val: 0.7286 time: 0.1394s\n","Epoch: 0071 loss_train: 1.1361 acc_train: 0.7714 loss_val: 1.3528 acc_val: 0.7301 time: 0.1404s\n","Epoch: 0072 loss_train: 1.1435 acc_train: 0.7857 loss_val: 1.3430 acc_val: 0.7301 time: 0.1407s\n","Epoch: 0073 loss_train: 1.1622 acc_train: 0.7500 loss_val: 1.3335 acc_val: 0.7317 time: 0.1404s\n","Epoch: 0074 loss_train: 1.2677 acc_train: 0.7214 loss_val: 1.3247 acc_val: 0.7321 time: 0.1408s\n","Epoch: 0075 loss_train: 1.2421 acc_train: 0.7000 loss_val: 1.3160 acc_val: 0.7344 time: 0.1411s\n","Epoch: 0076 loss_train: 1.1593 acc_train: 0.6714 loss_val: 1.3072 acc_val: 0.7371 time: 0.1398s\n","Epoch: 0077 loss_train: 1.0967 acc_train: 0.7714 loss_val: 1.2984 acc_val: 0.7383 time: 0.1396s\n","Epoch: 0078 loss_train: 1.1649 acc_train: 0.7357 loss_val: 1.2897 acc_val: 0.7375 time: 0.1399s\n","Epoch: 0079 loss_train: 1.1445 acc_train: 0.7500 loss_val: 1.2812 acc_val: 0.7395 time: 0.1404s\n","Epoch: 0080 loss_train: 1.1564 acc_train: 0.7214 loss_val: 1.2728 acc_val: 0.7410 time: 0.1394s\n","Epoch: 0081 loss_train: 1.1857 acc_train: 0.7286 loss_val: 1.2645 acc_val: 0.7418 time: 0.1407s\n","Epoch: 0082 loss_train: 1.0459 acc_train: 0.8214 loss_val: 1.2563 acc_val: 0.7442 time: 0.1407s\n","Epoch: 0083 loss_train: 1.1400 acc_train: 0.7500 loss_val: 1.2485 acc_val: 0.7461 time: 0.1407s\n","Epoch: 0084 loss_train: 1.0564 acc_train: 0.7857 loss_val: 1.2408 acc_val: 0.7469 time: 0.1396s\n","Epoch: 0085 loss_train: 1.0582 acc_train: 0.7571 loss_val: 1.2328 acc_val: 0.7481 time: 0.1396s\n","Epoch: 0086 loss_train: 1.0974 acc_train: 0.7643 loss_val: 1.2249 acc_val: 0.7508 time: 0.1401s\n","Epoch: 0087 loss_train: 1.1905 acc_train: 0.6857 loss_val: 1.2186 acc_val: 0.7531 time: 0.1399s\n","Epoch: 0088 loss_train: 1.0615 acc_train: 0.7786 loss_val: 1.2121 acc_val: 0.7578 time: 0.1403s\n","Epoch: 0089 loss_train: 1.0843 acc_train: 0.7071 loss_val: 1.2055 acc_val: 0.7590 time: 0.1400s\n","Epoch: 0090 loss_train: 1.0586 acc_train: 0.7643 loss_val: 1.1991 acc_val: 0.7609 time: 0.1399s\n","Epoch: 0091 loss_train: 1.1042 acc_train: 0.7143 loss_val: 1.1924 acc_val: 0.7625 time: 0.1407s\n","Epoch: 0092 loss_train: 0.9983 acc_train: 0.7571 loss_val: 1.1853 acc_val: 0.7644 time: 0.1400s\n","Epoch: 0093 loss_train: 1.0241 acc_train: 0.7929 loss_val: 1.1784 acc_val: 0.7648 time: 0.1399s\n","Epoch: 0094 loss_train: 0.9673 acc_train: 0.7714 loss_val: 1.1715 acc_val: 0.7656 time: 0.1402s\n","Epoch: 0095 loss_train: 0.9742 acc_train: 0.7357 loss_val: 1.1645 acc_val: 0.7660 time: 0.1393s\n","Epoch: 0096 loss_train: 1.0267 acc_train: 0.7357 loss_val: 1.1575 acc_val: 0.7679 time: 0.1407s\n","Epoch: 0097 loss_train: 1.0319 acc_train: 0.7714 loss_val: 1.1504 acc_val: 0.7652 time: 0.1394s\n","Epoch: 0098 loss_train: 0.9416 acc_train: 0.8000 loss_val: 1.1433 acc_val: 0.7652 time: 0.1411s\n","Epoch: 0099 loss_train: 0.9958 acc_train: 0.8000 loss_val: 1.1360 acc_val: 0.7648 time: 0.1408s\n","Epoch: 0100 loss_train: 0.9740 acc_train: 0.7357 loss_val: 1.1286 acc_val: 0.7644 time: 0.1410s\n","Optimization Finished!\n","Total time elapsed: 14.1210s\n","Test set results: loss= 1.1286 accuracy= 0.7644\n"]}],"source":["# Train model\n","t_total = time.time()\n","for epoch in range(args[\"epochs\"]):\n","    train(epoch)\n","print(\"Optimization Finished!\")\n","print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n","\n","# Testing\n","test()"]},{"cell_type":"markdown","source":["# Question: (Your task)\n","Compare the evaluation results for Vanilla GCN and GAT. Comment on the discrepancy in their performance (if any) and briefly explain why you think it's the case (in 1-2 sentences). "],"metadata":{"id":"n6Ox3fbTG7rc"}},{"cell_type":"markdown","source":["The reason for GAT performing slightly more accurate than the vanilla GCN is that in the GAT we are utilizing the attention mechanism so that we can gather more context information when we do the classification task on the node-level."],"metadata":{"id":"btFL8bjabzKw"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"urJ8Q-neDzHU"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vZhHh8k4DzJu"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KmvJ46OfGlf2"},"outputs":[],"source":[""]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"Copy of a4_GCN.ipynb","provenance":[{"file_id":"https://github.com/uoft-csc413/2022/blob/master/assets/assignments/a4_GCN.ipynb","timestamp":1648600942647}],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}