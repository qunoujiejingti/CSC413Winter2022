{"cells":[{"cell_type":"markdown","metadata":{"id":"TjPTaRB4mpCd"},"source":["# Colab FAQ\n","\n","For some basic overview and features offered in Colab notebooks, check out: [Overview of Colaboratory Features](https://colab.research.google.com/notebooks/basic_features_overview.ipynb)\n","\n","You need to use the colab GPU for this assignment by selecting:\n","\n","> **Runtime**   →   **Change runtime type**   →   **Hardware Accelerator: GPU**"]},{"cell_type":"markdown","metadata":{"id":"s9IS9B9-yUU5"},"source":["# Setup PyTorch\n","\n","All files will be stored at /content/csc421/a3/ folder\n"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"Z-6MQhMOlHXD","colab":{"base_uri":"https://localhost:8080/"},"cellView":"code","executionInfo":{"status":"ok","timestamp":1647459944908,"user_tz":240,"elapsed":3772,"user":{"displayName":"TONGFEI ZHOU","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15125844894166682257"}},"outputId":"37b2c00a-c68c-4efa-a721-33552367218a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (7.1.2)\n","/content/content/csc421/a3\n"]}],"source":["######################################################################\n","# Setup python environment and change the current working directory\n","######################################################################\n","!pip install Pillow\n","%mkdir -p ./content/csc421/a3/\n","%cd ./content/csc421/a3"]},{"cell_type":"markdown","metadata":{"id":"9DaTdRNuUra7"},"source":["# Helper code"]},{"cell_type":"markdown","metadata":{"id":"4BIpGwANoQOg"},"source":["## Utility functions"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"D-UJHBYZkh7f","executionInfo":{"status":"ok","timestamp":1647459951513,"user_tz":240,"elapsed":6627,"user":{"displayName":"TONGFEI ZHOU","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15125844894166682257"}}},"outputs":[],"source":["%matplotlib inline\n","\n","import os\n","import pdb\n","import argparse\n","import pickle as pkl\n","from pathlib import Path\n","\n","from collections import defaultdict\n","\n","import numpy as np\n","import matplotlib as mpl\n","import matplotlib.pyplot as plt\n","import matplotlib.ticker as ticker\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torch.autograd import Variable\n","\n","from six.moves.urllib.request import urlretrieve\n","import tarfile\n","import pickle\n","import sys\n","\n","\n","def get_file(\n","    fname, origin, untar=False, extract=False, archive_format=\"auto\", cache_dir=\"data\"\n","):\n","    datadir = os.path.join(cache_dir)\n","    if not os.path.exists(datadir):\n","        os.makedirs(datadir)\n","\n","    if untar:\n","        untar_fpath = os.path.join(datadir, fname)\n","        fpath = untar_fpath + \".tar.gz\"\n","    else:\n","        fpath = os.path.join(datadir, fname)\n","\n","    print(fpath)\n","    if not os.path.exists(fpath):\n","        print(\"Downloading data from\", origin)\n","\n","        error_msg = \"URL fetch failure on {}: {} -- {}\"\n","        try:\n","            try:\n","                urlretrieve(origin, fpath)\n","            except URLError as e:\n","                raise Exception(error_msg.format(origin, e.errno, e.reason))\n","            except HTTPError as e:\n","                raise Exception(error_msg.format(origin, e.code, e.msg))\n","        except (Exception, KeyboardInterrupt) as e:\n","            if os.path.exists(fpath):\n","                os.remove(fpath)\n","            raise\n","\n","    if untar:\n","        if not os.path.exists(untar_fpath):\n","            print(\"Extracting file.\")\n","            with tarfile.open(fpath) as archive:\n","                archive.extractall(datadir)\n","        return untar_fpath\n","\n","    if extract:\n","        _extract_archive(fpath, datadir, archive_format)\n","\n","    return fpath\n","\n","\n","class AttrDict(dict):\n","    def __init__(self, *args, **kwargs):\n","        super(AttrDict, self).__init__(*args, **kwargs)\n","        self.__dict__ = self\n","\n","\n","def to_var(tensor, cuda):\n","    \"\"\"Wraps a Tensor in a Variable, optionally placing it on the GPU.\n","\n","    Arguments:\n","        tensor: A Tensor object.\n","        cuda: A boolean flag indicating whether to use the GPU.\n","\n","    Returns:\n","        A Variable object, on the GPU if cuda==True.\n","    \"\"\"\n","    if cuda:\n","        return Variable(tensor.cuda())\n","    else:\n","        return Variable(tensor)\n","\n","\n","def create_dir_if_not_exists(directory):\n","    \"\"\"Creates a directory if it doesn't already exist.\"\"\"\n","    if not os.path.exists(directory):\n","        os.makedirs(directory)\n","\n","\n","def save_loss_plot(train_losses, val_losses, opts):\n","    \"\"\"Saves a plot of the training and validation loss curves.\"\"\"\n","    plt.figure()\n","    plt.plot(range(len(train_losses)), train_losses)\n","    plt.plot(range(len(val_losses)), val_losses)\n","    plt.title(\"BS={}, nhid={}\".format(opts.batch_size, opts.hidden_size), fontsize=20)\n","    plt.xlabel(\"Epochs\", fontsize=16)\n","    plt.ylabel(\"Loss\", fontsize=16)\n","    plt.xticks(fontsize=14)\n","    plt.yticks(fontsize=14)\n","    plt.tight_layout()\n","    plt.savefig(os.path.join(opts.checkpoint_path, \"loss_plot.pdf\"))\n","    plt.close()\n","\n","\n","def save_loss_comparison_gru(l1, l2, o1, o2, fn, s=500):\n","    \"\"\"Plot comparison of training and val loss curves from GRU runs.\n","\n","    Arguments:\n","        l1: Tuple of lists containing training / val losses for model 1.\n","        l2: Tuple of lists containing training / val losses for model 2.\n","        o1: Options for model 1.\n","        o2: Options for model 2.\n","        fn: Output file name.\n","        s: Number of training iterations to average over.\n","    \"\"\"\n","    mean_l1 = [np.mean(l1[0][i * s : (i + 1) * s]) for i in range(len(l1[0]) // s)]\n","    mean_l2 = [np.mean(l2[0][i * s : (i + 1) * s]) for i in range(len(l2[0]) // s)]\n","\n","    plt.figure()\n","\n","    fig, ax = plt.subplots(1, 2, figsize=(10, 4))\n","\n","    ax[0].plot(range(len(mean_l1)), mean_l1, label=\"ds=\" + o1.data_file_name)\n","    ax[0].plot(range(len(mean_l2)), mean_l2, label=\"ds=\" + o2.data_file_name)\n","    ax[0].title.set_text(\"Train Loss | GRU Hidden Size = {}\".format(o2.hidden_size))\n","\n","    # Validation losses are assumed to be by epoch\n","    ax[1].plot(range(len(l1[1])), l1[1], label=\"ds=\" + o1.data_file_name)\n","    ax[1].plot(range(len(l2[1])), l2[1], label=\"ds=\" + o2.data_file_name)\n","    ax[1].title.set_text(\"Val Loss | GRU Hidden Size = {}\".format(o2.hidden_size))\n","\n","    ax[0].set_xlabel(\"Iterations (x{})\".format(s), fontsize=10)\n","    ax[0].set_ylabel(\"Loss\", fontsize=10)\n","    ax[1].set_xlabel(\"Epochs\", fontsize=10)\n","    ax[1].set_ylabel(\"Loss\", fontsize=10)\n","    ax[0].legend(loc=\"upper right\")\n","    ax[1].legend(loc=\"upper right\")\n","\n","    fig.suptitle(\"GRU Performance by Dataset\", fontsize=14)\n","    plt.tight_layout()\n","    fig.subplots_adjust(top=0.85)\n","    plt.legend()\n","\n","    plt_path = \"./loss_plot_{}.pdf\".format(fn)\n","    plt.savefig(plt_path)\n","    print(f\"Plot saved to: {Path(plt_path).resolve()}\")\n","\n","\n","def save_loss_comparison_by_dataset(l1, l2, l3, l4, o1, o2, o3, o4, fn, s=500):\n","    \"\"\"Plot comparison of training and validation loss curves from all four\n","    runs in Part 3, comparing by dataset while holding hidden size constant.\n","\n","    Models within each pair (l1, l2) and (l3, l4) have the same hidden sizes.\n","\n","    Arguments:\n","        l1: Tuple of lists containing training / val losses for model 1.\n","        l2: Tuple of lists containing training / val losses for model 2.\n","        l3: Tuple of lists containing training / val losses for model 3.\n","        l4: Tuple of lists containing training / val losses for model 4.\n","        o1: Options for model 1.\n","        o2: Options for model 2.\n","        o3: Options for model 3.\n","        o4: Options for model 4.\n","        fn: Output file name.\n","        s: Number of training iterations to average over.\n","    \"\"\"\n","    mean_l1 = [np.mean(l1[0][i * s : (i + 1) * s]) for i in range(len(l1[0]) // s)]\n","    mean_l2 = [np.mean(l2[0][i * s : (i + 1) * s]) for i in range(len(l2[0]) // s)]\n","    mean_l3 = [np.mean(l3[0][i * s : (i + 1) * s]) for i in range(len(l3[0]) // s)]\n","    mean_l4 = [np.mean(l4[0][i * s : (i + 1) * s]) for i in range(len(l4[0]) // s)]\n","\n","    plt.figure()\n","    fig, ax = plt.subplots(2, 2, figsize=(10, 8))\n","\n","    ax[0][0].plot(range(len(mean_l1)), mean_l1, label=\"ds=\" + o1.data_file_name)\n","    ax[0][0].plot(range(len(mean_l2)), mean_l2, label=\"ds=\" + o2.data_file_name)\n","    ax[0][0].title.set_text(\n","        \"Train Loss | Model Hidden Size = {}\".format(o1.hidden_size)\n","    )\n","\n","    # Validation losses are assumed to be by epoch\n","    ax[0][1].plot(range(len(l1[1])), l1[1], label=\"ds=\" + o1.data_file_name)\n","    ax[0][1].plot(range(len(l2[1])), l2[1], label=\"ds=\" + o2.data_file_name)\n","    ax[0][1].title.set_text(\"Val Loss | Model Hidden Size = {}\".format(o1.hidden_size))\n","\n","    ax[1][0].plot(range(len(mean_l3)), mean_l3, label=\"ds=\" + o3.data_file_name)\n","    ax[1][0].plot(range(len(mean_l4)), mean_l4, label=\"ds=\" + o4.data_file_name)\n","    ax[1][0].title.set_text(\n","        \"Train Loss | Model Hidden Size = {}\".format(o3.hidden_size)\n","    )\n","\n","    ax[1][1].plot(range(len(l3[1])), l3[1], label=\"ds=\" + o3.data_file_name)\n","    ax[1][1].plot(range(len(l4[1])), l4[1], label=\"ds=\" + o4.data_file_name)\n","    ax[1][1].title.set_text(\"Val Loss | Model Hidden Size = {}\".format(o4.hidden_size))\n","\n","    for i in range(2):\n","        ax[i][0].set_xlabel(\"Iterations (x{})\".format(s), fontsize=10)\n","        ax[i][0].set_ylabel(\"Loss\", fontsize=10)\n","        ax[i][1].set_xlabel(\"Epochs\", fontsize=10)\n","        ax[i][1].set_ylabel(\"Loss\", fontsize=10)\n","        ax[i][0].legend(loc=\"upper right\")\n","        ax[i][1].legend(loc=\"upper right\")\n","\n","    fig.suptitle(\"Performance by Dataset Size\", fontsize=16)\n","    plt.tight_layout()\n","    fig.subplots_adjust(top=0.9)\n","    plt.legend()\n","    plt.savefig(\"./loss_plot_{}.pdf\".format(fn))\n","    plt.close()\n","\n","\n","def save_loss_comparison_by_hidden(l1, l2, l3, l4, o1, o2, o3, o4, fn, s=500):\n","    \"\"\"Plot comparison of training and validation loss curves from all four\n","    runs in Part 3, comparing by hidden size while holding dataset constant.\n","\n","    Models within each pair (l1, l3) and (l2, l4) have the same dataset.\n","\n","    Arguments:\n","        l1: Tuple of lists containing training / val losses for model 1.\n","        l2: Tuple of lists containing training / val losses for model 2.\n","        l3: Tuple of lists containing training / val losses for model 3.\n","        l4: Tuple of lists containing training / val losses for model 4.\n","        o1: Options for model 1.\n","        o2: Options for model 2.\n","        o3: Options for model 3.\n","        o4: Options for model 4.\n","        fn: Output file name.\n","        s: Number of training iterations to average over.\n","    \"\"\"\n","    mean_l1 = [np.mean(l1[0][i * s : (i + 1) * s]) for i in range(len(l1[0]) // s)]\n","    mean_l2 = [np.mean(l2[0][i * s : (i + 1) * s]) for i in range(len(l2[0]) // s)]\n","    mean_l3 = [np.mean(l3[0][i * s : (i + 1) * s]) for i in range(len(l3[0]) // s)]\n","    mean_l4 = [np.mean(l4[0][i * s : (i + 1) * s]) for i in range(len(l4[0]) // s)]\n","\n","    plt.figure()\n","    fig, ax = plt.subplots(2, 2, figsize=(10, 8))\n","\n","    ax[0][0].plot(range(len(mean_l1)), mean_l1, label=\"hid_size=\" + str(o1.hidden_size))\n","    ax[0][0].plot(range(len(mean_l3)), mean_l3, label=\"hid_size=\" + str(o3.hidden_size))\n","    ax[0][0].title.set_text(\"Train Loss | Dataset = \" + o1.data_file_name)\n","\n","    # Validation losses are assumed to be by epoch\n","    ax[0][1].plot(range(len(l1[1])), l1[1], label=\"hid_size=\" + str(o1.hidden_size))\n","    ax[0][1].plot(range(len(l3[1])), l3[1], label=\"hid_size=\" + str(o3.hidden_size))\n","    ax[0][1].title.set_text(\"Val Loss | Dataset = \" + o1.data_file_name)\n","\n","    ax[1][0].plot(range(len(mean_l2)), mean_l2, label=\"hid_size=\" + str(o2.hidden_size))\n","    ax[1][0].plot(range(len(mean_l4)), mean_l4, label=\"hid_size=\" + str(o4.hidden_size))\n","    ax[1][0].title.set_text(\"Train Loss | Dataset = \" + o3.data_file_name)\n","\n","    ax[1][1].plot(range(len(l2[1])), l2[1], label=\"hid_size=\" + str(o2.hidden_size))\n","    ax[1][1].plot(range(len(l4[1])), l4[1], label=\"hid_size=\" + str(o4.hidden_size))\n","    ax[1][1].title.set_text(\"Val Loss | Dataset = \" + o4.data_file_name)\n","\n","    for i in range(2):\n","        ax[i][0].set_xlabel(\"Iterations (x{})\".format(s), fontsize=10)\n","        ax[i][0].set_ylabel(\"Loss\", fontsize=10)\n","        ax[i][1].set_xlabel(\"Epochs\", fontsize=10)\n","        ax[i][1].set_ylabel(\"Loss\", fontsize=10)\n","        ax[i][0].legend(loc=\"upper right\")\n","        ax[i][1].legend(loc=\"upper right\")\n","\n","    fig.suptitle(\"Performance by Hidden State Size\", fontsize=16)\n","    plt.tight_layout()\n","    fig.subplots_adjust(top=0.9)\n","    plt.legend()\n","    plt.savefig(\"./loss_plot_{}.pdf\".format(fn))\n","    plt.close()\n","\n","\n","def checkpoint(encoder, decoder, idx_dict, opts):\n","    \"\"\"Saves the current encoder and decoder models, along with idx_dict, which\n","    contains the char_to_index and index_to_char mappings, and the start_token\n","    and end_token values.\n","    \"\"\"\n","    with open(os.path.join(opts.checkpoint_path, \"encoder.pt\"), \"wb\") as f:\n","        torch.save(encoder, f)\n","\n","    with open(os.path.join(opts.checkpoint_path, \"decoder.pt\"), \"wb\") as f:\n","        torch.save(decoder, f)\n","\n","    with open(os.path.join(opts.checkpoint_path, \"idx_dict.pkl\"), \"wb\") as f:\n","        pkl.dump(idx_dict, f)"]},{"cell_type":"markdown","metadata":{"id":"pbvpn4MaV0I1"},"source":["## Data loader"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"XVT4TNTOV3Eg","executionInfo":{"status":"ok","timestamp":1647459951516,"user_tz":240,"elapsed":35,"user":{"displayName":"TONGFEI ZHOU","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15125844894166682257"}}},"outputs":[],"source":["def read_lines(filename):\n","    \"\"\"Read a file and split it into lines.\"\"\"\n","    lines = open(filename).read().strip().lower().split(\"\\n\")\n","    return lines\n","\n","\n","def read_pairs(filename):\n","    \"\"\"Reads lines that consist of two words, separated by a space.\n","\n","    Returns:\n","        source_words: A list of the first word in each line of the file.\n","        target_words: A list of the second word in each line of the file.\n","    \"\"\"\n","    lines = read_lines(filename)\n","    source_words, target_words = [], []\n","    for line in lines:\n","        line = line.strip()\n","        if line:\n","            source, target = line.split()\n","            source_words.append(source)\n","            target_words.append(target)\n","    return source_words, target_words\n","\n","\n","def all_alpha_or_dash(s):\n","    \"\"\"Helper function to check whether a string is alphabetic, allowing dashes '-'.\"\"\"\n","    return all(c.isalpha() or c == \"-\" for c in s)\n","\n","\n","def filter_lines(lines):\n","    \"\"\"Filters lines to consist of only alphabetic characters or dashes \"-\".\"\"\"\n","    return [line for line in lines if all_alpha_or_dash(line)]\n","\n","\n","def load_data(file_name):\n","    \"\"\"Loads (English, Pig-Latin) word pairs, and creates mappings from characters to indexes.\"\"\"\n","    path = \"./data/{}.txt\".format(file_name)\n","    source_lines, target_lines = read_pairs(path)\n","\n","    # Filter lines\n","    source_lines = filter_lines(source_lines)\n","    target_lines = filter_lines(target_lines)\n","\n","    all_characters = set(\"\".join(source_lines)) | set(\"\".join(target_lines))\n","\n","    # Create a dictionary mapping each character to a unique index\n","    char_to_index = {\n","        char: index for (index, char) in enumerate(sorted(list(all_characters)))\n","    }\n","\n","    # Add start and end tokens to the dictionary\n","    start_token = len(char_to_index)\n","    end_token = len(char_to_index) + 1\n","    char_to_index[\"SOS\"] = start_token\n","    char_to_index[\"EOS\"] = end_token\n","\n","    # Create the inverse mapping, from indexes to characters (used to decode the model's predictions)\n","    index_to_char = {index: char for (char, index) in char_to_index.items()}\n","\n","    # Store the final size of the vocabulary\n","    vocab_size = len(char_to_index)\n","\n","    line_pairs = list(set(zip(source_lines, target_lines)))  # Python 3\n","\n","    idx_dict = {\n","        \"char_to_index\": char_to_index,\n","        \"index_to_char\": index_to_char,\n","        \"start_token\": start_token,\n","        \"end_token\": end_token,\n","    }\n","\n","    return line_pairs, vocab_size, idx_dict\n","\n","\n","def create_dict(pairs):\n","    \"\"\"Creates a mapping { (source_length, target_length): [list of (source, target) pairs]\n","    This is used to make batches: each batch consists of two parallel tensors, one containing\n","    all source indexes and the other containing all corresponding target indexes.\n","    Within a batch, all the source words are the same length, and all the target words are\n","    the same length.\n","    \"\"\"\n","    unique_pairs = list(set(pairs))  # Find all unique (source, target) pairs\n","\n","    d = defaultdict(list)\n","    for (s, t) in unique_pairs:\n","        d[(len(s), len(t))].append((s, t))\n","\n","    return d"]},{"cell_type":"markdown","metadata":{"id":"bRWfRdmVVjUl"},"source":["## Training and evaluation code"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"wa5-onJhoSeM","executionInfo":{"status":"ok","timestamp":1647459952347,"user_tz":240,"elapsed":862,"user":{"displayName":"TONGFEI ZHOU","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15125844894166682257"}}},"outputs":[],"source":["def string_to_index_list(s, char_to_index, end_token):\n","    \"\"\"Converts a sentence into a list of indexes (for each character).\"\"\"\n","    return [char_to_index[char] for char in s] + [\n","        end_token\n","    ]  # Adds the end token to each index list\n","\n","\n","def translate_sentence(sentence, encoder, decoder, idx_dict, opts):\n","    \"\"\"Translates a sentence from English to Pig-Latin, by splitting the sentence into\n","    words (whitespace-separated), running the encoder-decoder model to translate each\n","    word independently, and then stitching the words back together with spaces between them.\n","    \"\"\"\n","    if idx_dict is None:\n","        line_pairs, vocab_size, idx_dict = load_data(opts[\"data_file_name\"])\n","    return \" \".join(\n","        [translate(word, encoder, decoder, idx_dict, opts) for word in sentence.split()]\n","    )\n","\n","\n","def translate(input_string, encoder, decoder, idx_dict, opts):\n","    \"\"\"Translates a given string from English to Pig-Latin.\"\"\"\n","\n","    char_to_index = idx_dict[\"char_to_index\"]\n","    index_to_char = idx_dict[\"index_to_char\"]\n","    start_token = idx_dict[\"start_token\"]\n","    end_token = idx_dict[\"end_token\"]\n","\n","    max_generated_chars = 20\n","    gen_string = \"\"\n","\n","    indexes = string_to_index_list(input_string, char_to_index, end_token)\n","    indexes = to_var(\n","        torch.LongTensor(indexes).unsqueeze(0), opts.cuda\n","    )  # Unsqueeze to make it like BS = 1\n","\n","    encoder_annotations, encoder_last_hidden = encoder(indexes)\n","\n","    decoder_hidden = encoder_last_hidden\n","    decoder_input = to_var(torch.LongTensor([[start_token]]), opts.cuda)  # For BS = 1\n","    decoder_inputs = decoder_input\n","\n","    for i in range(max_generated_chars):\n","        ## slow decoding, recompute everything at each time\n","        decoder_outputs, attention_weights = decoder(\n","            decoder_inputs, encoder_annotations, decoder_hidden\n","        )\n","\n","        generated_words = F.softmax(decoder_outputs, dim=2).max(2)[1]\n","        ni = generated_words.cpu().numpy().reshape(-1)  # LongTensor of size 1\n","        ni = ni[-1]  # latest output token\n","\n","        decoder_inputs = torch.cat([decoder_input, generated_words], dim=1)\n","\n","        if ni == end_token:\n","            break\n","        else:\n","            gen_string = \"\".join(\n","                [\n","                    index_to_char[int(item)]\n","                    for item in generated_words.cpu().numpy().reshape(-1)\n","                ]\n","            )\n","\n","    return gen_string\n","\n","\n","def visualize_attention(input_string, encoder, decoder, idx_dict, opts):\n","    \"\"\"Generates a heatmap to show where attention is focused in each decoder step.\"\"\"\n","    if idx_dict is None:\n","        line_pairs, vocab_size, idx_dict = load_data(opts[\"data_file_name\"])\n","    char_to_index = idx_dict[\"char_to_index\"]\n","    index_to_char = idx_dict[\"index_to_char\"]\n","    start_token = idx_dict[\"start_token\"]\n","    end_token = idx_dict[\"end_token\"]\n","\n","    max_generated_chars = 20\n","    gen_string = \"\"\n","\n","    indexes = string_to_index_list(input_string, char_to_index, end_token)\n","    indexes = to_var(\n","        torch.LongTensor(indexes).unsqueeze(0), opts.cuda\n","    )  # Unsqueeze to make it like BS = 1\n","\n","    encoder_annotations, encoder_hidden = encoder(indexes)\n","\n","    decoder_hidden = encoder_hidden\n","    decoder_input = to_var(torch.LongTensor([[start_token]]), opts.cuda)  # For BS = 1\n","    decoder_inputs = decoder_input\n","\n","    produced_end_token = False\n","\n","    for i in range(max_generated_chars):\n","        ## slow decoding, recompute everything at each time\n","        decoder_outputs, attention_weights = decoder(\n","            decoder_inputs, encoder_annotations, decoder_hidden\n","        )\n","        generated_words = F.softmax(decoder_outputs, dim=2).max(2)[1]\n","        ni = generated_words.cpu().numpy().reshape(-1)  # LongTensor of size 1\n","        ni = ni[-1]  # latest output token\n","\n","        decoder_inputs = torch.cat([decoder_input, generated_words], dim=1)\n","\n","        if ni == end_token:\n","            break\n","        else:\n","            gen_string = \"\".join(\n","                [\n","                    index_to_char[int(item)]\n","                    for item in generated_words.cpu().numpy().reshape(-1)\n","                ]\n","            )\n","\n","    if isinstance(attention_weights, tuple):\n","        ## transformer's attention mweights\n","        attention_weights, self_attention_weights = attention_weights\n","\n","    all_attention_weights = attention_weights.data.cpu().numpy()\n","\n","    for i in range(len(all_attention_weights)):\n","        attention_weights_matrix = all_attention_weights[i].squeeze()\n","        fig = plt.figure()\n","        ax = fig.add_subplot(111)\n","        cax = ax.matshow(attention_weights_matrix, cmap=\"bone\")\n","        fig.colorbar(cax)\n","\n","        # Set up axes\n","        ax.set_yticklabels([\"\"] + list(input_string) + [\"EOS\"], rotation=90)\n","        ax.set_xticklabels(\n","            [\"\"] + list(gen_string) + ([\"EOS\"] if produced_end_token else [])\n","        )\n","\n","        # Show label at every tick\n","        ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n","        ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n","        # Add title\n","        plt.xlabel(\"Attention weights to the source sentence in layer {}\".format(i + 1))\n","        plt.tight_layout()\n","        plt.grid(\"off\")\n","        plt.show()\n","\n","    return gen_string\n","\n","\n","def compute_loss(data_dict, encoder, decoder, idx_dict, criterion, optimizer, opts):\n","    \"\"\"Train/Evaluate the model on a dataset.\n","\n","    Arguments:\n","        data_dict: The validation/test word pairs, organized by source and target lengths.\n","        encoder: An encoder model to produce annotations for each step of the input sequence.\n","        decoder: A decoder model (with or without attention) to generate output tokens.\n","        idx_dict: Contains char-to-index and index-to-char mappings, and start & end token indexes.\n","        criterion: Used to compute the CrossEntropyLoss for each decoder output.\n","        optimizer: Train the weights if an optimizer is given. None if only evaluate the model.\n","        opts: The command-line arguments.\n","\n","    Returns:\n","        mean_loss: The average loss over all batches from data_dict.\n","    \"\"\"\n","    start_token = idx_dict[\"start_token\"]\n","    end_token = idx_dict[\"end_token\"]\n","    char_to_index = idx_dict[\"char_to_index\"]\n","\n","    losses = []\n","    for key in data_dict:\n","        input_strings, target_strings = zip(*data_dict[key])\n","        input_tensors = [\n","            torch.LongTensor(string_to_index_list(s, char_to_index, end_token))\n","            for s in input_strings\n","        ]\n","        target_tensors = [\n","            torch.LongTensor(string_to_index_list(s, char_to_index, end_token))\n","            for s in target_strings\n","        ]\n","\n","        num_tensors = len(input_tensors)\n","        num_batches = int(np.ceil(num_tensors / float(opts.batch_size)))\n","\n","        for i in range(num_batches):\n","\n","            start = i * opts.batch_size\n","            end = start + opts.batch_size\n","\n","            inputs = to_var(torch.stack(input_tensors[start:end]), opts.cuda)\n","            targets = to_var(torch.stack(target_tensors[start:end]), opts.cuda)\n","\n","            # The batch size may be different in each epoch\n","            BS = inputs.size(0)\n","\n","            encoder_annotations, encoder_hidden = encoder(inputs)\n","\n","            # The last hidden state of the encoder becomes the first hidden state of the decoder\n","            decoder_hidden = encoder_hidden\n","\n","            start_vector = (\n","                torch.ones(BS).long().unsqueeze(1) * start_token\n","            )  # BS x 1 --> 16x1  CHECKED\n","            decoder_input = to_var(start_vector, opts.cuda)  # BS x 1 --> 16x1  CHECKED\n","\n","            loss = 0.0\n","\n","            seq_len = targets.size(1)  # Gets seq_len from BS x seq_len\n","\n","            decoder_inputs = torch.cat(\n","                [decoder_input, targets[:, 0:-1]], dim=1\n","            )  # Gets decoder inputs by shifting the targets to the right\n","\n","            decoder_outputs, attention_weights = decoder(\n","                decoder_inputs, encoder_annotations, decoder_hidden\n","            )\n","            decoder_outputs_flatten = decoder_outputs.view(-1, decoder_outputs.size(2))\n","            targets_flatten = targets.view(-1)\n","\n","            loss = criterion(decoder_outputs_flatten, targets_flatten)\n","\n","            losses.append(loss.item())\n","\n","            ## training if an optimizer is provided\n","            if optimizer:\n","                # Zero gradients\n","                optimizer.zero_grad()\n","                # Compute gradients\n","                loss.backward()\n","                # Update the parameters of the encoder and decoder\n","                optimizer.step()\n","\n","    return losses\n","\n","\n","def training_loop(\n","    train_dict, val_dict, idx_dict, encoder, decoder, criterion, optimizer, opts\n","):\n","    \"\"\"Runs the main training loop; evaluates the model on the val set every epoch.\n","        * Prints training and val loss each epoch.\n","        * Prints qualitative translation results each epoch using TEST_SENTENCE\n","        * Saves an attention map for TEST_WORD_ATTN each epoch\n","        * Returns loss curves for comparison\n","\n","    Arguments:\n","        train_dict: The training word pairs, organized by source and target lengths.\n","        val_dict: The validation word pairs, organized by source and target lengths.\n","        idx_dict: Contains char-to-index and index-to-char mappings, and start & end token indexes.\n","        encoder: An encoder model to produce annotations for each step of the input sequence.\n","        decoder: A decoder model (with or without attention) to generate output tokens.\n","        criterion: Used to compute the CrossEntropyLoss for each decoder output.\n","        optimizer: Implements a step rule to update the parameters of the encoder and decoder.\n","        opts: The command-line arguments.\n","\n","    Returns:\n","        losses: Lists containing training and validation loss curves.\n","    \"\"\"\n","\n","    start_token = idx_dict[\"start_token\"]\n","    end_token = idx_dict[\"end_token\"]\n","    char_to_index = idx_dict[\"char_to_index\"]\n","\n","    loss_log = open(os.path.join(opts.checkpoint_path, \"loss_log.txt\"), \"w\")\n","\n","    best_val_loss = 1e6\n","    train_losses = []\n","    val_losses = []\n","\n","    mean_train_losses = []\n","    mean_val_losses = []\n","\n","    early_stopping_counter = 0\n","\n","    for epoch in range(opts.nepochs):\n","\n","        optimizer.param_groups[0][\"lr\"] *= opts.lr_decay\n","\n","        train_loss = compute_loss(\n","            train_dict, encoder, decoder, idx_dict, criterion, optimizer, opts\n","        )\n","        val_loss = compute_loss(\n","            val_dict, encoder, decoder, idx_dict, criterion, None, opts\n","        )\n","\n","        mean_train_loss = np.mean(train_loss)\n","        mean_val_loss = np.mean(val_loss)\n","\n","        if mean_val_loss < best_val_loss:\n","            checkpoint(encoder, decoder, idx_dict, opts)\n","            best_val_loss = mean_val_loss\n","            early_stopping_counter = 0\n","        else:\n","            early_stopping_counter += 1\n","\n","        if early_stopping_counter > opts.early_stopping_patience:\n","            print(\n","                \"Validation loss has not improved in {} epochs, stopping early\".format(\n","                    opts.early_stopping_patience\n","                )\n","            )\n","            print(\"Obtained lowest validation loss of: {}\".format(best_val_loss))\n","            return (train_losses, mean_val_losses)\n","\n","        gen_string = translate_sentence(TEST_SENTENCE, encoder, decoder, idx_dict, opts)\n","        print(\n","            \"Epoch: {:3d} | Train loss: {:.3f} | Val loss: {:.3f} | Gen: {:20s}\".format(\n","                epoch, mean_train_loss, mean_val_loss, gen_string\n","            )\n","        )\n","\n","        loss_log.write(\"{} {} {}\\n\".format(epoch, train_loss, val_loss))\n","        loss_log.flush()\n","\n","        train_losses += train_loss\n","        val_losses += val_loss\n","\n","        mean_train_losses.append(mean_train_loss)\n","        mean_val_losses.append(mean_val_loss)\n","\n","        save_loss_plot(mean_train_losses, mean_val_losses, opts)\n","\n","    print(\"Obtained lowest validation loss of: {}\".format(best_val_loss))\n","    return (train_losses, mean_val_losses)\n","\n","\n","def print_data_stats(line_pairs, vocab_size, idx_dict):\n","    \"\"\"Prints example word pairs, the number of data points, and the vocabulary.\"\"\"\n","    print(\"=\" * 80)\n","    print(\"Data Stats\".center(80))\n","    print(\"-\" * 80)\n","    for pair in line_pairs[:5]:\n","        print(pair)\n","    print(\"Num unique word pairs: {}\".format(len(line_pairs)))\n","    print(\"Vocabulary: {}\".format(idx_dict[\"char_to_index\"].keys()))\n","    print(\"Vocab size: {}\".format(vocab_size))\n","    print(\"=\" * 80)\n","\n","\n","def train(opts):\n","    line_pairs, vocab_size, idx_dict = load_data(opts[\"data_file_name\"])\n","    print_data_stats(line_pairs, vocab_size, idx_dict)\n","\n","    # Split the line pairs into an 80% train and 20% val split\n","    num_lines = len(line_pairs)\n","    num_train = int(0.8 * num_lines)\n","    train_pairs, val_pairs = line_pairs[:num_train], line_pairs[num_train:]\n","\n","    # Group the data by the lengths of the source and target words, to form batches\n","    train_dict = create_dict(train_pairs)\n","    val_dict = create_dict(val_pairs)\n","\n","    ##########################################################################\n","    ### Setup: Create Encoder, Decoder, Learning Criterion, and Optimizers ###\n","    ##########################################################################\n","    if opts.encoder_type == \"rnn\":\n","        encoder = GRUEncoder(\n","            vocab_size=vocab_size, hidden_size=opts.hidden_size, opts=opts\n","        )\n","    elif opts.encoder_type == \"transformer\":\n","        encoder = TransformerEncoder(\n","            vocab_size=vocab_size,\n","            hidden_size=opts.hidden_size,\n","            num_layers=opts.num_transformer_layers,\n","            opts=opts,\n","        )\n","    elif opts.encoder_type == \"attention\":\n","      encoder = AttentionEncoder(\n","            vocab_size=vocab_size,\n","            hidden_size=opts.hidden_size,\n","            opts=opts,\n","        )\n","    else:\n","        raise NotImplementedError\n","\n","    if opts.decoder_type == \"rnn\":\n","        decoder = RNNDecoder(vocab_size=vocab_size, hidden_size=opts.hidden_size)\n","    elif opts.decoder_type == \"rnn_attention\":\n","        decoder = RNNAttentionDecoder(\n","            vocab_size=vocab_size,\n","            hidden_size=opts.hidden_size,\n","            attention_type=opts.attention_type,\n","        )\n","    elif opts.decoder_type == \"transformer\":\n","        decoder = TransformerDecoder(\n","            vocab_size=vocab_size,\n","            hidden_size=opts.hidden_size,\n","            num_layers=opts.num_transformer_layers,\n","        )\n","    elif opts.encoder_type == \"attention\":\n","      decoder = AttentionDecoder(\n","            vocab_size=vocab_size,\n","            hidden_size=opts.hidden_size,\n","        )\n","    else:\n","        raise NotImplementedError\n","\n","    #### setup checkpoint path\n","    model_name = \"h{}-bs{}-{}-{}\".format(\n","        opts.hidden_size, opts.batch_size, opts.decoder_type, opts.data_file_name\n","    )\n","    opts.checkpoint_path = model_name\n","    create_dir_if_not_exists(opts.checkpoint_path)\n","    ####\n","\n","    if opts.cuda:\n","        encoder.cuda()\n","        decoder.cuda()\n","        print(\"Moved models to GPU!\")\n","\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = optim.Adam(\n","        list(encoder.parameters()) + list(decoder.parameters()), lr=opts.learning_rate\n","    )\n","\n","    try:\n","        losses = training_loop(\n","            train_dict, val_dict, idx_dict, encoder, decoder, criterion, optimizer, opts\n","        )\n","    except KeyboardInterrupt:\n","        print(\"Exiting early from training.\")\n","        return encoder, decoder, losses\n","\n","    return encoder, decoder, losses\n","\n","\n","def print_opts(opts):\n","    \"\"\"Prints the values of all command-line arguments.\"\"\"\n","    print(\"=\" * 80)\n","    print(\"Opts\".center(80))\n","    print(\"-\" * 80)\n","    for key in opts.__dict__:\n","        print(\"{:>30}: {:<30}\".format(key, opts.__dict__[key]).center(80))\n","    print(\"=\" * 80)"]},{"cell_type":"markdown","metadata":{"id":"0yh08KhgnA30"},"source":["## Download dataset"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"aROU2xZanDKq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647459953567,"user_tz":240,"elapsed":1240,"user":{"displayName":"TONGFEI ZHOU","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15125844894166682257"}},"outputId":"dcd3bedc-eddc-465f-f297-d74facc3376e"},"outputs":[{"output_type":"stream","name":"stdout","text":["data/pig_latin_small.txt\n","Downloading data from http://www.cs.toronto.edu/~jba/pig_latin_small.txt\n","data/pig_latin_large.txt\n","Downloading data from http://www.cs.toronto.edu/~jba/pig_latin_large.txt\n"]}],"source":["######################################################################\n","# Download Translation datasets\n","######################################################################\n","data_fpath = get_file(\n","    fname=\"pig_latin_small.txt\",\n","    origin=\"http://www.cs.toronto.edu/~jba/pig_latin_small.txt\",\n","    untar=False,\n",")\n","\n","data_fpath = get_file(\n","    fname=\"pig_latin_large.txt\",\n","    origin=\"http://www.cs.toronto.edu/~jba/pig_latin_large.txt\",\n","    untar=False,\n",")"]},{"cell_type":"markdown","metadata":{"id":"YDYMr7NclZdw"},"source":["# Part 1: Neural machine translation (NMT)\n","\n","In this section, you will implement a Gated Recurrent Unit (GRU) cell, a common type of recurrent neural network (RNN). The GRU cell is a simplification of the Long Short-Term Memory cell. Therefore, we have provided you with an implemented LSTM cell (`MyLSTMCell`), which you can reference when completing `MyGRUCell`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cOnALRQkkjDO"},"outputs":[],"source":["class MyLSTMCell(nn.Module):\n","    def __init__(self, input_size, hidden_size):\n","        super(MyLSTMCell, self).__init__()\n","\n","        self.input_size = input_size\n","        self.hidden_size = hidden_size\n","\n","        self.Wif = nn.Linear(input_size, hidden_size)\n","        self.Whf = nn.Linear(hidden_size, hidden_size)\n","\n","        self.Wii = nn.Linear(input_size, hidden_size)\n","        self.Whi = nn.Linear(hidden_size, hidden_size)\n","\n","        self.Wic = nn.Linear(input_size, hidden_size)\n","        self.Whc = nn.Linear(hidden_size, hidden_size)\n","\n","        self.Wio = nn.Linear(input_size, hidden_size)\n","        self.Who = nn.Linear(hidden_size, hidden_size)\n","\n","    def forward(self, x, h_prev, c_prev):\n","        \"\"\"Forward pass of the LSTM computation for one time step.\n","\n","        Arguments\n","            x: batch_size x input_size\n","            h_prev: batch_size x hidden_size\n","            c_prev: batch_size x hidden_size\n","\n","        Returns:\n","            h_new: batch_size x hidden_size\n","            c_new: batch_size x hidden_size\n","        \"\"\"\n","\n","        f = torch.sigmoid(self.Wif(x) + self.Whf(h_prev))\n","        i = torch.sigmoid(self.Wii(x) + self.Whi(h_prev))\n","\n","        c = torch.tanh(self.Wic(x) + self.Whc(h_prev))\n","        o = torch.sigmoid(self.Wio(x) + self.Who(h_prev))\n","\n","        c_new = f * c_prev + i * c\n","        h_new = o * torch.tanh(c_new)\n","\n","        return h_new, c_new"]},{"cell_type":"markdown","metadata":{"id":"dCae1mOUlZrC"},"source":["## Step 1: GRU Cell\n","Please implement the `MyGRUCell` class defined in the next cell. "]},{"cell_type":"code","execution_count":9,"metadata":{"id":"DGyxqZIQzTJH","executionInfo":{"status":"ok","timestamp":1647460035880,"user_tz":240,"elapsed":266,"user":{"displayName":"TONGFEI ZHOU","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15125844894166682257"}}},"outputs":[],"source":["class MyGRUCell(nn.Module):\n","    def __init__(self, input_size, hidden_size):\n","        super(MyGRUCell, self).__init__()\n","\n","        self.input_size = input_size\n","        self.hidden_size = hidden_size\n","\n","        # ------------\n","        # FILL THIS IN\n","        # ------------\n","        # Input linear layers\n","        self.Wiz = nn.Linear(input_size, hidden_size)\n","        self.Wir = nn.Linear(input_size, hidden_size)\n","        self.Wih = nn.Linear(input_size, hidden_size)\n","\n","        # Hidden linear layers\n","        self.Whz = nn.Linear(hidden_size, hidden_size)\n","        self.Whr = nn.Linear(hidden_size, hidden_size)\n","        self.Whh = nn.Linear(hidden_size, hidden_size)\n","\n","    def forward(self, x, h_prev):\n","        \"\"\"Forward pass of the GRU computation for one time step.\n","\n","        Arguments\n","            x: batch_size x input_size\n","            h_prev: batch_size x hidden_size\n","\n","        Returns:\n","            h_new: batch_size x hidden_size\n","        \"\"\"\n","\n","        # ------------\n","        # FILL THIS IN\n","        # ------------\n","        z = torch.sigmoid(self.Wiz(x) + self.Whz(h_prev))\n","        r = torch.sigmoid(self.Wir(x) + self.Whr(h_prev))\n","        g = torch.tanh(self.Wih(x) + self.Whh(r * h_prev))\n","        h_new = (1.0-z) * g + z * h_prev\n","        return h_new"]},{"cell_type":"markdown","metadata":{"id":"ecEq4TP2lZ4Z"},"source":["## Step 2: GRU Encoder\n","\n","The following cells use your `MyGRUCell` implementation to build a recurrent encoder and decoder. Please read the implementations to understand what they do and run the cells before proceeding."]},{"cell_type":"code","execution_count":10,"metadata":{"id":"8jDNim2fmVJV","executionInfo":{"status":"ok","timestamp":1647460039119,"user_tz":240,"elapsed":236,"user":{"displayName":"TONGFEI ZHOU","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15125844894166682257"}}},"outputs":[],"source":["class GRUEncoder(nn.Module):\n","    def __init__(self, vocab_size, hidden_size, opts):\n","        super(GRUEncoder, self).__init__()\n","\n","        self.vocab_size = vocab_size\n","        self.hidden_size = hidden_size\n","        self.opts = opts\n","\n","        self.embedding = nn.Embedding(vocab_size, hidden_size)\n","        self.gru = MyGRUCell(hidden_size, hidden_size)\n","\n","    def forward(self, inputs):\n","        \"\"\"Forward pass of the encoder RNN.\n","\n","        Arguments:\n","            inputs: Input token indexes across a batch for all time steps in the sequence. (batch_size x seq_len)\n","\n","        Returns:\n","            annotations: The hidden states computed at each step of the input sequence. (batch_size x seq_len x hidden_size)\n","            hidden: The final hidden state of the encoder, for each sequence in a batch. (batch_size x hidden_size)\n","        \"\"\"\n","\n","        batch_size, seq_len = inputs.size()\n","        hidden = self.init_hidden(batch_size)\n","\n","        encoded = self.embedding(inputs)  # batch_size x seq_len x hidden_size\n","        annotations = []\n","\n","        for i in range(seq_len):\n","            x = encoded[:, i, :]  # Get the current time step, across the whole batch\n","            hidden = self.gru(x, hidden)\n","            annotations.append(hidden)\n","\n","        annotations = torch.stack(annotations, dim=1)\n","        return annotations, hidden\n","\n","    def init_hidden(self, bs):\n","        \"\"\"Creates a tensor of zeros to represent the initial hidden states\n","        of a batch of sequences.\n","\n","        Arguments:\n","            bs: The batch size for the initial hidden state.\n","\n","        Returns:\n","            hidden: An initial hidden state of all zeros. (batch_size x hidden_size)\n","        \"\"\"\n","        return to_var(torch.zeros(bs, self.hidden_size), self.opts.cuda)"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"HvwizYM9ma4p","executionInfo":{"status":"ok","timestamp":1647460042364,"user_tz":240,"elapsed":270,"user":{"displayName":"TONGFEI ZHOU","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15125844894166682257"}}},"outputs":[],"source":["class RNNDecoder(nn.Module):\n","    def __init__(self, vocab_size, hidden_size):\n","        super(RNNDecoder, self).__init__()\n","        self.vocab_size = vocab_size\n","        self.hidden_size = hidden_size\n","\n","        self.embedding = nn.Embedding(vocab_size, hidden_size)\n","        self.rnn = MyGRUCell(input_size=hidden_size, hidden_size=hidden_size)\n","        self.out = nn.Linear(hidden_size, vocab_size)\n","\n","    def forward(self, inputs, annotations, hidden_init):\n","        \"\"\"Forward pass of the non-attentional decoder RNN.\n","\n","        Arguments:\n","            inputs: Input token indexes across a batch. (batch_size x seq_len)\n","            annotations: This is not used here. It just maintains consistency with the\n","                    interface used by the AttentionDecoder class.\n","            hidden_init: The hidden states from the last step of encoder, across a batch. (batch_size x hidden_size)\n","\n","        Returns:\n","            output: Un-normalized scores for each token in the vocabulary, across a batch for all the decoding time steps. (batch_size x decoder_seq_len x vocab_size)\n","            None\n","        \"\"\"\n","        batch_size, seq_len = inputs.size()\n","        embed = self.embedding(inputs)  # batch_size x seq_len x hidden_size\n","\n","        hiddens = []\n","        h_prev = hidden_init\n","\n","        for i in range(seq_len):\n","            x = embed[\n","                :, i, :\n","            ]  # Get the current time step input tokens, across the whole batch\n","            h_prev = self.rnn(x, h_prev)  # batch_size x hidden_size\n","            hiddens.append(h_prev)\n","\n","        hiddens = torch.stack(hiddens, dim=1)  # batch_size x seq_len x hidden_size\n","\n","        output = self.out(hiddens)  # batch_size x seq_len x vocab_size\n","        return output, None"]},{"cell_type":"markdown","metadata":{"id":"TSDTbsydlaGI"},"source":["## Step 3: Training and Analysis\n","\n","Train the encoder-decoder model to perform English --> Pig Latin translation. We will start by training on the smaller dataset."]},{"cell_type":"code","execution_count":12,"metadata":{"id":"XmVuXTozTPF7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647460189437,"user_tz":240,"elapsed":140370,"user":{"displayName":"TONGFEI ZHOU","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15125844894166682257"}},"outputId":"da1e33b0-518d-42c4-c6a6-bede0474a3dd"},"outputs":[{"output_type":"stream","name":"stdout","text":["================================================================================\n","                                      Opts                                      \n","--------------------------------------------------------------------------------\n","                         data_file_name: pig_latin_small                        \n","                                   cuda: 1                                      \n","                                nepochs: 50                                     \n","                         checkpoint_dir: checkpoints                            \n","                          learning_rate: 0.005                                  \n","                               lr_decay: 0.99                                   \n","                early_stopping_patience: 20                                     \n","                             batch_size: 64                                     \n","                            hidden_size: 32                                     \n","                           encoder_type: rnn                                    \n","                           decoder_type: rnn                                    \n","                         attention_type:                                        \n","================================================================================\n","================================================================================\n","                                   Data Stats                                   \n","--------------------------------------------------------------------------------\n","('substance', 'ubstancesay')\n","('smallness', 'allnesssmay')\n","('garrets', 'arretsgay')\n","('several', 'everalsay')\n","('suspicion', 'uspicionsay')\n","Num unique word pairs: 3198\n","Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n","Vocab size: 29\n","================================================================================\n","Moved models to GPU!\n","Epoch:   0 | Train loss: 2.295 | Val loss: 2.019 | Gen: ay-insay-insay-insay ay-insay-insay-insay insay-ingsay-insay-i onsay-insay-insay-in onsay-insay-insay-in\n","Epoch:   1 | Train loss: 1.852 | Val loss: 1.835 | Gen: eay-ingsay-ingsay-in ay-ay-ingsay-ingsay- onsay-ingsay-ingsay- ingsay-ingsay-ingsay onsay-ingsay-ingsay-\n","Epoch:   2 | Train loss: 1.688 | Val loss: 1.711 | Gen: eday ay-ingsay ontay ingsay ongsay-ingsay-ingsay\n","Epoch:   3 | Train loss: 1.564 | Val loss: 1.616 | Gen: eay-ateway ariontay ontiontay-ingsay inteday ortay-ingsay\n","Epoch:   4 | Train loss: 1.463 | Val loss: 1.556 | Gen: etay ariontay-ingsay ontionsionsay insay ortionsionsay\n","Epoch:   5 | Train loss: 1.381 | Val loss: 1.495 | Gen: eay-ateway aitray onsingsay istay-ingsay ortionsionsay\n","Epoch:   6 | Train loss: 1.304 | Val loss: 1.491 | Gen: eway aitray-ingsay onsingway istay-away-away-ingw ortionsway\n","Epoch:   7 | Train loss: 1.243 | Val loss: 1.467 | Gen: eway aitray onsingway istentay-away-away-a ortionsway\n","Epoch:   8 | Train loss: 1.223 | Val loss: 1.443 | Gen: eway istray onsingsay istedway ortionday\n","Epoch:   9 | Train loss: 1.177 | Val loss: 1.430 | Gen: eway airtay-oday onsingway issedway ortionsway-ingsay\n","Epoch:  10 | Train loss: 1.121 | Val loss: 1.388 | Gen: eway aitray onsingway issay-ightnay orkingway-ingsay\n","Epoch:  11 | Train loss: 1.071 | Val loss: 1.402 | Gen: eway airday onsingway issay orkingway\n","Epoch:  12 | Train loss: 1.032 | Val loss: 1.373 | Gen: etay airday onsingway issay-ightnay-ingsay ongray\n","Epoch:  13 | Train loss: 0.996 | Val loss: 1.373 | Gen: eway ardway onsingway issay orkingshay-ingsay\n","Epoch:  14 | Train loss: 0.990 | Val loss: 1.416 | Gen: eway airday onstantay-away-iecep issay ordway-inway-awlay-a\n","Epoch:  15 | Train loss: 0.994 | Val loss: 1.365 | Gen: eway agray onsingsay-away-etay- issay-away-ingsay-aw orkingshay\n","Epoch:  16 | Train loss: 0.960 | Val loss: 1.300 | Gen: eway airday onsingshay-inway-awa issay orkiveway\n","Epoch:  17 | Train loss: 0.934 | Val loss: 1.292 | Gen: eway airway onsingsay issay orkingshay\n","Epoch:  18 | Train loss: 0.902 | Val loss: 1.283 | Gen: etay airday onsingscay issay orkiveway\n","Epoch:  19 | Train loss: 0.871 | Val loss: 1.245 | Gen: eway airday onsingscay issay orkingspay\n","Epoch:  20 | Train loss: 0.836 | Val loss: 1.243 | Gen: eway airlyway onsinglay issay orkingunday\n","Epoch:  21 | Train loss: 0.820 | Val loss: 1.249 | Gen: etay airday onsinglay-ingsway issay orkivedway\n","Epoch:  22 | Train loss: 0.802 | Val loss: 1.259 | Gen: ethay airway onsinglay-ingsway issay orkingshay-ingshay\n","Epoch:  23 | Train loss: 0.796 | Val loss: 1.231 | Gen: ethay airday onsinglay-ougshay issay orkingingway\n","Epoch:  24 | Train loss: 0.784 | Val loss: 1.267 | Gen: ethay airlyway onsingway issay orkifiecepay\n","Epoch:  25 | Train loss: 0.770 | Val loss: 1.268 | Gen: ethay airlyway onsinglay-ingshay-ig issay orkinguingslyway\n","Epoch:  26 | Train loss: 0.753 | Val loss: 1.279 | Gen: ethay airdway onsinglay-awlay issay orkifiedway\n","Epoch:  27 | Train loss: 0.743 | Val loss: 1.272 | Gen: ethay aightray onsinglay-awlay-ings issay orkifitionsway\n","Epoch:  28 | Train loss: 0.747 | Val loss: 1.333 | Gen: etay airway onsinglay-ingslay isssssssay-inway-inw orkingway\n","Epoch:  29 | Train loss: 0.769 | Val loss: 1.240 | Gen: ethay airlyway onsinglay-awlay-awla issay orkifiedcay\n","Epoch:  30 | Train loss: 0.729 | Val loss: 1.328 | Gen: ethay-ightnay airtway onsinglay-ingshay-yb issay orkifiedway\n","Epoch:  31 | Train loss: 0.716 | Val loss: 1.251 | Gen: ethay airtway onsinglay-awlay-etet isssay orkifiedway\n","Epoch:  32 | Train loss: 0.690 | Val loss: 1.211 | Gen: ethay airtway onsinglay-awlay issay oodgryay\n","Epoch:  33 | Train loss: 0.670 | Val loss: 1.224 | Gen: ethay airtway onsinglay-awlay-ouse issay orkifiedway\n","Epoch:  34 | Train loss: 0.666 | Val loss: 1.238 | Gen: ethay airtway onsinemtay-ingsay issway orkinguray\n","Epoch:  35 | Train loss: 0.660 | Val loss: 1.261 | Gen: ethay airway onsinglay-ingday issay orkinguray\n","Epoch:  36 | Train loss: 0.662 | Val loss: 1.280 | Gen: ethay airfray onsinemtionmationmay issay orkinguredpay\n","Epoch:  37 | Train loss: 0.677 | Val loss: 1.249 | Gen: eway airay onsitioncay issway orkifiedway\n","Epoch:  38 | Train loss: 0.649 | Val loss: 1.212 | Gen: ethay aringway onsinoncilityway issway orkifiedway\n","Epoch:  39 | Train loss: 0.623 | Val loss: 1.193 | Gen: ethay airway onsinelfay issway orkingdibay\n","Epoch:  40 | Train loss: 0.609 | Val loss: 1.181 | Gen: ethay airay onsinglay-ingsay issay orkifuedlay\n","Epoch:  41 | Train loss: 0.602 | Val loss: 1.210 | Gen: ethay ayringway onsintlingsay-ougspa issway orkinguringway\n","Epoch:  42 | Train loss: 0.603 | Val loss: 1.233 | Gen: ethay airkway onsinglantmay-ougsen issway orkifieddbay\n","Epoch:  43 | Train loss: 0.613 | Val loss: 1.243 | Gen: ethay aridway onsinelway-ixsay issway orningday\n","Epoch:  44 | Train loss: 0.637 | Val loss: 1.291 | Gen: ethay airway onsinglanceway issway orkibleway\n","Epoch:  45 | Train loss: 0.625 | Val loss: 1.254 | Gen: eway ay-ightray onstioncay-icepay issway orkifuleway\n","Epoch:  46 | Train loss: 0.608 | Val loss: 1.245 | Gen: ethay ayray onsinedtay isssay orkingingwray\n","Epoch:  47 | Train loss: 0.593 | Val loss: 1.277 | Gen: eway ayray onsinglangay-umously issway orkingurednay\n","Epoch:  48 | Train loss: 0.584 | Val loss: 1.255 | Gen: ethay ayray onsinglantingcay issway orkinguray\n","Epoch:  49 | Train loss: 0.568 | Val loss: 1.195 | Gen: ethay ayringway onsinicationsiblay issway orkinguray\n","Obtained lowest validation loss of: 1.1813948427637417\n","source:\t\tthe air conditioning is working \n","translated:\tethay ayringway onsinicationsiblay issway orkinguray\n"]}],"source":["TEST_SENTENCE = \"the air conditioning is working\"\n","\n","rnn_args_s = AttrDict()\n","args_dict = {\n","    \"data_file_name\": \"pig_latin_small\",\n","    \"cuda\": True,\n","    \"nepochs\": 50,\n","    \"checkpoint_dir\": \"checkpoints\",\n","    \"learning_rate\": 0.005,\n","    \"lr_decay\": 0.99,\n","    \"early_stopping_patience\": 20,\n","    \"batch_size\": 64,\n","    \"hidden_size\": 32,\n","    \"encoder_type\": \"rnn\",  # options: rnn / transformer\n","    \"decoder_type\": \"rnn\",  # options: rnn / rnn_attention / transformer\n","    \"attention_type\": \"\",   # options: additive / scaled_dot\n","}\n","rnn_args_s.update(args_dict)\n","\n","print_opts(rnn_args_s)\n","rnn_encode_s, rnn_decoder_s, rnn_losses_s = train(rnn_args_s)\n","\n","translated = translate_sentence(\n","    TEST_SENTENCE, rnn_encode_s, rnn_decoder_s, None, rnn_args_s\n",")\n","print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"]},{"cell_type":"markdown","metadata":{"id":"0mR97V_NtER6"},"source":["Next, we train on the larger dataset. This experiment investigates if increasing dataset size improves model generalization on the validation set. \n","\n","For a fair comparison, the number of iterations (not number of epochs) for each run should be similar. This is done in a quick and dirty way by adjusting the batch size so approximately the same number of batches is processed per epoch."]},{"cell_type":"code","execution_count":13,"metadata":{"id":"H3YLrAjsmx_W","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647460384402,"user_tz":240,"elapsed":169418,"user":{"displayName":"TONGFEI ZHOU","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15125844894166682257"}},"outputId":"f4354d62-2239-4ec9-f096-71261dcb602e"},"outputs":[{"output_type":"stream","name":"stdout","text":["================================================================================\n","                                      Opts                                      \n","--------------------------------------------------------------------------------\n","                         data_file_name: pig_latin_large                        \n","                                   cuda: 1                                      \n","                                nepochs: 50                                     \n","                         checkpoint_dir: checkpoints                            \n","                          learning_rate: 0.005                                  \n","                               lr_decay: 0.99                                   \n","                early_stopping_patience: 10                                     \n","                             batch_size: 512                                    \n","                            hidden_size: 32                                     \n","                           encoder_type: rnn                                    \n","                           decoder_type: rnn                                    \n","                         attention_type:                                        \n","================================================================================\n","================================================================================\n","                                   Data Stats                                   \n","--------------------------------------------------------------------------------\n","('several', 'everalsay')\n","('assistant', 'assistantway')\n","('overdose', 'overdoseway')\n","('tnt', 'tntay')\n","('eleven', 'elevenway')\n","Num unique word pairs: 22402\n","Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n","Vocab size: 29\n","================================================================================\n","Moved models to GPU!\n","Epoch:   0 | Train loss: 2.352 | Val loss: 2.047 | Gen: inngray-inngray-inng intinngray-inngray-i ongray-inngray-inngr inngray-inngray-inng ongray-inngray-inngr\n","Epoch:   1 | Train loss: 1.932 | Val loss: 1.874 | Gen: elway-ingsay away-ingay onghay inghay onghay\n","Epoch:   2 | Train loss: 1.765 | Val loss: 1.787 | Gen: elway away-onghay ongtay-inghay ingtay onghay-onghay\n","Epoch:   3 | Train loss: 1.648 | Val loss: 1.690 | Gen: elway away-onghay ontingtay-onghay intay-onghay oongthay-onghay\n","Epoch:   4 | Train loss: 1.545 | Val loss: 1.613 | Gen: elway away-away ontinghay-inway inway otinghtay-inghay\n","Epoch:   5 | Train loss: 1.452 | Val loss: 1.563 | Gen: eway away ontinghay-inway istay othtinghtay\n","Epoch:   6 | Train loss: 1.380 | Val loss: 1.537 | Gen: estay away ontingray istay othtingstay\n","Epoch:   7 | Train loss: 1.326 | Val loss: 1.541 | Gen: eway ailiway ontingay-ortingway issay oothingsway\n","Epoch:   8 | Train loss: 1.270 | Val loss: 1.430 | Gen: eway ay-inway-away-away ontingay-inway-awlay issay oughtway\n","Epoch:   9 | Train loss: 1.204 | Val loss: 1.503 | Gen: eway ay-inway-awlay ontingay-inghay issay othtinghtay-ortersta\n","Epoch:  10 | Train loss: 1.183 | Val loss: 1.429 | Gen: eway awiontay ontingingay-ortay-ay issway orgstightnay\n","Epoch:  11 | Train loss: 1.128 | Val loss: 1.433 | Gen: eway ailitedway ontingicationtay-ond issinay ortingstay-ondedgay\n","Epoch:  12 | Train loss: 1.099 | Val loss: 1.494 | Gen: eway ayiationay ontingray-ortertay-o issay orgstay-oughtertay\n","Epoch:  13 | Train loss: 1.075 | Val loss: 1.366 | Gen: eway ay-inway-awlay ontingay-ondgay issay orgstay\n","Epoch:  14 | Train loss: 1.037 | Val loss: 1.360 | Gen: eway ayiationway ontingrationtay-awla issay orfay-oftoughtedway\n","Epoch:  15 | Train loss: 1.006 | Val loss: 1.314 | Gen: eway ayiay ongingay-omouray issay orgingsay\n","Epoch:  16 | Train loss: 0.992 | Val loss: 1.290 | Gen: eway ayioncay ontingay-inway-awlay isway orgstay\n","Epoch:  17 | Train loss: 0.960 | Val loss: 1.414 | Gen: eway ayway oningay-ingratorway issay orgightay-ofway\n","Epoch:  18 | Train loss: 0.932 | Val loss: 1.238 | Gen: eway ayway onitingay-ordentay issway orkingway\n","Epoch:  19 | Train loss: 0.919 | Val loss: 1.349 | Gen: eway ayway ongingrationantay issay orgifusay-ougedway\n","Epoch:  20 | Train loss: 0.939 | Val loss: 1.292 | Gen: eway aillway onitingay-inway-awla issay orkingshay\n","Epoch:  21 | Train loss: 0.927 | Val loss: 1.320 | Gen: eway airway onitingay-ordentay isway ookingsway\n","Epoch:  22 | Train loss: 0.905 | Val loss: 1.242 | Gen: eway airway onidinationtay-ayday isway orgivhway\n","Epoch:  23 | Train loss: 0.867 | Val loss: 1.214 | Gen: eway airway oningay-ingway issay orkingshay\n","Epoch:  24 | Train loss: 0.835 | Val loss: 1.174 | Gen: eway airway onidingray-inway-awl isway orgivhay\n","Epoch:  25 | Train loss: 0.813 | Val loss: 1.175 | Gen: eway airway onidicationtay-ayday isway orgivhay\n","Epoch:  26 | Train loss: 0.795 | Val loss: 1.187 | Gen: eway airway oninitingray-inway-a isway orkingway\n","Epoch:  27 | Train loss: 0.783 | Val loss: 1.183 | Gen: eway airway onidicationtay isway orkingway\n","Epoch:  28 | Train loss: 0.786 | Val loss: 1.250 | Gen: eway airway oninitingmay-inway-a issay orgifhay\n","Epoch:  29 | Train loss: 0.813 | Val loss: 1.214 | Gen: eway ayway onintionay-inway-awl issway olkinghay\n","Epoch:  30 | Train loss: 0.829 | Val loss: 1.162 | Gen: eway airway onintay-interay issay olkingway\n","Epoch:  31 | Train loss: 0.773 | Val loss: 1.173 | Gen: etay airway oninitinationway isway orgifgay\n","Epoch:  32 | Train loss: 0.748 | Val loss: 1.171 | Gen: eway airway onitingtay isway orkingshay\n","Epoch:  33 | Train loss: 0.727 | Val loss: 1.097 | Gen: eway airway oninitingway isway orkingway\n","Epoch:  34 | Train loss: 0.710 | Val loss: 1.114 | Gen: eway airway oninitionationtay isway olkingway\n","Epoch:  35 | Train loss: 0.704 | Val loss: 1.095 | Gen: eway airway oninitionay-inway-aw isway orgifhay\n","Epoch:  36 | Train loss: 0.702 | Val loss: 1.140 | Gen: eway airway onitingmay-inway-awl issay orkighhmay\n","Epoch:  37 | Train loss: 0.710 | Val loss: 1.108 | Gen: eway airway oninitionay isway olkingway\n","Epoch:  38 | Train loss: 0.727 | Val loss: 1.094 | Gen: eway airway onintionarymay isway orgiknay\n","Epoch:  39 | Train loss: 0.715 | Val loss: 1.142 | Gen: eway airway onitingray issay orkighday\n","Epoch:  40 | Train loss: 0.694 | Val loss: 1.101 | Gen: eway airway onitionationmay isway orkingway\n","Epoch:  41 | Train loss: 0.665 | Val loss: 1.071 | Gen: etay airway onitingray isway orkingway\n","Epoch:  42 | Train loss: 0.650 | Val loss: 1.083 | Gen: etay airway onitingtay isway orkingway\n","Epoch:  43 | Train loss: 0.633 | Val loss: 1.067 | Gen: etay airway onitingray isway orkingway\n","Epoch:  44 | Train loss: 0.624 | Val loss: 1.061 | Gen: etay airway onitingmay isway orkingway\n","Epoch:  45 | Train loss: 0.619 | Val loss: 1.076 | Gen: etay airway onitiontay-inway-awl isway orkinghjay\n","Epoch:  46 | Train loss: 0.620 | Val loss: 1.107 | Gen: etay airway onitiontionay isway orkingway\n","Epoch:  47 | Train loss: 0.628 | Val loss: 1.120 | Gen: eway airway oninitingray isway olkingway\n","Epoch:  48 | Train loss: 0.634 | Val loss: 1.092 | Gen: ethay airway onitiontay-aidsway isway orkingjay\n","Epoch:  49 | Train loss: 0.610 | Val loss: 1.085 | Gen: etay airway onitingtay-inway-awl isway olikiongsday\n","Obtained lowest validation loss of: 1.0612267980972925\n","source:\t\tthe air conditioning is working \n","translated:\tetay airway onitingtay-inway-awl isway olikiongsday\n"]}],"source":["TEST_SENTENCE = \"the air conditioning is working\"\n","\n","rnn_args_l = AttrDict()\n","args_dict = {\n","    \"data_file_name\": \"pig_latin_large\",\n","    \"cuda\": True,\n","    \"nepochs\": 50,\n","    \"checkpoint_dir\": \"checkpoints\",\n","    \"learning_rate\": 0.005,\n","    \"lr_decay\": 0.99,\n","    \"early_stopping_patience\": 10,\n","    \"batch_size\": 512,\n","    \"hidden_size\": 32,\n","    \"encoder_type\": \"rnn\",  # options: rnn / transformer\n","    \"decoder_type\": \"rnn\",  # options: rnn / rnn_attention / transformer\n","    \"attention_type\": \"\",   # options: additive / scaled_dot\n","}\n","rnn_args_l.update(args_dict)\n","\n","print_opts(rnn_args_l)\n","rnn_encode_l, rnn_decoder_l, rnn_losses_l = train(rnn_args_l)\n","\n","translated = translate_sentence(\n","    TEST_SENTENCE, rnn_encode_l, rnn_decoder_l, None, rnn_args_l\n",")\n","print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"]},{"cell_type":"markdown","metadata":{"id":"01HsZ6EItc56"},"source":["The code below plots the training and validation losses of each model, as a function of the number of gradient descent iterations. Are there significant differences in the validation performance of each model? (see follow-up questions in handout)"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"Qyk_9-Fwtekj","colab":{"base_uri":"https://localhost:8080/","height":338},"executionInfo":{"status":"ok","timestamp":1647460411424,"user_tz":240,"elapsed":686,"user":{"displayName":"TONGFEI ZHOU","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15125844894166682257"}},"outputId":"384f05f7-1e92-44cd-e809-83868d24a471"},"outputs":[{"output_type":"stream","name":"stdout","text":["Plot saved to: /content/content/csc421/a3/loss_plot_gru.pdf\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 0 Axes>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<Figure size 720x288 with 2 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}],"source":["save_loss_comparison_gru(rnn_losses_s, rnn_losses_l, rnn_args_s, rnn_args_l, \"gru\")"]},{"cell_type":"markdown","metadata":{"id":"cE4ijaCzneAt"},"source":["Select best performing model, and try translating different sentences by changing the variable TEST_SENTENCE. Identify a failure mode and briefly describe it (see follow-up questions in handout)."]},{"cell_type":"code","execution_count":15,"metadata":{"id":"WrNnz8W1nULf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647460422689,"user_tz":240,"elapsed":542,"user":{"displayName":"TONGFEI ZHOU","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15125844894166682257"}},"outputId":"e74fcfb8-40af-45cb-f0cc-2845e77610da"},"outputs":[{"output_type":"stream","name":"stdout","text":["source:\t\tthe air conditioning is working \n","translated:\tetay airway onitingtay-inway-awl isway olikiongsday\n"]}],"source":["best_encoder = rnn_encode_l  # Replace with rnn_encode_s or rnn_encode_l\n","best_decoder = rnn_decoder_l  # Replace with rnn_decoder_s or rnn_decoder_l\n","best_args = rnn_args_l     # Replace with rnn_args_s or rnn_args_l\n","\n","TEST_SENTENCE = \"the air conditioning is working\"\n","translated = translate_sentence(\n","    TEST_SENTENCE, best_encoder, best_decoder, None, best_args\n",")\n","print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"]},{"cell_type":"code","source":["TEST_SENTENCE = \"the air streaming is working\"\n","translated = translate_sentence(\n","    TEST_SENTENCE, best_encoder, best_decoder, None, best_args\n",")\n","print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647460425430,"user_tz":240,"elapsed":275,"user":{"displayName":"TONGFEI ZHOU","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15125844894166682257"}},"outputId":"9ac789d5-b272-4f90-86be-f31f9f9d6bf0","id":"1tdeVzBtcVYA"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["source:\t\tthe air streaming is working \n","translated:\tetay airway easityway isway olikiongsday\n"]}]},{"cell_type":"code","source":["TEST_SENTENCE = \"the fair conding is working\"\n","translated = translate_sentence(\n","    TEST_SENTENCE, best_encoder, best_decoder, None, best_args\n",")\n","print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647460429157,"user_tz":240,"elapsed":232,"user":{"displayName":"TONGFEI ZHOU","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15125844894166682257"}},"outputId":"42411587-fd06-4a22-dd72-9953300e50c4","id":"w9GQntdDceNF"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["source:\t\tthe fair conding is working \n","translated:\tetay airway oningstay isway olikiongsday\n"]}]},{"cell_type":"code","source":["TEST_SENTENCE = \"the pair performing is working\"\n","translated = translate_sentence(\n","    TEST_SENTENCE, best_encoder, best_decoder, None, best_args\n",")\n","print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3OwCl1AzXAjA","executionInfo":{"status":"ok","timestamp":1647460433030,"user_tz":240,"elapsed":596,"user":{"displayName":"TONGFEI ZHOU","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15125844894166682257"}},"outputId":"51559f86-2b78-47bd-d9d8-5dbeba210d6c"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["source:\t\tthe pair performing is working \n","translated:\tetay airway erfichingray isway olikiongsday\n"]}]},{"cell_type":"markdown","metadata":{"id":"RWwA6OGqlaTq"},"source":["# Part 2: Attention mechanisms"]},{"cell_type":"markdown","metadata":{"id":"AJSafHSAmu_w"},"source":["## Step 1: Additive attention\n","\n","In the next cell, the [additive attention](https://paperswithcode.com/method/additive-attention) mechanism has been implemented for you. Please take a momement to read through it and understand what it is doing. See the assignment handouts for details."]},{"cell_type":"code","execution_count":19,"metadata":{"id":"AdewEVSMo5jJ","executionInfo":{"status":"ok","timestamp":1647460745691,"user_tz":240,"elapsed":275,"user":{"displayName":"TONGFEI ZHOU","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15125844894166682257"}}},"outputs":[],"source":["class AdditiveAttention(nn.Module):\n","    def __init__(self, hidden_size):\n","        super(AdditiveAttention, self).__init__()\n","\n","        self.hidden_size = hidden_size\n","\n","        # A two layer fully-connected network\n","        # hidden_size * 2 --> hidden_size, ReLU, hidden_size --> 1\n","        self.attention_network = nn.Sequential(\n","            nn.Linear(hidden_size * 2, hidden_size),\n","            nn.ReLU(),\n","            nn.Linear(hidden_size, 1),\n","        )\n","\n","        self.softmax = nn.Softmax(dim=1)\n","\n","    def forward(self, queries, keys, values):\n","        \"\"\"The forward pass of the additive attention mechanism.\n","\n","        Arguments:\n","            queries: The current decoder hidden state. (batch_size x hidden_size)\n","            keys: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n","            values: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n","\n","        Returns:\n","            context: weighted average of the values (batch_size x 1 x hidden_size)\n","            attention_weights: Normalized attention weights for each encoder hidden state. (batch_size x seq_len x 1)\n","\n","            The attention_weights must be a softmax weighting over the seq_len annotations.\n","        \"\"\"\n","        batch_size = keys.size(0)\n","        expanded_queries = queries.view(batch_size, -1, self.hidden_size).expand_as(\n","            keys\n","        )\n","        concat_inputs = torch.cat([expanded_queries, keys], dim=2)\n","        unnormalized_attention = self.attention_network(concat_inputs)\n","        attention_weights = self.softmax(unnormalized_attention)\n","        context = torch.bmm(attention_weights.transpose(2, 1), values)\n","        return context, attention_weights"]},{"cell_type":"markdown","metadata":{"id":"73_p8d5EmvOJ"},"source":["## Step 2: RNN + additive attention\n","\n","In the next cell, a modification of our `RNNDecoder` that makes use of an additive attention mechanism as been implemented for your. Please take a momement to read through it and understand what it is doing. See the assignment handouts for details."]},{"cell_type":"code","execution_count":20,"metadata":{"id":"RJaABkXrpJSw","executionInfo":{"status":"ok","timestamp":1647460748441,"user_tz":240,"elapsed":299,"user":{"displayName":"TONGFEI ZHOU","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15125844894166682257"}}},"outputs":[],"source":["class RNNAttentionDecoder(nn.Module):\n","    def __init__(self, vocab_size, hidden_size, attention_type=\"scaled_dot\"):\n","        super(RNNAttentionDecoder, self).__init__()\n","        self.vocab_size = vocab_size\n","        self.hidden_size = hidden_size\n","\n","        self.embedding = nn.Embedding(vocab_size, hidden_size)\n","\n","        self.rnn = MyGRUCell(input_size=hidden_size * 2, hidden_size=hidden_size)\n","        if attention_type == \"additive\":\n","            self.attention = AdditiveAttention(hidden_size=hidden_size)\n","        elif attention_type == \"scaled_dot\":\n","            self.attention = ScaledDotAttention(hidden_size=hidden_size)\n","\n","        self.out = nn.Linear(hidden_size, vocab_size)\n","\n","    def forward(self, inputs, annotations, hidden_init):\n","        \"\"\"Forward pass of the attention-based decoder RNN.\n","\n","        Arguments:\n","            inputs: Input token indexes across a batch for all the time step. (batch_size x decoder_seq_len)\n","            annotations: The encoder hidden states for each step of the input.\n","                         sequence. (batch_size x seq_len x hidden_size)\n","            hidden_init: The final hidden states from the encoder, across a batch. (batch_size x hidden_size)\n","\n","        Returns:\n","            output: Un-normalized scores for each token in the vocabulary, across a batch for all the decoding time steps. (batch_size x decoder_seq_len x vocab_size)\n","            attentions: The stacked attention weights applied to the encoder annotations (batch_size x encoder_seq_len x decoder_seq_len)\n","        \"\"\"\n","\n","        batch_size, seq_len = inputs.size()\n","        embed = self.embedding(inputs)  # batch_size x seq_len x hidden_size\n","\n","        hiddens = []\n","        attentions = []\n","        h_prev = hidden_init\n","\n","        for i in range(seq_len):\n","            embed_current = embed[\n","                :, i, :\n","            ]  # Get the current time step, across the whole batch\n","            context, attention_weights = self.attention(\n","                h_prev, annotations, annotations\n","            )  # batch_size x 1 x hidden_size\n","            embed_and_context = torch.cat(\n","                [embed_current, context.squeeze(1)], dim=1\n","            )  # batch_size x (2*hidden_size)\n","            h_prev = self.rnn(embed_and_context, h_prev)  # batch_size x hidden_size\n","\n","            hiddens.append(h_prev)\n","            attentions.append(attention_weights)\n","\n","        hiddens = torch.stack(hiddens, dim=1)  # batch_size x seq_len x hidden_size\n","        attentions = torch.cat(attentions, dim=2)  # batch_size x seq_len x seq_len\n","\n","        output = self.out(hiddens)  # batch_size x seq_len x vocab_size\n","        return output, attentions"]},{"cell_type":"markdown","metadata":{"id":"vYPae08Io1Fi"},"source":["## Step 3: Training and analysis (with additive attention)\n","\n","Now, run the following cell to train our recurrent encoder-decoder model with additive attention. How does it perform compared to the recurrent encoder-decoder model without attention?"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"ke6t6rCezpZV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647460928736,"user_tz":240,"elapsed":176205,"user":{"displayName":"TONGFEI ZHOU","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15125844894166682257"}},"outputId":"acad70b7-93fa-4c7d-ee50-4706ee688452"},"outputs":[{"output_type":"stream","name":"stdout","text":["================================================================================\n","                                      Opts                                      \n","--------------------------------------------------------------------------------\n","                         data_file_name: pig_latin_small                        \n","                                   cuda: 1                                      \n","                                nepochs: 50                                     \n","                         checkpoint_dir: checkpoints                            \n","                          learning_rate: 0.005                                  \n","                               lr_decay: 0.99                                   \n","                early_stopping_patience: 10                                     \n","                             batch_size: 64                                     \n","                            hidden_size: 64                                     \n","                           encoder_type: rnn                                    \n","                           decoder_type: rnn_attention                          \n","                         attention_type: additive                               \n","================================================================================\n","================================================================================\n","                                   Data Stats                                   \n","--------------------------------------------------------------------------------\n","('substance', 'ubstancesay')\n","('smallness', 'allnesssmay')\n","('garrets', 'arretsgay')\n","('several', 'everalsay')\n","('suspicion', 'uspicionsay')\n","Num unique word pairs: 3198\n","Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n","Vocab size: 29\n","================================================================================\n","Moved models to GPU!\n","Epoch:   0 | Train loss: 1.964 | Val loss: 1.741 | Gen: etay-etay-etay-etay- illay-oarway-oarway- iongway-onday issionssay-inssay-in oray-oday-oday-oday-\n","Epoch:   1 | Train loss: 1.391 | Val loss: 1.430 | Gen: etthtethtethtethteth away-away ontionday issay ordingray\n","Epoch:   2 | Train loss: 1.101 | Val loss: 1.348 | Gen: ethay-itehtehtehteht ararirariray-irarira onday isssway-isway orray-irghorgway\n","Epoch:   3 | Train loss: 0.884 | Val loss: 1.041 | Gen: ethay away ondingway issway orvingway\n","Epoch:   4 | Train loss: 0.705 | Val loss: 0.882 | Gen: ethay ayway onditingcay isway orwingshay\n","Epoch:   5 | Train loss: 0.550 | Val loss: 0.703 | Gen: ethay airway onditingcay isway orkingway\n","Epoch:   6 | Train loss: 0.395 | Val loss: 0.650 | Gen: eway iarway onditingicay isway oringway\n","Epoch:   7 | Train loss: 0.290 | Val loss: 0.526 | Gen: etay airway onditingcay isway orway-igngway\n","Epoch:   8 | Train loss: 0.235 | Val loss: 0.527 | Gen: ethay arirway onditiongcay isway orway-ingway\n","Epoch:   9 | Train loss: 0.264 | Val loss: 0.561 | Gen: ethay airway onditiondcay isway orkingway\n","Epoch:  10 | Train loss: 0.209 | Val loss: 0.411 | Gen: ethay airway onditingcay isway orkingway\n","Epoch:  11 | Train loss: 0.122 | Val loss: 0.311 | Gen: ethay airway onditioncay isway orkingway\n","Epoch:  12 | Train loss: 0.078 | Val loss: 0.308 | Gen: ethay airway onditiongcay isway orkingway\n","Epoch:  13 | Train loss: 0.100 | Val loss: 0.396 | Gen: ethay airiway onditioncay isway orkingway\n","Epoch:  14 | Train loss: 0.126 | Val loss: 0.549 | Gen: ethay airway onditingway isway orkingway\n","Epoch:  15 | Train loss: 0.113 | Val loss: 0.261 | Gen: ethay airway onditiongcay isway orkingway\n","Epoch:  16 | Train loss: 0.057 | Val loss: 0.225 | Gen: ethay airway onditiongcay isway orkingway\n","Epoch:  17 | Train loss: 0.036 | Val loss: 0.172 | Gen: ethay airway onditioningcay isway orkingway\n","Epoch:  18 | Train loss: 0.019 | Val loss: 0.201 | Gen: ethay airway ondditioncay isway orkingway\n","Epoch:  19 | Train loss: 0.021 | Val loss: 0.178 | Gen: ethay airway onditioningcay isway orkingway\n","Epoch:  20 | Train loss: 0.011 | Val loss: 0.156 | Gen: ethay airway onditiongcay isway orkingway\n","Epoch:  21 | Train loss: 0.008 | Val loss: 0.170 | Gen: ethay airway onditioningcay isway orkingway\n","Epoch:  22 | Train loss: 0.007 | Val loss: 0.155 | Gen: ethay airway onditiongcay isway orkingway\n","Epoch:  23 | Train loss: 0.006 | Val loss: 0.152 | Gen: ethay airway onditiongcay isway orkingway\n","Epoch:  24 | Train loss: 0.004 | Val loss: 0.144 | Gen: ethay airway onditiongcay isway orkingway\n","Epoch:  25 | Train loss: 0.003 | Val loss: 0.144 | Gen: ethay airway onditiongcay isway orkingway\n","Epoch:  26 | Train loss: 0.003 | Val loss: 0.143 | Gen: ethay airway onditiongcay isway orkingway\n","Epoch:  27 | Train loss: 0.002 | Val loss: 0.143 | Gen: ethay airway onditiongcay isway orkingway\n","Epoch:  28 | Train loss: 0.002 | Val loss: 0.142 | Gen: ethay airway onditiongcay isway orkingway\n","Epoch:  29 | Train loss: 0.002 | Val loss: 0.142 | Gen: ethay airway onditiongcay isway orkingway\n","Epoch:  30 | Train loss: 0.002 | Val loss: 0.141 | Gen: ethay airway onditiongcay isway orkingway\n","Epoch:  31 | Train loss: 0.002 | Val loss: 0.141 | Gen: ethay airway onditiongcay isway orkingway\n","Epoch:  32 | Train loss: 0.001 | Val loss: 0.140 | Gen: ethay airway onditiongcay isway orkingway\n","Epoch:  33 | Train loss: 0.001 | Val loss: 0.140 | Gen: ethay airway onditiongcay isway orkingway\n","Epoch:  34 | Train loss: 0.001 | Val loss: 0.140 | Gen: ethay airway onditiongcay isway orkingway\n","Epoch:  35 | Train loss: 0.001 | Val loss: 0.140 | Gen: ethay airway onditiongcay isway orkingway\n","Epoch:  36 | Train loss: 0.001 | Val loss: 0.139 | Gen: ethay airway onditiongcay isway orkingway\n","Epoch:  37 | Train loss: 0.001 | Val loss: 0.139 | Gen: ethay airway onditiongcay isway orkingway\n","Epoch:  38 | Train loss: 0.001 | Val loss: 0.139 | Gen: ethay airway onditiongcay isway orkingway\n","Epoch:  39 | Train loss: 0.001 | Val loss: 0.139 | Gen: ethay airway onditiongcay isway orkingway\n","Epoch:  40 | Train loss: 0.001 | Val loss: 0.138 | Gen: ethay airway onditiongcay isway orkingway\n","Epoch:  41 | Train loss: 0.001 | Val loss: 0.138 | Gen: ethay airway onditiongcay isway orkingway\n","Epoch:  42 | Train loss: 0.001 | Val loss: 0.138 | Gen: ethay airway onditiongcay isway orkingway\n","Epoch:  43 | Train loss: 0.001 | Val loss: 0.138 | Gen: ethay airway onditiongcay isway orkingway\n","Epoch:  44 | Train loss: 0.001 | Val loss: 0.138 | Gen: ethay airway onditiongcay isway orkingway\n","Epoch:  45 | Train loss: 0.001 | Val loss: 0.138 | Gen: ethay airway onditiongcay isway orkingway\n","Epoch:  46 | Train loss: 0.001 | Val loss: 0.138 | Gen: ethay airway onditiongcay isway orkingway\n","Epoch:  47 | Train loss: 0.001 | Val loss: 0.138 | Gen: ethay airway onditiongcay isway orkingway\n","Epoch:  48 | Train loss: 0.001 | Val loss: 0.139 | Gen: ethay airway onditiongcay isway orkingway\n","Epoch:  49 | Train loss: 0.000 | Val loss: 0.139 | Gen: ethay airway onditiongcay isway orkingway\n","Obtained lowest validation loss of: 0.13802540764219076\n","source:\t\tthe air conditioning is working \n","translated:\tethay airway onditiongcay isway orkingway\n"]}],"source":["TEST_SENTENCE = \"the air conditioning is working\"\n","\n","rnn_attn_args = AttrDict()\n","args_dict = {\n","    \"data_file_name\": \"pig_latin_small\",\n","    \"cuda\": True,\n","    \"nepochs\": 50,\n","    \"checkpoint_dir\": \"checkpoints\",\n","    \"learning_rate\": 0.005,\n","    \"lr_decay\": 0.99,\n","    \"early_stopping_patience\": 10,\n","    \"batch_size\": 64,\n","    \"hidden_size\": 64,\n","    \"encoder_type\": \"rnn\",            # options: rnn / transformer\n","    \"decoder_type\": \"rnn_attention\",  # options: rnn / rnn_attention / transformer\n","    \"attention_type\": \"additive\",     # options: additive / scaled_dot\n","}\n","rnn_attn_args.update(args_dict)\n","\n","print_opts(rnn_attn_args)\n","rnn_attn_encoder, rnn_attn_decoder, rnn_attn_losses = train(rnn_attn_args)\n","\n","translated = translate_sentence(\n","    TEST_SENTENCE, rnn_attn_encoder, rnn_attn_decoder, None, rnn_attn_args\n",")\n","print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"]},{"cell_type":"code","execution_count":22,"metadata":{"id":"VNVKbLc0ACj_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647460929095,"user_tz":240,"elapsed":388,"user":{"displayName":"TONGFEI ZHOU","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15125844894166682257"}},"outputId":"38a4a4cd-4814-403e-d84b-06f88f3659d3"},"outputs":[{"output_type":"stream","name":"stdout","text":["source:\t\tthe air conditioning is working \n","translated:\tethay airway onditiongcay isway orkingway\n"]}],"source":["TEST_SENTENCE = \"the air conditioning is working\"\n","translated = translate_sentence(\n","    TEST_SENTENCE, rnn_attn_encoder, rnn_attn_decoder, None, rnn_attn_args\n",")\n","print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"]},{"cell_type":"markdown","metadata":{"id":"xq7nhsEio1w-"},"source":["## Step 4: Implement scaled dot-product attention\n","\n","In the next cell, you will implement the [scaled dot-product attention](https://paperswithcode.com/method/scaled) mechanism. See the assignment handouts for details."]},{"cell_type":"code","execution_count":23,"metadata":{"id":"d_j3oY3hqsJQ","executionInfo":{"status":"ok","timestamp":1647460929099,"user_tz":240,"elapsed":34,"user":{"displayName":"TONGFEI ZHOU","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15125844894166682257"}}},"outputs":[],"source":["class ScaledDotAttention(nn.Module):\n","    def __init__(self, hidden_size):\n","        super(ScaledDotAttention, self).__init__()\n","\n","        self.hidden_size = hidden_size\n","\n","        self.Q = nn.Linear(hidden_size, hidden_size)\n","        self.K = nn.Linear(hidden_size, hidden_size)\n","        self.V = nn.Linear(hidden_size, hidden_size)\n","        self.softmax = nn.Softmax(dim=1)\n","        self.scaling_factor = torch.rsqrt(\n","            torch.tensor(self.hidden_size, dtype=torch.float)\n","        )\n","\n","    def forward(self, queries, keys, values):\n","        \"\"\"The forward pass of the scaled dot attention mechanism.\n","\n","        Arguments:\n","            queries: The current decoder hidden state, 2D or 3D tensor. (batch_size x (k) x hidden_size)\n","            keys: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n","            values: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n","\n","        Returns:\n","            context: weighted average of the values (batch_size x k x hidden_size)\n","            attention_weights: Normalized attention weights for each encoder hidden state. (batch_size x seq_len x k)\n","\n","            The output must be a softmax weighting over the seq_len annotations.\n","        \"\"\"\n","\n","        # ------------\n","        # FILL THIS IN\n","        # ------------\n","        batch_size = keys.size(0)\n","        if queries.dim() == 2:\n","          queries = queries.unsqueeze(queries, 1)\n","        q = self.Q(queries)\n","        k = self.K(keys)\n","        v = self.V(values)\n","        unnormalized_attention = torch.bmm(k, q.transpose(2,1)) * self.scaling_factor\n","        attention_weights = self.softmax(unnormalized_attention)\n","        context = torch.bmm(attention_weights.transpose(2,1), v)\n","        return context, attention_weights"]},{"cell_type":"markdown","metadata":{"id":"unReAOrjo113"},"source":["## Step 5: Implement causal dot-product Attention\n","\n","\n","Now, implement the casual scaled dot-product attention mechanism. It will be very similar to your implementation for `ScaledDotAttention`. The additional step is to mask out the attention to future timesteps so this attention mechanism can be used in a decoder. See the assignment handouts for details."]},{"cell_type":"code","execution_count":24,"metadata":{"id":"ovigzQffrKqj","executionInfo":{"status":"ok","timestamp":1647460929102,"user_tz":240,"elapsed":34,"user":{"displayName":"TONGFEI ZHOU","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15125844894166682257"}}},"outputs":[],"source":["class CausalScaledDotAttention(nn.Module):\n","    def __init__(self, hidden_size):\n","        super(CausalScaledDotAttention, self).__init__()\n","\n","        self.hidden_size = hidden_size\n","        self.neg_inf = torch.tensor(-1e7)\n","\n","        self.Q = nn.Linear(hidden_size, hidden_size)\n","        self.K = nn.Linear(hidden_size, hidden_size)\n","        self.V = nn.Linear(hidden_size, hidden_size)\n","        self.softmax = nn.Softmax(dim=1)\n","        self.scaling_factor = torch.rsqrt(\n","            torch.tensor(self.hidden_size, dtype=torch.float)\n","        )\n","\n","    def forward(self, queries, keys, values):\n","        \"\"\"The forward pass of the scaled dot attention mechanism.\n","\n","        Arguments:\n","            queries: The current decoder hidden state, 2D or 3D tensor. (batch_size x (k) x hidden_size)\n","            keys: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n","            values: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n","\n","        Returns:\n","            context: weighted average of the values (batch_size x k x hidden_size)\n","            attention_weights: Normalized attention weights for each encoder hidden state. (batch_size x seq_len x k)\n","\n","            The output must be a softmax weighting over the seq_len annotations.\n","        \"\"\"\n","\n","        # ------------\n","        # FILL THIS IN\n","        # ------------\n","        batch_size = keys.size(0)\n","        if queries.dim() == 2:\n","          queries = queries.unsqueeze(queries, 1)\n","        q = self.Q(queries)\n","        k = self.K(keys)\n","        v = self.V(values)\n","        unnormalized_attention = torch.bmm(k, q.transpose(2,1)) * self.scaling_factor\n","        mask = torch.triu(unnormalized_attention)\n","        mask[mask == 0] = self.neg_inf\n","        attention_weights = self.softmax(mask)\n","        context = torch.bmm(attention_weights.transpose(2,1), v)\n","        return context, attention_weights"]},{"cell_type":"markdown","source":["## Step 6: Attention encoder and decoder\n","\n","The following cells provide an implementation of an encoder and decoder that use a single `ScaledDotAttention` block. Please read through them to understand what they are doing."],"metadata":{"id":"ZkjHbtvT6Qxs"}},{"cell_type":"code","source":["class AttentionEncoder(nn.Module):\n","    def __init__(self, vocab_size, hidden_size, opts):\n","        super(AttentionEncoder, self).__init__()\n","\n","        self.vocab_size = vocab_size\n","        self.hidden_size = hidden_size\n","        self.opts = opts\n","\n","        self.embedding = nn.Embedding(vocab_size, hidden_size)\n","\n","        self.self_attention = ScaledDotAttention(\n","                    hidden_size=hidden_size,\n","                )\n","               \n","        self.attention_mlp = nn.Sequential(\n","                                nn.Linear(hidden_size, hidden_size),\n","                                nn.ReLU(),\n","                              )\n","\n","    def forward(self, inputs):\n","        \"\"\"Forward pass of the encoder scaled dot attention.\n","\n","        Arguments:\n","            inputs: Input token indexes across a batch for all time steps in the sequence. (batch_size x seq_len)\n","\n","        Returns:\n","            annotations: The hidden states computed at each step of the input sequence. (batch_size x seq_len x hidden_size)\n","            None: Used to conform to standard encoder return signature.\n","        \"\"\"\n","        batch_size, seq_len = inputs.size()\n","\n","        encoded = self.embedding(inputs)  # batch_size x seq_len x hidden_size\n","\n","        annotations = encoded\n","        new_annotations, self_attention_weights = self.self_attention(\n","            annotations, annotations, annotations\n","        )  # batch_size x seq_len x hidden_size\n","        residual_annotations = annotations + new_annotations\n","        new_annotations = self.attention_mlp(residual_annotations)\n","        annotations = residual_annotations + new_annotations\n","\n","        return annotations, None\n"],"metadata":{"id":"yKGNqUaX6RLO","executionInfo":{"status":"ok","timestamp":1647460929104,"user_tz":240,"elapsed":33,"user":{"displayName":"TONGFEI ZHOU","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15125844894166682257"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["class AttentionDecoder(nn.Module):\n","    def __init__(self, vocab_size, hidden_size):\n","        super(AttentionDecoder, self).__init__()\n","        self.vocab_size = vocab_size\n","        self.hidden_size = hidden_size\n","\n","        self.embedding = nn.Embedding(vocab_size, hidden_size)\n","\n","        self.self_attention = CausalScaledDotAttention(\n","                                hidden_size=hidden_size,\n","                                )\n","                \n","        self.decoder_attention = ScaledDotAttention(\n","                                  hidden_size=hidden_size,\n","                                  )\n","                \n","        self.attention_mlp = nn.Sequential(\n","                                nn.Linear(hidden_size, hidden_size),\n","                                nn.ReLU(),\n","                              )\n","                \n","        self.out = nn.Linear(hidden_size, vocab_size)\n","\n","\n","    def forward(self, inputs, annotations, hidden_init):\n","        \"\"\"Forward pass of the attention-based decoder RNN.\n","\n","        Arguments:\n","            inputs: Input token indexes across a batch for all the time step. (batch_size x decoder_seq_len)\n","            annotations: The encoder hidden states for each step of the input.\n","                         sequence. (batch_size x seq_len x hidden_size)\n","            hidden_init: Not used in the transformer decoder\n","        Returns:\n","            output: Un-normalized scores for each token in the vocabulary, across a batch for all the decoding time steps. (batch_size x decoder_seq_len x vocab_size)\n","            attentions: The stacked attention weights applied to the encoder annotations (batch_size x encoder_seq_len x decoder_seq_len)\n","        \"\"\"\n","\n","        batch_size, seq_len = inputs.size()\n","        embed = self.embedding(inputs)  # batch_size x seq_len x hidden_size\n","\n","        encoder_attention_weights_list = []\n","        self_attention_weights_list = []\n","        contexts = embed\n","        new_contexts, self_attention_weights = self.self_attention(\n","            contexts, contexts, contexts\n","        )  # batch_size x seq_len x hidden_size\n","        residual_contexts = contexts + new_contexts\n","        new_contexts, encoder_attention_weights = self.decoder_attention(\n","            residual_contexts, annotations, annotations\n","        )  # batch_size x seq_len x hidden_size\n","        residual_contexts = residual_contexts + new_contexts\n","        new_contexts = self.attention_mlp(residual_contexts)\n","        contexts = residual_contexts + new_contexts\n","\n","        encoder_attention_weights_list.append(encoder_attention_weights)\n","        self_attention_weights_list.append(self_attention_weights)\n","\n","        output = self.out(contexts)\n","        encoder_attention_weights = torch.stack(encoder_attention_weights_list)\n","        self_attention_weights = torch.stack(self_attention_weights_list)\n","\n","        return output, (encoder_attention_weights, self_attention_weights)"],"metadata":{"id":"vDUvtOee7cMy","executionInfo":{"status":"ok","timestamp":1647460929106,"user_tz":240,"elapsed":33,"user":{"displayName":"TONGFEI ZHOU","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15125844894166682257"}}},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":["## Step 7: Training and analysis (single scaled dot-product attention block)\n","\n","Now, train the following model, with an encoder and decoder each composed a single `ScaledDotAttention` block."],"metadata":{"id":"B7gJLw5t_rnW"}},{"cell_type":"code","source":["TEST_SENTENCE = \"the air conditioning is working\"\n","\n","attention_args_s = AttrDict()\n","args_dict = {\n","    \"data_file_name\": \"pig_latin_small\",\n","    \"cuda\": True,\n","    \"nepochs\": 100,\n","    \"checkpoint_dir\": \"checkpoints\",\n","    \"learning_rate\": 5e-4,\n","    \"early_stopping_patience\": 100,\n","    \"lr_decay\": 0.99,\n","    \"batch_size\": 64,\n","    \"hidden_size\": 32,\n","    \"encoder_type\": \"attention\",\n","    \"decoder_type\": \"attention\",  # options: rnn / rnn_attention / attention / transformer\n","}\n","attention_args_s.update(args_dict)\n","print_opts(attention_args_s)\n","\n","attention_encoder_s, attention_decoder_s, attention_losses_s = train(attention_args_s)\n","\n","translated = translate_sentence(\n","    TEST_SENTENCE, attention_encoder_s, attention_decoder_s, None, attention_args_s\n",")\n","print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"],"metadata":{"id":"7MOkZonC8T3f","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647207123620,"user_tz":240,"elapsed":66768,"user":{"displayName":"TONGFEI ZHOU","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15125844894166682257"}},"outputId":"801962f2-dd75-4403-e30a-642f85e6f4de"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["================================================================================\n","                                      Opts                                      \n","--------------------------------------------------------------------------------\n","                         data_file_name: pig_latin_small                        \n","                                   cuda: 1                                      \n","                                nepochs: 100                                    \n","                         checkpoint_dir: checkpoints                            \n","                          learning_rate: 0.0005                                 \n","                early_stopping_patience: 100                                    \n","                               lr_decay: 0.99                                   \n","                             batch_size: 64                                     \n","                            hidden_size: 32                                     \n","                           encoder_type: attention                              \n","                           decoder_type: attention                              \n","================================================================================\n","================================================================================\n","                                   Data Stats                                   \n","--------------------------------------------------------------------------------\n","('sheath', 'eathshay')\n","('lucy', 'ucylay')\n","('inventive', 'inventiveway')\n","('bedroom', 'edroombay')\n","('anything', 'anythingway')\n","Num unique word pairs: 3198\n","Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n","Vocab size: 29\n","================================================================================\n","Moved models to GPU!\n","Epoch:   0 | Train loss: 3.112 | Val loss: 2.549 | Gen: ay ay ay ay ay      \n","Epoch:   1 | Train loss: 2.351 | Val loss: 2.248 | Gen: ay ay innay ay ay   \n","Epoch:   2 | Train loss: 2.121 | Val loss: 2.096 | Gen: ay ay innay isisisisisisisisisis ay\n","Epoch:   3 | Train loss: 1.988 | Val loss: 1.994 | Gen: ay ay insay isisisisisisisisisis ingay\n","Epoch:   4 | Train loss: 1.891 | Val loss: 1.927 | Gen: ay iway intintintinay isisisisisisisisisay ingay\n","Epoch:   5 | Train loss: 1.814 | Val loss: 1.866 | Gen: ay iway ingay isisisisisisisay ongay\n","Epoch:   6 | Train loss: 1.747 | Val loss: 1.812 | Gen: ay iway ingay isisisisisisisisisay ongay\n","Epoch:   7 | Train loss: 1.688 | Val loss: 1.760 | Gen: ay iway ingay isisisisisisisisisis ongway\n","Epoch:   8 | Train loss: 1.635 | Val loss: 1.715 | Gen: ethay ay ongay isisisissisisissway ongway\n","Epoch:   9 | Train loss: 1.588 | Val loss: 1.678 | Gen: ethay ay ongay isisissississway ongway\n","Epoch:  10 | Train loss: 1.548 | Val loss: 1.653 | Gen: ethay ay ongay isissississway ongway\n","Epoch:  11 | Train loss: 1.516 | Val loss: 1.632 | Gen: ethay ay ongay isisssissway ongway\n","Epoch:  12 | Train loss: 1.489 | Val loss: 1.614 | Gen: ethay ay ongay ississisway ongway\n","Epoch:  13 | Train loss: 1.464 | Val loss: 1.599 | Gen: ethay ay ingway ississisway ongway\n","Epoch:  14 | Train loss: 1.443 | Val loss: 1.587 | Gen: ethay ay ingway ississway ongway\n","Epoch:  15 | Train loss: 1.423 | Val loss: 1.574 | Gen: ethay ay ingway ississway ongway\n","Epoch:  16 | Train loss: 1.403 | Val loss: 1.560 | Gen: ethay ay ingway ississway ongway\n","Epoch:  17 | Train loss: 1.385 | Val loss: 1.548 | Gen: ethay ay ingway ississway ongway\n","Epoch:  18 | Train loss: 1.368 | Val loss: 1.534 | Gen: ethay ay ingway issisway ongway\n","Epoch:  19 | Train loss: 1.351 | Val loss: 1.522 | Gen: ethay ay ingway issisway ongway\n","Epoch:  20 | Train loss: 1.335 | Val loss: 1.509 | Gen: ethay ay ingway issisway ongway\n","Epoch:  21 | Train loss: 1.319 | Val loss: 1.495 | Gen: ethay ay ingway issisway ongray\n","Epoch:  22 | Train loss: 1.304 | Val loss: 1.483 | Gen: ethay ay ingway issisway ongray\n","Epoch:  23 | Train loss: 1.290 | Val loss: 1.471 | Gen: ethay ay ingway issisway ongray\n","Epoch:  24 | Train loss: 1.276 | Val loss: 1.459 | Gen: ethay ay ingway issisway ongray\n","Epoch:  25 | Train loss: 1.262 | Val loss: 1.447 | Gen: ethay ay ingway issway ongray\n","Epoch:  26 | Train loss: 1.249 | Val loss: 1.438 | Gen: ethay ay ingway issway ongray\n","Epoch:  27 | Train loss: 1.236 | Val loss: 1.429 | Gen: ethay ay ingontindway issway okgringway\n","Epoch:  28 | Train loss: 1.224 | Val loss: 1.421 | Gen: ethay ay ingontindway issway okgringway\n","Epoch:  29 | Train loss: 1.212 | Val loss: 1.411 | Gen: ethay ay ingontindway issway okgringway\n","Epoch:  30 | Train loss: 1.201 | Val loss: 1.404 | Gen: ethay ay ingontindway issway okgringway\n","Epoch:  31 | Train loss: 1.190 | Val loss: 1.395 | Gen: ethay ay ingontindway issway okgringway\n","Epoch:  32 | Train loss: 1.179 | Val loss: 1.388 | Gen: ethay ay ingontindway issway okgringway\n","Epoch:  33 | Train loss: 1.168 | Val loss: 1.381 | Gen: ethay ay ingontindway issway okgringway\n","Epoch:  34 | Train loss: 1.158 | Val loss: 1.374 | Gen: ethay ay ingontindway issway okgringway\n","Epoch:  35 | Train loss: 1.149 | Val loss: 1.367 | Gen: ethay ay ingontiondway issway okgringway\n","Epoch:  36 | Train loss: 1.140 | Val loss: 1.360 | Gen: ethay ay ingontiondway issway okgringway\n","Epoch:  37 | Train loss: 1.130 | Val loss: 1.354 | Gen: ethay ay ingontiondway issway okgringway\n","Epoch:  38 | Train loss: 1.122 | Val loss: 1.347 | Gen: ethay ay ongay issway okgringway\n","Epoch:  39 | Train loss: 1.113 | Val loss: 1.341 | Gen: ethay ay ongay issway okgringway\n","Epoch:  40 | Train loss: 1.105 | Val loss: 1.334 | Gen: ethay ay ongay issway okgringway\n","Epoch:  41 | Train loss: 1.097 | Val loss: 1.328 | Gen: ethay ay ongay issway okgringway\n","Epoch:  42 | Train loss: 1.089 | Val loss: 1.321 | Gen: ethay ay ongay issway okgringway\n","Epoch:  43 | Train loss: 1.081 | Val loss: 1.316 | Gen: ethay ay ongay issway okgringway\n","Epoch:  44 | Train loss: 1.074 | Val loss: 1.310 | Gen: ethay ay ongay issway okgringway\n","Epoch:  45 | Train loss: 1.067 | Val loss: 1.304 | Gen: ethay ay ongay issway okgringway\n","Epoch:  46 | Train loss: 1.061 | Val loss: 1.300 | Gen: ethay ay oningway issway okgringway\n","Epoch:  47 | Train loss: 1.055 | Val loss: 1.294 | Gen: ethay ay oningway issway okgringway\n","Epoch:  48 | Train loss: 1.049 | Val loss: 1.289 | Gen: ethay ay oningway issway okgringway\n","Epoch:  49 | Train loss: 1.043 | Val loss: 1.285 | Gen: ethay ay oningdiontiongway issay okgringway\n","Epoch:  50 | Train loss: 1.038 | Val loss: 1.281 | Gen: ethay ay oningdiontiongway issay okgringway\n","Epoch:  51 | Train loss: 1.033 | Val loss: 1.278 | Gen: ethay ay oningdiontiongway issay okgringway\n","Epoch:  52 | Train loss: 1.027 | Val loss: 1.276 | Gen: ethay ay oningdiontiongway issay okgringway\n","Epoch:  53 | Train loss: 1.021 | Val loss: 1.272 | Gen: ethay ay oningdiontiongway issay okgringway\n","Epoch:  54 | Train loss: 1.015 | Val loss: 1.270 | Gen: ethay ay oningdiontiongway issay okgringway\n","Epoch:  55 | Train loss: 1.010 | Val loss: 1.269 | Gen: ethay ay oningdiontiongway issay okgringway\n","Epoch:  56 | Train loss: 1.005 | Val loss: 1.266 | Gen: ethay ay oningdiontiongway issay okgringway\n","Epoch:  57 | Train loss: 1.000 | Val loss: 1.262 | Gen: ethay ay oningdiontiongway issay okgringway\n","Epoch:  58 | Train loss: 0.995 | Val loss: 1.256 | Gen: ethay ay oningdiontiongway issay okgringway\n","Epoch:  59 | Train loss: 0.990 | Val loss: 1.253 | Gen: ethay ay oningdiontiongway issay okgringway\n","Epoch:  60 | Train loss: 0.985 | Val loss: 1.245 | Gen: ethay ay oningdiontiongway issay okgringway\n","Epoch:  61 | Train loss: 0.980 | Val loss: 1.245 | Gen: ethay ay oningdiontiongway issay okgringway\n","Epoch:  62 | Train loss: 0.976 | Val loss: 1.242 | Gen: ethay ay oningdiontiongway issay okgringway\n","Epoch:  63 | Train loss: 0.972 | Val loss: 1.238 | Gen: ethay ay oningdiontiongway issay okingray\n","Epoch:  64 | Train loss: 0.968 | Val loss: 1.233 | Gen: ethay ay oningdiontiongway issay okingray\n","Epoch:  65 | Train loss: 0.964 | Val loss: 1.226 | Gen: ethay ay oningdiontiongway issay okingray\n","Epoch:  66 | Train loss: 0.959 | Val loss: 1.223 | Gen: ethay ay oningdiontiongway issay okingray\n","Epoch:  67 | Train loss: 0.956 | Val loss: 1.227 | Gen: ethay ay oningdiontiongway issay okingray\n","Epoch:  68 | Train loss: 0.952 | Val loss: 1.221 | Gen: ethay ay oningdiontiongway issay okingray\n","Epoch:  69 | Train loss: 0.949 | Val loss: 1.217 | Gen: ethay ay oningdiontiongway issay okingray\n","Epoch:  70 | Train loss: 0.945 | Val loss: 1.211 | Gen: ethay ay oningdiontiongway issay okingray\n","Epoch:  71 | Train loss: 0.941 | Val loss: 1.208 | Gen: ethay ay oningdiontiongway issay okingray\n","Epoch:  72 | Train loss: 0.938 | Val loss: 1.205 | Gen: ethay ay oningdiontiongway issay okingray\n","Epoch:  73 | Train loss: 0.934 | Val loss: 1.202 | Gen: ethay ay oningdiontiongway issay okingray\n","Epoch:  74 | Train loss: 0.930 | Val loss: 1.198 | Gen: ethay ay oningdiontiongway issay okingray\n","Epoch:  75 | Train loss: 0.926 | Val loss: 1.196 | Gen: ethay ay oningdiontiongway issay okingray\n","Epoch:  76 | Train loss: 0.923 | Val loss: 1.193 | Gen: ethay ay oningdiontiongway issay okingray\n","Epoch:  77 | Train loss: 0.921 | Val loss: 1.192 | Gen: ethay ay oningdiontiongway issay okingray\n","Epoch:  78 | Train loss: 0.917 | Val loss: 1.192 | Gen: ethay ay oningdiontiongway issay okingray\n","Epoch:  79 | Train loss: 0.915 | Val loss: 1.199 | Gen: ethay ay oningdiontiongway issay okingray\n","Epoch:  80 | Train loss: 0.912 | Val loss: 1.196 | Gen: ethay iriay oningdiontiongway isway okingray\n","Epoch:  81 | Train loss: 0.909 | Val loss: 1.187 | Gen: ethay ay oningdiontiongway isway okingray\n","Epoch:  82 | Train loss: 0.906 | Val loss: 1.185 | Gen: ethay ay oningdiontiongway isway okingray\n","Epoch:  83 | Train loss: 0.903 | Val loss: 1.185 | Gen: ethay ay oningdiontiongway isway okingray\n","Epoch:  84 | Train loss: 0.901 | Val loss: 1.195 | Gen: ethay iriay oningdiontiongway isway okingray\n","Epoch:  85 | Train loss: 0.898 | Val loss: 1.184 | Gen: ethay iriay oningdiontiongway isway okingray\n","Epoch:  86 | Train loss: 0.897 | Val loss: 1.193 | Gen: ethay iriay oningdiontiongway isway okingray\n","Epoch:  87 | Train loss: 0.895 | Val loss: 1.179 | Gen: ethay iriay oningdiontiongway isway okingray\n","Epoch:  88 | Train loss: 0.891 | Val loss: 1.175 | Gen: ethay iriay oningdiontiongway isway okingray\n","Epoch:  89 | Train loss: 0.889 | Val loss: 1.172 | Gen: ethay iriay oningdiontiongway isway okingray\n","Epoch:  90 | Train loss: 0.885 | Val loss: 1.171 | Gen: ethay ay oningdiontiongway isway okingray\n","Epoch:  91 | Train loss: 0.879 | Val loss: 1.172 | Gen: ethay ay oningdway isway okingray\n","Epoch:  92 | Train loss: 0.874 | Val loss: 1.168 | Gen: ethay ay oningdiontiongway isway okingray\n","Epoch:  93 | Train loss: 0.871 | Val loss: 1.168 | Gen: ethay ay oningdiontiongway isway okingray\n","Epoch:  94 | Train loss: 0.870 | Val loss: 1.165 | Gen: ethay iay oningdiontiongway isway okingray\n","Epoch:  95 | Train loss: 0.863 | Val loss: 1.173 | Gen: ethay iay oningdiontiongway isway okingray\n","Epoch:  96 | Train loss: 0.859 | Val loss: 1.161 | Gen: ethay ay oningdiontiongway isway okingray\n","Epoch:  97 | Train loss: 0.853 | Val loss: 1.157 | Gen: ethay ay oningdiontiongway isway okingray\n","Epoch:  98 | Train loss: 0.848 | Val loss: 1.155 | Gen: ethay ay oningdiontiongway isway okingray\n","Epoch:  99 | Train loss: 0.843 | Val loss: 1.153 | Gen: ethay ay oningdiontiongway isway okingray\n","Obtained lowest validation loss of: 1.1528463782299132\n","source:\t\tthe air conditioning is working \n","translated:\tethay ay oningdiontiongway isway okingray\n"]}]},{"cell_type":"markdown","metadata":{"id":"9tcpUFKqo2Oi"},"source":["## Step 8: Transformer encoder and decoder\n","\n","The following cells provide an implementation of the transformer encoder and decoder that use your `ScaledDotAttention` and `CausalScaledDotAttention`. Please read through them to understand what they are doing."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N3B-fWsarlVk"},"outputs":[],"source":["class TransformerEncoder(nn.Module):\n","    def __init__(self, vocab_size, hidden_size, num_layers, opts):\n","        super(TransformerEncoder, self).__init__()\n","\n","        self.vocab_size = vocab_size\n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","        self.opts = opts\n","\n","        self.embedding = nn.Embedding(vocab_size, hidden_size)\n","\n","        self.self_attentions = nn.ModuleList(\n","            [\n","                ScaledDotAttention(\n","                    hidden_size=hidden_size,\n","                )\n","                for i in range(self.num_layers)\n","            ]\n","        )\n","        self.attention_mlps = nn.ModuleList(\n","            [\n","                nn.Sequential(\n","                    nn.Linear(hidden_size, hidden_size),\n","                    nn.ReLU(),\n","                )\n","                for i in range(self.num_layers)\n","            ]\n","        )\n","\n","        self.positional_encodings = self.create_positional_encodings()\n","\n","    def forward(self, inputs):\n","        \"\"\"Forward pass of the encoder RNN.\n","\n","        Arguments:\n","            inputs: Input token indexes across a batch for all time steps in the sequence. (batch_size x seq_len)\n","\n","        Returns:\n","            annotations: The hidden states computed at each step of the input sequence. (batch_size x seq_len x hidden_size)\n","            None: Used to conform to standard encoder return signature.\n","            None: Used to conform to standard encoder return signature.\n","        \"\"\"\n","        batch_size, seq_len = inputs.size()\n","\n","        encoded = self.embedding(inputs)  # batch_size x seq_len x hidden_size\n","\n","        # Add positinal embeddings from self.create_positional_encodings. (a'la https://arxiv.org/pdf/1706.03762.pdf, section 3.5)\n","        encoded = encoded + self.positional_encodings[:seq_len]\n","\n","        annotations = encoded\n","        for i in range(self.num_layers):\n","            new_annotations, self_attention_weights = self.self_attentions[i](\n","                annotations, annotations, annotations\n","            )  # batch_size x seq_len x hidden_size\n","            residual_annotations = annotations + new_annotations\n","            new_annotations = self.attention_mlps[i](residual_annotations)\n","            annotations = residual_annotations + new_annotations\n","\n","        # Transformer encoder does not have a last hidden or cell layer.\n","        return annotations, None\n","        # return annotations, None, None\n","    def create_positional_encodings(self, max_seq_len=1000):\n","        \"\"\"Creates positional encodings for the inputs.\n","\n","        Arguments:\n","            max_seq_len: a number larger than the maximum string length we expect to encounter during training\n","\n","        Returns:\n","            pos_encodings: (max_seq_len, hidden_dim) Positional encodings for a sequence with length max_seq_len.\n","        \"\"\"\n","        pos_indices = torch.arange(max_seq_len)[..., None]\n","        dim_indices = torch.arange(self.hidden_size // 2)[None, ...]\n","        exponents = (2 * dim_indices).float() / (self.hidden_size)\n","        trig_args = pos_indices / (10000**exponents)\n","        sin_terms = torch.sin(trig_args)\n","        cos_terms = torch.cos(trig_args)\n","\n","        pos_encodings = torch.zeros((max_seq_len, self.hidden_size))\n","        pos_encodings[:, 0::2] = sin_terms\n","        pos_encodings[:, 1::2] = cos_terms\n","\n","        if self.opts.cuda:\n","            pos_encodings = pos_encodings.cuda()\n","\n","        return pos_encodings"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nyvTZFxtrvc6"},"outputs":[],"source":["class TransformerDecoder(nn.Module):\n","    def __init__(self, vocab_size, hidden_size, num_layers):\n","        super(TransformerDecoder, self).__init__()\n","        self.vocab_size = vocab_size\n","        self.hidden_size = hidden_size\n","\n","        self.embedding = nn.Embedding(vocab_size, hidden_size)\n","        self.num_layers = num_layers\n","\n","        self.self_attentions = nn.ModuleList(\n","            [\n","                CausalScaledDotAttention(\n","                    hidden_size=hidden_size,\n","                )\n","                for i in range(self.num_layers)\n","            ]\n","        )\n","        self.encoder_attentions = nn.ModuleList(\n","            [\n","                ScaledDotAttention(\n","                    hidden_size=hidden_size,\n","                )\n","                for i in range(self.num_layers)\n","            ]\n","        )\n","        self.attention_mlps = nn.ModuleList(\n","            [\n","                nn.Sequential(\n","                    nn.Linear(hidden_size, hidden_size),\n","                    nn.ReLU(),\n","                )\n","                for i in range(self.num_layers)\n","            ]\n","        )\n","        self.out = nn.Linear(hidden_size, vocab_size)\n","\n","        self.positional_encodings = self.create_positional_encodings()\n","\n","    def forward(self, inputs, annotations, hidden_init):\n","        \"\"\"Forward pass of the attention-based decoder RNN.\n","\n","        Arguments:\n","            inputs: Input token indexes across a batch for all the time step. (batch_size x decoder_seq_len)\n","            annotations: The encoder hidden states for each step of the input.\n","                         sequence. (batch_size x seq_len x hidden_size)\n","            hidden_init: Not used in the transformer decoder\n","        Returns:\n","            output: Un-normalized scores for each token in the vocabulary, across a batch for all the decoding time steps. (batch_size x decoder_seq_len x vocab_size)\n","            attentions: The stacked attention weights applied to the encoder annotations (batch_size x encoder_seq_len x decoder_seq_len)\n","        \"\"\"\n","\n","        batch_size, seq_len = inputs.size()\n","        embed = self.embedding(inputs)  # batch_size x seq_len x hidden_size\n","\n","        embed = embed + self.positional_encodings[:seq_len]\n","\n","        encoder_attention_weights_list = []\n","        self_attention_weights_list = []\n","        contexts = embed\n","        for i in range(self.num_layers):\n","            new_contexts, self_attention_weights = self.self_attentions[i](\n","                contexts, contexts, contexts\n","            )  # batch_size x seq_len x hidden_size\n","            residual_contexts = contexts + new_contexts\n","            new_contexts, encoder_attention_weights = self.encoder_attentions[i](\n","                residual_contexts, annotations, annotations\n","            )  # batch_size x seq_len x hidden_size\n","            residual_contexts = residual_contexts + new_contexts\n","            new_contexts = self.attention_mlps[i](residual_contexts)\n","            contexts = residual_contexts + new_contexts\n","\n","            encoder_attention_weights_list.append(encoder_attention_weights)\n","            self_attention_weights_list.append(self_attention_weights)\n","\n","        output = self.out(contexts)\n","        encoder_attention_weights = torch.stack(encoder_attention_weights_list)\n","        self_attention_weights = torch.stack(self_attention_weights_list)\n","\n","        return output, (encoder_attention_weights, self_attention_weights)\n","\n","    def create_positional_encodings(self, max_seq_len=1000):\n","        \"\"\"Creates positional encodings for the inputs.\n","\n","        Arguments:\n","            max_seq_len: a number larger than the maximum string length we expect to encounter during training\n","\n","        Returns:\n","            pos_encodings: (max_seq_len, hidden_dim) Positional encodings for a sequence with length max_seq_len.\n","        \"\"\"\n","        pos_indices = torch.arange(max_seq_len)[..., None]\n","        dim_indices = torch.arange(self.hidden_size // 2)[None, ...]\n","        exponents = (2 * dim_indices).float() / (self.hidden_size)\n","        trig_args = pos_indices / (10000**exponents)\n","        sin_terms = torch.sin(trig_args)\n","        cos_terms = torch.cos(trig_args)\n","\n","        pos_encodings = torch.zeros((max_seq_len, self.hidden_size))\n","        pos_encodings[:, 0::2] = sin_terms\n","        pos_encodings[:, 1::2] = cos_terms\n","\n","        pos_encodings = pos_encodings.cuda()\n","\n","        return pos_encodings"]},{"cell_type":"markdown","metadata":{"id":"29ZjkXTNrUKb"},"source":["\n","## Step 9: Training and analysis (with scaled dot-product attention)\n","\n","Now we will train a (simplified) transformer encoder-decoder model.\n","\n","First, we train our smaller model on the small dataset. Use this model to answer Question 4 in the handout."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mk8e4KSnuZ8N","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647208200934,"user_tz":240,"elapsed":175938,"user":{"displayName":"TONGFEI ZHOU","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15125844894166682257"}},"outputId":"832a46f0-25df-4a47-e41d-3d0f5ee753d9"},"outputs":[{"output_type":"stream","name":"stdout","text":["================================================================================\n","                                      Opts                                      \n","--------------------------------------------------------------------------------\n","                         data_file_name: pig_latin_small                        \n","                                   cuda: 1                                      \n","                                nepochs: 100                                    \n","                         checkpoint_dir: checkpoints                            \n","                          learning_rate: 0.0005                                 \n","                early_stopping_patience: 100                                    \n","                               lr_decay: 0.99                                   \n","                             batch_size: 64                                     \n","                            hidden_size: 32                                     \n","                           encoder_type: transformer                            \n","                           decoder_type: transformer                            \n","                 num_transformer_layers: 4                                      \n","================================================================================\n","================================================================================\n","                                   Data Stats                                   \n","--------------------------------------------------------------------------------\n","('sheath', 'eathshay')\n","('lucy', 'ucylay')\n","('inventive', 'inventiveway')\n","('bedroom', 'edroombay')\n","('anything', 'anythingway')\n","Num unique word pairs: 3198\n","Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n","Vocab size: 29\n","================================================================================\n","Moved models to GPU!\n","Epoch:   0 | Train loss: 3.208 | Val loss: 2.414 | Gen: eeeeeeeeeeeeeeay ay i-i-way isisay oway\n","Epoch:   1 | Train loss: 2.158 | Val loss: 2.071 | Gen: eeay aradraray oway-iddddday issay oway\n","Epoch:   2 | Train loss: 1.871 | Val loss: 1.929 | Gen: eay araray onay-inay isssssay oway-ay\n","Epoch:   3 | Train loss: 1.694 | Val loss: 1.807 | Gen: eay araray inay-inay isssssay ondway\n","Epoch:   4 | Train loss: 1.556 | Val loss: 1.702 | Gen: eay araray onindionay isssssay ondway-ay\n","Epoch:   5 | Train loss: 1.455 | Val loss: 1.638 | Gen: eay ay onindidway issssay ondway\n","Epoch:   6 | Train loss: 1.378 | Val loss: 1.616 | Gen: eay arway indingay-ionay issay ondway\n","Epoch:   7 | Train loss: 1.321 | Val loss: 1.639 | Gen: ehay arway ontiongay isssay ongray\n","Epoch:   8 | Train loss: 1.263 | Val loss: 1.494 | Gen: ethay arway onindiondway isssay ondingway\n","Epoch:   9 | Train loss: 1.163 | Val loss: 1.477 | Gen: etaytetehededey arwayiway oniodiondioay isay ongringway\n","Epoch:  10 | Train loss: 1.099 | Val loss: 1.472 | Gen: ethay arway onindiondioay isway ondingway\n","Epoch:  11 | Train loss: 1.043 | Val loss: 1.414 | Gen: eththay arway ontiondioway isway ongrway\n","Epoch:  12 | Train loss: 1.016 | Val loss: 1.426 | Gen: ethay arway onindiongay isway ongringway\n","Epoch:  13 | Train loss: 0.980 | Val loss: 1.334 | Gen: eththay arway onindiongtiongay isssay ongringway\n","Epoch:  14 | Train loss: 0.930 | Val loss: 1.390 | Gen: eththay ariway oni-indiogay isway ongingway-ingway\n","Epoch:  15 | Train loss: 0.907 | Val loss: 1.407 | Gen: ethay ariway onidiongay-ionday isway oringway\n","Epoch:  16 | Train loss: 0.869 | Val loss: 1.339 | Gen: eththay arway ontindiongay isway oringway\n","Epoch:  17 | Train loss: 0.810 | Val loss: 1.266 | Gen: ethay ariway ondiongtionay isay oringway\n","Epoch:  18 | Train loss: 0.763 | Val loss: 1.262 | Gen: ethay ariway onidiongay-ionday isway oringway\n","Epoch:  19 | Train loss: 0.730 | Val loss: 1.328 | Gen: ethay ariway oniniondongay isay oringway\n","Epoch:  20 | Train loss: 0.704 | Val loss: 1.290 | Gen: ethay ariway onitiongay isay oringway\n","Epoch:  21 | Train loss: 0.676 | Val loss: 1.234 | Gen: ethay ariway onidiongtionay isisay oringway\n","Epoch:  22 | Train loss: 0.645 | Val loss: 1.279 | Gen: eththay ariway onitiondioay isiway oringway\n","Epoch:  23 | Train loss: 0.647 | Val loss: 1.295 | Gen: eththay ariway onidionationgay isay oringway\n","Epoch:  24 | Train loss: 0.616 | Val loss: 1.240 | Gen: ethay ariway ontingsay-onay isway oringway\n","Epoch:  25 | Train loss: 0.600 | Val loss: 1.224 | Gen: ethay away onindiongay iway oringway\n","Epoch:  26 | Train loss: 0.575 | Val loss: 1.286 | Gen: ethay ariway onidiongsay isway oringway\n","Epoch:  27 | Train loss: 0.596 | Val loss: 1.248 | Gen: ethay aiway onidiongay iway oway-iway\n","Epoch:  28 | Train loss: 0.591 | Val loss: 1.146 | Gen: ethay airway onindintingay isway oway-ingway\n","Epoch:  29 | Train loss: 0.536 | Val loss: 1.183 | Gen: ethay ariway onidingay-iongway isway oway-ingway\n","Epoch:  30 | Train loss: 0.514 | Val loss: 1.069 | Gen: ethay airway onindiongtinay isway oway-ingway\n","Epoch:  31 | Train loss: 0.490 | Val loss: 1.101 | Gen: ethay airway onindiongtinay isway oway-ingway\n","Epoch:  32 | Train loss: 0.462 | Val loss: 1.045 | Gen: ethay airway indoningtinay isway oway-ingway\n","Epoch:  33 | Train loss: 0.442 | Val loss: 1.055 | Gen: ethay airway onindingay isway oway-ingway\n","Epoch:  34 | Train loss: 0.419 | Val loss: 1.030 | Gen: ethay airway onindingtingay isway oway-ingway\n","Epoch:  35 | Train loss: 0.403 | Val loss: 1.056 | Gen: ethay airway onindiongtinay isway oway-ingway\n","Epoch:  36 | Train loss: 0.398 | Val loss: 1.462 | Gen: ethay away ondingtay sway oway-iwdwwway\n","Epoch:  37 | Train loss: 0.537 | Val loss: 1.070 | Gen: ethay aiway onindiongtingay isway owingrngway\n","Epoch:  38 | Train loss: 0.407 | Val loss: 1.054 | Gen: ethay ariway onindingcay isway oringway\n","Epoch:  39 | Train loss: 0.369 | Val loss: 1.017 | Gen: ethay aiway onindingcay isway oway-ingway\n","Epoch:  40 | Train loss: 0.349 | Val loss: 1.033 | Gen: ethay aiway ondiiontinggay isway oway-ingway\n","Epoch:  41 | Train loss: 0.334 | Val loss: 1.062 | Gen: ethay aiway onindingcay isway oway-ingway\n","Epoch:  42 | Train loss: 0.329 | Val loss: 1.009 | Gen: ethay airway ondiiontinggcay isway owingrngway\n","Epoch:  43 | Train loss: 0.322 | Val loss: 1.046 | Gen: ethay airay ondiiontinggay isway owingway\n","Epoch:  44 | Train loss: 0.304 | Val loss: 1.033 | Gen: ethay aiway ondiiontinggcay isway owingrngway\n","Epoch:  45 | Train loss: 0.289 | Val loss: 1.059 | Gen: ethay airway ondiiontinggcay isway owingrngway\n","Epoch:  46 | Train loss: 0.276 | Val loss: 1.046 | Gen: ethay airway ondiiontinggcay isway oringway\n","Epoch:  47 | Train loss: 0.263 | Val loss: 1.044 | Gen: ethay airway ondiiontinggcay isway oringway\n","Epoch:  48 | Train loss: 0.255 | Val loss: 1.060 | Gen: ethay airway ondiiontinggcay isway oringway\n","Epoch:  49 | Train loss: 0.246 | Val loss: 1.065 | Gen: ethay airaway ondiiontinggcay isway oringway\n","Epoch:  50 | Train loss: 0.241 | Val loss: 1.071 | Gen: ethay airway ondiitiongcay isway oringway\n","Epoch:  51 | Train loss: 0.232 | Val loss: 1.068 | Gen: ethay airway onditiongcay isway oringway\n","Epoch:  52 | Train loss: 0.226 | Val loss: 1.091 | Gen: ethway airway ondiitiongcay isway oringway\n","Epoch:  53 | Train loss: 0.218 | Val loss: 1.127 | Gen: ethay airay ondiitiongcay isway oringway\n","Epoch:  54 | Train loss: 0.221 | Val loss: 1.138 | Gen: ethay ariway ondiintiongcay isway oringway\n","Epoch:  55 | Train loss: 0.254 | Val loss: 1.404 | Gen: ethhhay iraway onditionioghay isway oringwhway\n","Epoch:  56 | Train loss: 0.434 | Val loss: 1.274 | Gen: ethay ariway oiatingtinay isay oringway\n","Epoch:  57 | Train loss: 0.448 | Val loss: 0.990 | Gen: ethay airway ondiitiongcay isway oway-ingway\n","Epoch:  58 | Train loss: 0.312 | Val loss: 0.934 | Gen: ethay airway ontiingtingcay isway oringway\n","Epoch:  59 | Train loss: 0.241 | Val loss: 0.907 | Gen: ethay airway ondiintiongcay isway oringway\n","Epoch:  60 | Train loss: 0.212 | Val loss: 0.902 | Gen: ethay airway ondiitiongcay isway oringway\n","Epoch:  61 | Train loss: 0.199 | Val loss: 0.897 | Gen: ethay airway ondiitiongcay isway orwingway\n","Epoch:  62 | Train loss: 0.186 | Val loss: 0.904 | Gen: ethay airway ondiitiongcay isway orwingway\n","Epoch:  63 | Train loss: 0.179 | Val loss: 0.910 | Gen: ethay airway ondiitiongcay isway orwingway\n","Epoch:  64 | Train loss: 0.172 | Val loss: 0.921 | Gen: ethay airway ondiitiongcay isway oringway\n","Epoch:  65 | Train loss: 0.167 | Val loss: 0.927 | Gen: ethay airway ondiitiongcay isway orwingway\n","Epoch:  66 | Train loss: 0.160 | Val loss: 0.940 | Gen: ethay airway ondiitiongcay isway orwingway\n","Epoch:  67 | Train loss: 0.155 | Val loss: 0.947 | Gen: ethay airway ondiitiongcay isway orwkingway\n","Epoch:  68 | Train loss: 0.149 | Val loss: 0.957 | Gen: ethay airway ondiitiongcay isway orwkingway\n","Epoch:  69 | Train loss: 0.146 | Val loss: 0.966 | Gen: ethay airway ondiitiongcay isway orwkingway\n","Epoch:  70 | Train loss: 0.141 | Val loss: 0.977 | Gen: ethay airway ondiitiongcay isway orkingway\n","Epoch:  71 | Train loss: 0.138 | Val loss: 0.981 | Gen: ethay airway ondiitiongcay isway orkingway\n","Epoch:  72 | Train loss: 0.132 | Val loss: 0.996 | Gen: ethway airway ondiitiongcay isway orkingway\n","Epoch:  73 | Train loss: 0.129 | Val loss: 1.003 | Gen: ethway airway ondiitiongcay isway orkingway\n","Epoch:  74 | Train loss: 0.124 | Val loss: 1.016 | Gen: ethway airway ondiitiongcay isway orkingway\n","Epoch:  75 | Train loss: 0.121 | Val loss: 1.021 | Gen: ethway airway ondiitiongcay isway orkingway\n","Epoch:  76 | Train loss: 0.118 | Val loss: 1.068 | Gen: ethay airway ondiitiongcay isway orkingway\n","Epoch:  77 | Train loss: 0.119 | Val loss: 1.022 | Gen: ethay airway ondiitiongcay isway orkingway\n","Epoch:  78 | Train loss: 0.112 | Val loss: 1.060 | Gen: ethway airway ondiitiongcay isway orkingway\n","Epoch:  79 | Train loss: 0.114 | Val loss: 1.074 | Gen: ethway airway ondiitiongcay isway orkingway\n","Epoch:  80 | Train loss: 0.113 | Val loss: 1.101 | Gen: ethway iraway ondiitiongcay isway orkingway\n","Epoch:  81 | Train loss: 0.109 | Val loss: 1.066 | Gen: ethway airway ondiitiongcay isway orkingway\n","Epoch:  82 | Train loss: 0.103 | Val loss: 1.136 | Gen: ethay airway ondiitiongcay isway orkingway\n","Epoch:  83 | Train loss: 0.134 | Val loss: 1.095 | Gen: ethay airway ondiitiongcay isway orkingway\n","Epoch:  84 | Train loss: 0.144 | Val loss: 1.181 | Gen: ethay airway ondiitiongcay isway orkingway\n","Epoch:  85 | Train loss: 0.192 | Val loss: 1.136 | Gen: ethay ariway ontidintingcay isway orkingway\n","Epoch:  86 | Train loss: 0.161 | Val loss: 1.087 | Gen: ethay airway ondiitiontiongcay isway orkingway\n","Epoch:  87 | Train loss: 0.118 | Val loss: 0.999 | Gen: ethway airway ondiitiongcay isway orkingway\n","Epoch:  88 | Train loss: 0.098 | Val loss: 0.972 | Gen: ethway airway ondiitiongcay isway orkingway\n","Epoch:  89 | Train loss: 0.090 | Val loss: 0.987 | Gen: ethway airway ondiitiongcay isway orkingway\n","Epoch:  90 | Train loss: 0.085 | Val loss: 1.019 | Gen: ethway airway ondiitiongcay isway orkingway\n","Epoch:  91 | Train loss: 0.084 | Val loss: 1.019 | Gen: ethay airway ondiitiongcay isway orkingway\n","Epoch:  92 | Train loss: 0.081 | Val loss: 1.037 | Gen: ethway airway ondiitiongcay isway orkingway\n","Epoch:  93 | Train loss: 0.077 | Val loss: 1.044 | Gen: ethway airway ondiitiongcay isway orkingway\n","Epoch:  94 | Train loss: 0.074 | Val loss: 1.048 | Gen: ethway airway ondiitiongcay isway orkingway\n","Epoch:  95 | Train loss: 0.071 | Val loss: 1.064 | Gen: ethay airway ondiitiongcay isway orkingway\n","Epoch:  96 | Train loss: 0.070 | Val loss: 1.087 | Gen: ethay airaway ondiitiongcay isway orkingway\n","Epoch:  97 | Train loss: 0.071 | Val loss: 1.092 | Gen: ethway airway ondiitiongcay isway orkingway\n","Epoch:  98 | Train loss: 0.066 | Val loss: 1.086 | Gen: ethay airway ondiitiongcay isway orkingway\n","Epoch:  99 | Train loss: 0.062 | Val loss: 1.091 | Gen: ethay airway ondiitiongcay isway orkingway\n","Obtained lowest validation loss of: 0.8972819642651648\n","source:\t\tthe air conditioning is working \n","translated:\tethay airway ondiitiongcay isway orkingway\n"]}],"source":["TEST_SENTENCE = \"the air conditioning is working\"\n","\n","trans32_args_s = AttrDict()\n","args_dict = {\n","    \"data_file_name\": \"pig_latin_small\",\n","    \"cuda\": True,\n","    \"nepochs\": 100,\n","    \"checkpoint_dir\": \"checkpoints\",\n","    \"learning_rate\": 5e-4,\n","    \"early_stopping_patience\": 100,\n","    \"lr_decay\": 0.99,\n","    \"batch_size\": 64,\n","    \"hidden_size\": 32,\n","    \"encoder_type\": \"transformer\",\n","    \"decoder_type\": \"transformer\",  # options: rnn / rnn_attention / transformer\n","    \"num_transformer_layers\": 4,\n","}\n","trans32_args_s.update(args_dict)\n","print_opts(trans32_args_s)\n","\n","trans32_encoder_s, trans32_decoder_s, trans32_losses_s = train(trans32_args_s)\n","\n","translated = translate_sentence(\n","    TEST_SENTENCE, trans32_encoder_s, trans32_decoder_s, None, trans32_args_s\n",")\n","print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l28mKuZxvaRT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647208223596,"user_tz":240,"elapsed":196,"user":{"displayName":"TONGFEI ZHOU","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15125844894166682257"}},"outputId":"6b0b56b5-37a3-4928-f221-9b9947420ab7"},"outputs":[{"output_type":"stream","name":"stdout","text":["source:\t\tthe air conditioning is working \n","translated:\tethay airway ondiitiongcay isway orkingway\n"]}],"source":["TEST_SENTENCE = \"the air conditioning is working\"\n","translated = translate_sentence(\n","    TEST_SENTENCE, trans32_encoder_s, trans32_decoder_s, None, trans32_args_s\n",")\n","print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"]},{"cell_type":"markdown","metadata":{"id":"0L8EqLYFu48H"},"source":["In the following cells, we investigate the effects of increasing model size and dataset size on the training / validation curves and generalization of the Transformer. We will increase hidden size to 64, and also increase dataset size. Include the best achieved validation loss in your report."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FdZO69DozuUu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647208414297,"user_tz":240,"elapsed":184931,"user":{"displayName":"TONGFEI ZHOU","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15125844894166682257"}},"outputId":"9183b90a-c008-4969-e6b0-a4825c831ad8"},"outputs":[{"output_type":"stream","name":"stdout","text":["================================================================================\n","                                      Opts                                      \n","--------------------------------------------------------------------------------\n","                         data_file_name: pig_latin_large                        \n","                                   cuda: 1                                      \n","                                nepochs: 100                                    \n","                         checkpoint_dir: checkpoints                            \n","                          learning_rate: 0.0005                                 \n","                early_stopping_patience: 10                                     \n","                               lr_decay: 0.99                                   \n","                             batch_size: 512                                    \n","                            hidden_size: 32                                     \n","                           encoder_type: transformer                            \n","                           decoder_type: transformer                            \n","                 num_transformer_layers: 3                                      \n","================================================================================\n","================================================================================\n","                                   Data Stats                                   \n","--------------------------------------------------------------------------------\n","('wonder', 'onderway')\n","('loyal', 'oyallay')\n","('harlem', 'arlemhay')\n","('desperate', 'esperateday')\n","('morning', 'orningmay')\n","Num unique word pairs: 22402\n","Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n","Vocab size: 29\n","================================================================================\n","Moved models to GPU!\n","Epoch:   0 | Train loss: 2.787 | Val loss: 2.294 | Gen: elay ay oay ay oay-ay-ay-ay-ay-ay-a\n","Epoch:   1 | Train loss: 2.084 | Val loss: 2.056 | Gen: etay ay oonay-ay-ay-ay-ay-na ay onday-ay-ay-ay-ay-na\n","Epoch:   2 | Train loss: 1.847 | Val loss: 1.937 | Gen: etay-ay-ay-ay aay oooay-inay ay omoway-ay-ay-ay-ay-m\n","Epoch:   3 | Train loss: 1.701 | Val loss: 1.856 | Gen: etay-ay-ay-ay ay onconday-ay-onday-on iway omoway-ay-ay-ay-iy-i\n","Epoch:   4 | Train loss: 1.583 | Val loss: 1.767 | Gen: etay-ay ay otay-onay-onay way orlay-inay-oway\n","Epoch:   5 | Train loss: 1.495 | Val loss: 1.681 | Gen: etay aray otininday issssssay orlay-nway-ay-ay-inw\n","Epoch:   6 | Train loss: 1.409 | Val loss: 1.728 | Gen: etay-ay ay oniay-onay iway onay-inway\n","Epoch:   7 | Train loss: 1.346 | Val loss: 1.572 | Gen: etay aray ondinday isway oray-inway-inway\n","Epoch:   8 | Train loss: 1.275 | Val loss: 1.615 | Gen: etay aray onioniay isay ormay-inway\n","Epoch:   9 | Train loss: 1.232 | Val loss: 1.564 | Gen: etay aray otinday isway oray-oway-inway\n","Epoch:  10 | Train loss: 1.170 | Val loss: 1.485 | Gen: etay aray oniontiay isay ormay-inway\n","Epoch:  11 | Train loss: 1.135 | Val loss: 1.566 | Gen: etay aray ondictiontionday isway orray-inway-inway-in\n","Epoch:  12 | Train loss: 1.095 | Val loss: 1.417 | Gen: etay arway ondiontiontay isway orkay-away-inway\n","Epoch:  13 | Train loss: 1.039 | Val loss: 1.448 | Gen: etay arway ondictinday isway orray-inway-inway\n","Epoch:  14 | Train loss: 0.993 | Val loss: 1.407 | Gen: etay arway ondicay-ontay isay orray-inway\n","Epoch:  15 | Train loss: 0.948 | Val loss: 1.410 | Gen: etay arway ondicay-onday isway orkay-inway-inway\n","Epoch:  16 | Train loss: 0.918 | Val loss: 1.257 | Gen: etay ariray ondicay-ontay isay orkay-inway-inway\n","Epoch:  17 | Train loss: 0.871 | Val loss: 1.213 | Gen: etay arway ondicay-onday isway orrngingrway-inway\n","Epoch:  18 | Train loss: 0.843 | Val loss: 1.230 | Gen: ethay ariraway ondicantingway isay orkangrway\n","Epoch:  19 | Train loss: 0.807 | Val loss: 1.239 | Gen: ethay ariarway ondicantinday isay orkangrrninway\n","Epoch:  20 | Train loss: 0.788 | Val loss: 1.191 | Gen: ethay ariarway ondicintingway isay orkmmmmmrnay\n","Epoch:  21 | Train loss: 0.762 | Val loss: 1.188 | Gen: ethay ariarway ondicintiontway isay orkmmgarngway\n","Epoch:  22 | Train loss: 0.724 | Val loss: 1.188 | Gen: ethay ariraway ondicintingway isay orkkgangingway\n","Epoch:  23 | Train loss: 0.693 | Val loss: 1.178 | Gen: ethay ariraway ondicintingway isay orkkgangway\n","Epoch:  24 | Train loss: 0.674 | Val loss: 1.073 | Gen: ethay ariray ondicintingway isay orkkgangway\n","Epoch:  25 | Train loss: 0.642 | Val loss: 1.033 | Gen: ethay ariray ondicintingway isway orkngingway\n","Epoch:  26 | Train loss: 0.615 | Val loss: 1.032 | Gen: ethay ariray ondicintingway isay orkkgangway\n","Epoch:  27 | Train loss: 0.604 | Val loss: 0.980 | Gen: ethay ariway ondicintiongway isway orkkgingway\n","Epoch:  28 | Train loss: 0.585 | Val loss: 1.017 | Gen: ethay ariray ondicintingway isay orkkngingway\n","Epoch:  29 | Train loss: 0.561 | Val loss: 0.951 | Gen: ethay ariway onditiongingway isway orkkngingway\n","Epoch:  30 | Train loss: 0.536 | Val loss: 0.961 | Gen: ethway ariray ondicintiongway isway orkkgingway\n","Epoch:  31 | Train loss: 0.516 | Val loss: 0.936 | Gen: ethway ariway onditicingingway isway orkkgingway\n","Epoch:  32 | Train loss: 0.500 | Val loss: 0.921 | Gen: ethway arirway onditicingway isway orkkgingway\n","Epoch:  33 | Train loss: 0.487 | Val loss: 0.914 | Gen: ethway ariway onditicingingway isway orkkngingway\n","Epoch:  34 | Train loss: 0.478 | Val loss: 0.909 | Gen: ethway ariway onditiongingway isway orkkngingway\n","Epoch:  35 | Train loss: 0.469 | Val loss: 0.886 | Gen: ethway ariway onditicingway isway orkkgingway\n","Epoch:  36 | Train loss: 0.484 | Val loss: 1.001 | Gen: ethway arirway ondictingingway isway orkkgingway\n","Epoch:  37 | Train loss: 0.513 | Val loss: 0.972 | Gen: ethay ariway onditicingway isway orkmmgrnay\n","Epoch:  38 | Train loss: 0.462 | Val loss: 0.885 | Gen: ethway arirway onditicingioway isway orkkngray\n","Epoch:  39 | Train loss: 0.426 | Val loss: 0.869 | Gen: ethway ariway onditingingway isway orkkingway\n","Epoch:  40 | Train loss: 0.405 | Val loss: 0.821 | Gen: ethway ariway onditiongingway isway orkkgingway\n","Epoch:  41 | Train loss: 0.388 | Val loss: 0.827 | Gen: ethway ariway onditiongingway isway orkkingway\n","Epoch:  42 | Train loss: 0.374 | Val loss: 0.793 | Gen: ethway ariway onditiongingway isway orkkingway\n","Epoch:  43 | Train loss: 0.367 | Val loss: 0.798 | Gen: ethway ariway onditiongingway isway orkingway\n","Epoch:  44 | Train loss: 0.369 | Val loss: 0.829 | Gen: ethehay ariway onditiongingway isway orkkmgray\n","Epoch:  45 | Train loss: 0.385 | Val loss: 0.812 | Gen: ethway ariwway onditcingingway isway orkingway\n","Epoch:  46 | Train loss: 0.371 | Val loss: 0.892 | Gen: ethway ariway onditiongiongway isway orkkngray\n","Epoch:  47 | Train loss: 0.394 | Val loss: 0.770 | Gen: ethway ariwway onditicingingway isway orkkingway\n","Epoch:  48 | Train loss: 0.338 | Val loss: 0.758 | Gen: ethway ariway onditingingcay isway orkingway\n","Epoch:  49 | Train loss: 0.323 | Val loss: 0.751 | Gen: ethway ariway onditioningcay isway orkingway\n","Epoch:  50 | Train loss: 0.312 | Val loss: 0.731 | Gen: ethway ariway onditionicingway isway orkingway\n","Epoch:  51 | Train loss: 0.306 | Val loss: 0.733 | Gen: ethway ariwway onditiongicay isway orkingway\n","Epoch:  52 | Train loss: 0.295 | Val loss: 0.711 | Gen: ethway ariway onditioniongcay isway orkingway\n","Epoch:  53 | Train loss: 0.286 | Val loss: 0.716 | Gen: ethway ariwway onditionicingway isway orkingway\n","Epoch:  54 | Train loss: 0.277 | Val loss: 0.707 | Gen: ethway ariwway onditionicingway isway orkingway\n","Epoch:  55 | Train loss: 0.270 | Val loss: 0.717 | Gen: ethway ariwway onditionicingway isway orkingway\n","Epoch:  56 | Train loss: 0.264 | Val loss: 0.707 | Gen: ethway ariwway onditionicingway isway orkingway\n","Epoch:  57 | Train loss: 0.257 | Val loss: 0.711 | Gen: ethway ariwway onditionicingway isway orkingway\n","Epoch:  58 | Train loss: 0.251 | Val loss: 0.709 | Gen: ethway airway onditionicingway isway orkingway\n","Epoch:  59 | Train loss: 0.244 | Val loss: 0.706 | Gen: ethway ariwway onditionicingway isway orkingway\n","Epoch:  60 | Train loss: 0.239 | Val loss: 0.702 | Gen: ethway airway onditioniongcay isway orkingway\n","Epoch:  61 | Train loss: 0.233 | Val loss: 0.706 | Gen: ethway airway onditioniongcay isway orkingway\n","Epoch:  62 | Train loss: 0.227 | Val loss: 0.707 | Gen: ethway airway onditioniongcay isway orkingway\n","Epoch:  63 | Train loss: 0.222 | Val loss: 0.708 | Gen: ethway airway onditioniongcay isway orkingway\n","Epoch:  64 | Train loss: 0.217 | Val loss: 0.697 | Gen: ethway airway onditioniongcay isway orkingway\n","Epoch:  65 | Train loss: 0.214 | Val loss: 0.741 | Gen: ethway airway onditioniongcay isway orkingwkway\n","Epoch:  66 | Train loss: 0.218 | Val loss: 0.736 | Gen: ethway arirway onditionicingway isway orkingwkay\n","Epoch:  67 | Train loss: 0.257 | Val loss: 0.898 | Gen: ethay ariwway onditiongingcay isway orkingrnway\n","Epoch:  68 | Train loss: 0.346 | Val loss: 0.870 | Gen: ethay arirway onditioniongcay isway orkkingway\n","Epoch:  69 | Train loss: 0.294 | Val loss: 0.691 | Gen: ethway airway onditioniongcy isway orkingway\n","Epoch:  70 | Train loss: 0.252 | Val loss: 0.656 | Gen: ethay arirway onditioniciongway isway orkingway\n","Epoch:  71 | Train loss: 0.223 | Val loss: 0.622 | Gen: ethay arirway onditioniongcay isway orkingway\n","Epoch:  72 | Train loss: 0.206 | Val loss: 0.611 | Gen: ethay arirway onditioniongcay isway orkingway\n","Epoch:  73 | Train loss: 0.189 | Val loss: 0.603 | Gen: ethay arirway onditioniongcay isway orkingway\n","Epoch:  74 | Train loss: 0.183 | Val loss: 0.597 | Gen: ethay arirway onditioniongcay isway orkingmay\n","Epoch:  75 | Train loss: 0.177 | Val loss: 0.595 | Gen: ethay airway onditioniongcay isway orkingmay\n","Epoch:  76 | Train loss: 0.172 | Val loss: 0.593 | Gen: ethay airway onditioniongcay isway orkingmay\n","Epoch:  77 | Train loss: 0.167 | Val loss: 0.591 | Gen: ethay airway onditioniongcay isway orkingmay\n","Epoch:  78 | Train loss: 0.163 | Val loss: 0.589 | Gen: ethay airway onditioniongcay isway orkingmay\n","Epoch:  79 | Train loss: 0.159 | Val loss: 0.586 | Gen: ethay airway onditioniongcay isway orkingmay\n","Epoch:  80 | Train loss: 0.155 | Val loss: 0.586 | Gen: ethay airway onditioniongcay isway orkingmay\n","Epoch:  81 | Train loss: 0.151 | Val loss: 0.583 | Gen: ethay airway onditioniongcay isway orkingway\n","Epoch:  82 | Train loss: 0.147 | Val loss: 0.583 | Gen: ethay airway onditioniongcay isway orkingway\n","Epoch:  83 | Train loss: 0.144 | Val loss: 0.581 | Gen: ethay airway onditioniongcay isway orkingway\n","Epoch:  84 | Train loss: 0.140 | Val loss: 0.576 | Gen: ethay airway onditioniongcay isway orkingway\n","Epoch:  85 | Train loss: 0.137 | Val loss: 0.573 | Gen: ethay airway onditioniongcay isway orkingway\n","Epoch:  86 | Train loss: 0.134 | Val loss: 0.570 | Gen: ethay airway onditioniongcay isway orkingway\n","Epoch:  87 | Train loss: 0.131 | Val loss: 0.564 | Gen: ethay airway onditioniongcay isway orkingway\n","Epoch:  88 | Train loss: 0.127 | Val loss: 0.565 | Gen: ethay airway onditioniongcay isway orkingway\n","Epoch:  89 | Train loss: 0.124 | Val loss: 0.576 | Gen: ethay airway onditioniongcay isway orkingway\n","Epoch:  90 | Train loss: 0.121 | Val loss: 0.580 | Gen: ethay airway onditioniongcay isway orkingway\n","Epoch:  91 | Train loss: 0.119 | Val loss: 0.575 | Gen: ethay airway onditioniongcay isway orkingway\n","Epoch:  92 | Train loss: 0.116 | Val loss: 0.568 | Gen: ethay airway onditioniongcay isway orkingway\n","Epoch:  93 | Train loss: 0.113 | Val loss: 0.571 | Gen: ethay airway onditioniongcay isway orkingway\n","Epoch:  94 | Train loss: 0.111 | Val loss: 0.563 | Gen: ethay airway onditioniongcay isway orkingway\n","Epoch:  95 | Train loss: 0.112 | Val loss: 0.658 | Gen: ethay airway onditioniongcay isway orkingnway\n","Epoch:  96 | Train loss: 0.145 | Val loss: 0.688 | Gen: ethay airway onditioniongcay isway orkingway\n","Epoch:  97 | Train loss: 0.199 | Val loss: 0.868 | Gen: ethay arway ondittiongigway isway orkningway\n","Epoch:  98 | Train loss: 0.244 | Val loss: 0.626 | Gen: ethay airway onditioningcay isway orkingway\n","Epoch:  99 | Train loss: 0.175 | Val loss: 0.581 | Gen: ethway airway onditioniongcay isway orkingmay\n","Obtained lowest validation loss of: 0.5634385246251311\n","source:\t\tthe air conditioning is working \n","translated:\tethway airway onditioniongcay isway orkingmay\n"]}],"source":["TEST_SENTENCE = \"the air conditioning is working\"\n","\n","trans32_args_l = AttrDict()\n","args_dict = {\n","    \"data_file_name\": \"pig_latin_large\",  # Increased data set size\n","    \"cuda\": True,\n","    \"nepochs\": 100,\n","    \"checkpoint_dir\": \"checkpoints\",\n","    \"learning_rate\": 5e-4,\n","    \"early_stopping_patience\": 10,\n","    \"lr_decay\": 0.99,\n","    \"batch_size\": 512,\n","    \"hidden_size\": 32,\n","    \"encoder_type\": \"transformer\",\n","    \"decoder_type\": \"transformer\",  # options: rnn / rnn_attention / transformer\n","    \"num_transformer_layers\": 3,\n","}\n","trans32_args_l.update(args_dict)\n","print_opts(trans32_args_l)\n","\n","trans32_encoder_l, trans32_decoder_l, trans32_losses_l = train(trans32_args_l)\n","\n","translated = translate_sentence(\n","    TEST_SENTENCE, trans32_encoder_l, trans32_decoder_l, None, trans32_args_l\n",")\n","print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SmoTgrDcr_dw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647208588152,"user_tz":240,"elapsed":71054,"user":{"displayName":"TONGFEI ZHOU","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15125844894166682257"}},"outputId":"47804e8e-adc1-4bf0-a65c-11b82165ae01"},"outputs":[{"output_type":"stream","name":"stdout","text":["================================================================================\n","                                      Opts                                      \n","--------------------------------------------------------------------------------\n","                         data_file_name: pig_latin_small                        \n","                                   cuda: 1                                      \n","                                nepochs: 50                                     \n","                         checkpoint_dir: checkpoints                            \n","                          learning_rate: 0.0005                                 \n","                early_stopping_patience: 20                                     \n","                               lr_decay: 0.99                                   \n","                             batch_size: 64                                     \n","                            hidden_size: 64                                     \n","                           encoder_type: transformer                            \n","                           decoder_type: transformer                            \n","                 num_transformer_layers: 3                                      \n","================================================================================\n","================================================================================\n","                                   Data Stats                                   \n","--------------------------------------------------------------------------------\n","('sheath', 'eathshay')\n","('lucy', 'ucylay')\n","('inventive', 'inventiveway')\n","('bedroom', 'edroombay')\n","('anything', 'anythingway')\n","Num unique word pairs: 3198\n","Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n","Vocab size: 29\n","================================================================================\n","Moved models to GPU!\n","Epoch:   0 | Train loss: 2.463 | Val loss: 1.960 | Gen: eray iwwway inwdway iwwwwwwwwwwwwwwwwwww inway-way\n","Epoch:   1 | Train loss: 1.689 | Val loss: 1.675 | Gen: ethay-hay iway ontintintindintdway iway onggay-iggggay-hdhay\n","Epoch:   2 | Train loss: 1.381 | Val loss: 1.506 | Gen: ethay arway ontintingdndntindway iway oray-ingray-ingray\n","Epoch:   3 | Train loss: 1.207 | Val loss: 1.454 | Gen: ethay arway ontintinnday isy oray-ingray-ingray\n","Epoch:   4 | Train loss: 1.080 | Val loss: 1.504 | Gen: ehay-hthay arway oncinndindindway isway ongray-ingway\n","Epoch:   5 | Train loss: 1.027 | Val loss: 1.402 | Gen: eay ay onday-oncay isyway oway-ingway\n","Epoch:   6 | Train loss: 0.873 | Val loss: 1.206 | Gen: ehay-hththay arway ondindddcay isway oray-inggway\n","Epoch:   7 | Train loss: 0.738 | Val loss: 1.025 | Gen: ethay arway ondingtay iway oway-ingggway\n","Epoch:   8 | Train loss: 0.625 | Val loss: 1.104 | Gen: eththay iray ondcinnngway iway orlingway\n","Epoch:   9 | Train loss: 0.548 | Val loss: 0.893 | Gen: eththay ariway onditingcay iway oway-igggway\n","Epoch:  10 | Train loss: 0.479 | Val loss: 0.915 | Gen: ehthay arway ondciongcay iway owaringway\n","Epoch:  11 | Train loss: 0.479 | Val loss: 0.900 | Gen: ethay iway ondiotiongcay iway owrway-iggway\n","Epoch:  12 | Train loss: 0.434 | Val loss: 0.841 | Gen: eththay iray onditiongcay iway orkingway\n","Epoch:  13 | Train loss: 0.346 | Val loss: 0.819 | Gen: ehtway iway ondiotingcay iway oringingway\n","Epoch:  14 | Train loss: 0.314 | Val loss: 0.787 | Gen: ehthay irway onditiongway iway orkingway\n","Epoch:  15 | Train loss: 0.323 | Val loss: 0.868 | Gen: ethay arway ondiotiongcay iway orkingway\n","Epoch:  16 | Train loss: 0.309 | Val loss: 0.931 | Gen: ethay away ondiotingcay iway orkingway\n","Epoch:  17 | Train loss: 0.264 | Val loss: 0.708 | Gen: ethay ariway onditiongcationgway iway ooringway\n","Epoch:  18 | Train loss: 0.216 | Val loss: 0.703 | Gen: ehay away onditiongcay isway oorwangway\n","Epoch:  19 | Train loss: 0.189 | Val loss: 0.766 | Gen: ehay airway onditiongionay isway ooringway\n","Epoch:  20 | Train loss: 0.204 | Val loss: 0.887 | Gen: ethay arway ondittiongcay iway orkway\n","Epoch:  21 | Train loss: 0.243 | Val loss: 0.800 | Gen: ethay ariwayway onditionginay iway orkingway\n","Epoch:  22 | Train loss: 0.187 | Val loss: 0.737 | Gen: ehay airway onditiongionay iway orkingway\n","Epoch:  23 | Train loss: 0.156 | Val loss: 0.709 | Gen: ethay irway onditiongionay isway orkingway\n","Epoch:  24 | Train loss: 0.134 | Val loss: 0.682 | Gen: ethay irway onditionginay isiway okingway\n","Epoch:  25 | Train loss: 0.128 | Val loss: 0.675 | Gen: ethay irrway onditiongionay isiway orkingway\n","Epoch:  26 | Train loss: 0.121 | Val loss: 0.675 | Gen: ethay irway onditiongcay isiway orringway\n","Epoch:  27 | Train loss: 0.104 | Val loss: 0.622 | Gen: ehthay airway onditiongcay isiway orkingway\n","Epoch:  28 | Train loss: 0.089 | Val loss: 0.630 | Gen: ehthay irway onditionginay isiway orkingway\n","Epoch:  29 | Train loss: 0.074 | Val loss: 0.660 | Gen: ethay irriway onditioniongcay isiway orkingway\n","Epoch:  30 | Train loss: 0.073 | Val loss: 0.660 | Gen: ethay irrway onditioningibncay isiway orkingway\n","Epoch:  31 | Train loss: 0.062 | Val loss: 0.644 | Gen: ethay irway onditionginay isiiiiiiway orkingway\n","Epoch:  32 | Train loss: 0.058 | Val loss: 0.646 | Gen: ethay irrway onditionginay isiway orkingway\n","Epoch:  33 | Train loss: 0.054 | Val loss: 0.690 | Gen: ethay irway onditionginay isiway orkingway\n","Epoch:  34 | Train loss: 0.056 | Val loss: 0.706 | Gen: ethay irrway onditionginay isiway orkingway\n","Epoch:  35 | Train loss: 0.047 | Val loss: 0.725 | Gen: ethay irrway onditionngiongcay isway orkingway\n","Epoch:  36 | Train loss: 0.048 | Val loss: 0.668 | Gen: ethetway irrway onditionngcay isiiway orkingway\n","Epoch:  37 | Train loss: 0.046 | Val loss: 0.651 | Gen: ethay irway onditionginay isway orkingway\n","Epoch:  38 | Train loss: 0.074 | Val loss: 0.828 | Gen: ethay irrway onditiongiondinay isiway okingwray\n","Epoch:  39 | Train loss: 0.204 | Val loss: 0.824 | Gen: ethay irway ocintioningway iway orkingway\n","Epoch:  40 | Train loss: 0.203 | Val loss: 0.585 | Gen: ethay irway onditiongiongcay isiway orkingway\n","Epoch:  41 | Train loss: 0.120 | Val loss: 0.616 | Gen: ethay irway onditiongcay isway orkingway\n","Epoch:  42 | Train loss: 0.095 | Val loss: 0.552 | Gen: ethay irway onditiongcingcay isway orkingway\n","Epoch:  43 | Train loss: 0.052 | Val loss: 0.551 | Gen: ethay irrway onditiongcay isway orkingway\n","Epoch:  44 | Train loss: 0.037 | Val loss: 0.540 | Gen: ethay irway onditiongcay isiway orkingway\n","Epoch:  45 | Train loss: 0.034 | Val loss: 0.540 | Gen: ethay irrway onditiongcay isway orkingway\n","Epoch:  46 | Train loss: 0.026 | Val loss: 0.542 | Gen: ethay irrway onditiongcay isiway orkingway\n","Epoch:  47 | Train loss: 0.023 | Val loss: 0.535 | Gen: ethay irrway onditiongcay isway orkingway\n","Epoch:  48 | Train loss: 0.019 | Val loss: 0.565 | Gen: ethay irrway onditiongcay isiway orkingway\n","Epoch:  49 | Train loss: 0.016 | Val loss: 0.573 | Gen: ethay irway onditiongcay isway orkingway\n","Obtained lowest validation loss of: 0.534538182285836\n","source:\t\tthe air conditioning is working \n","translated:\tethay irway onditiongcay isway orkingway\n"]}],"source":["TEST_SENTENCE = \"the air conditioning is working\"\n","\n","trans64_args_s = AttrDict()\n","args_dict = {\n","    \"data_file_name\": \"pig_latin_small\",\n","    \"cuda\": True,\n","    \"nepochs\": 50,\n","    \"checkpoint_dir\": \"checkpoints\",\n","    \"learning_rate\": 5e-4,\n","    \"early_stopping_patience\": 20,\n","    \"lr_decay\": 0.99,\n","    \"batch_size\": 64,\n","    \"hidden_size\": 64,  # Increased model size\n","    \"encoder_type\": \"transformer\",\n","    \"decoder_type\": \"transformer\",  # options: rnn / rnn_attention / transformer\n","    \"num_transformer_layers\": 3,\n","}\n","trans64_args_s.update(args_dict)\n","print_opts(trans64_args_s)\n","\n","trans64_encoder_s, trans64_decoder_s, trans64_losses_s = train(trans64_args_s)\n","\n","translated = translate_sentence(\n","    TEST_SENTENCE, trans64_encoder_s, trans64_decoder_s, None, trans64_args_s\n",")\n","print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dardK4RWvUWV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647208682457,"user_tz":240,"elapsed":94453,"user":{"displayName":"TONGFEI ZHOU","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15125844894166682257"}},"outputId":"3bea9415-9eec-4c25-a87e-ce9864bf0894"},"outputs":[{"output_type":"stream","name":"stdout","text":["================================================================================\n","                                      Opts                                      \n","--------------------------------------------------------------------------------\n","                         data_file_name: pig_latin_large                        \n","                                   cuda: 1                                      \n","                                nepochs: 50                                     \n","                         checkpoint_dir: checkpoints                            \n","                          learning_rate: 0.0005                                 \n","                early_stopping_patience: 20                                     \n","                               lr_decay: 0.99                                   \n","                             batch_size: 512                                    \n","                            hidden_size: 64                                     \n","                           encoder_type: transformer                            \n","                           decoder_type: transformer                            \n","                 num_transformer_layers: 3                                      \n","================================================================================\n","================================================================================\n","                                   Data Stats                                   \n","--------------------------------------------------------------------------------\n","('wonder', 'onderway')\n","('loyal', 'oyallay')\n","('harlem', 'arlemhay')\n","('desperate', 'esperateday')\n","('morning', 'orningmay')\n","Num unique word pairs: 22402\n","Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n","Vocab size: 29\n","================================================================================\n","Moved models to GPU!\n","Epoch:   0 | Train loss: 2.418 | Val loss: 2.076 | Gen: eay ay-ay-ay ontay-inay isay oray-ay-ay-ay-ay-ay-\n","Epoch:   1 | Train loss: 1.728 | Val loss: 1.803 | Gen: - araray-iay onginginay-onay-onay isay oray-ingay-ay-ay\n","Epoch:   2 | Train loss: 1.443 | Val loss: 1.767 | Gen: eay aray ondngay-ingay iway oray-ay-ay\n","Epoch:   3 | Train loss: 1.282 | Val loss: 1.664 | Gen: eay aray otinway-onay-onay isay oway-ay-ay\n","Epoch:   4 | Train loss: 1.223 | Val loss: 1.499 | Gen: eway-ay-ay aray-iway otinway-onay-onay isay-iway oray-ingay-inay\n","Epoch:   5 | Train loss: 1.045 | Val loss: 1.519 | Gen: eway-ay aray ondioionginay-ingina isay oray-ingingway\n","Epoch:   6 | Train loss: 0.961 | Val loss: 1.289 | Gen: eway-ay ariway ondintinway isway oraway-ingdway\n","Epoch:   7 | Train loss: 0.844 | Val loss: 1.242 | Gen: ethway-eth arway ondidgintingay isay oray\n","Epoch:   8 | Train loss: 0.766 | Val loss: 1.125 | Gen: ethay arway ondway-ontingingway isay orkray-igdgray\n","Epoch:   9 | Train loss: 0.690 | Val loss: 1.352 | Gen: ethway arway ondwititingtintway isay orkrngingnway\n","Epoch:  10 | Train loss: 0.640 | Val loss: 1.060 | Gen: tay-ay arway ondidgionginay isay orkray-ingway\n","Epoch:  11 | Train loss: 0.562 | Val loss: 1.006 | Gen: ethay-ay arway onditiongingnay isay orkingngnway\n","Epoch:  12 | Train loss: 0.515 | Val loss: 1.083 | Gen: ethay arway onddidgionginay isay orkingdway\n","Epoch:  13 | Train loss: 0.461 | Val loss: 0.920 | Gen: ethay ariway ondititiongway isay orkingnway\n","Epoch:  14 | Train loss: 0.406 | Val loss: 0.841 | Gen: tehay arway ondnitiongday isay orkingnway\n","Epoch:  15 | Train loss: 0.353 | Val loss: 0.821 | Gen: ethway arway onditiondginay isway orkingnway\n","Epoch:  16 | Train loss: 0.320 | Val loss: 0.917 | Gen: ethay airway ondditioniongday isay orkingnway\n","Epoch:  17 | Train loss: 0.320 | Val loss: 0.722 | Gen: ethway airway onditiondcay isway orkingnway\n","Epoch:  18 | Train loss: 0.266 | Val loss: 0.750 | Gen: tehway airway ondidioninginway isway orkingnway\n","Epoch:  19 | Train loss: 0.269 | Val loss: 0.819 | Gen: ethway airway ondidioningcay isway orkingwway\n","Epoch:  20 | Train loss: 0.275 | Val loss: 1.099 | Gen: ethay iraway onditioningway isway orkingnway\n","Epoch:  21 | Train loss: 0.377 | Val loss: 1.045 | Gen: ethway arwaywaywaywaywayway onndioiongindnay isway orkingnwrwnwkway\n","Epoch:  22 | Train loss: 0.294 | Val loss: 0.704 | Gen: ethay airway onditionionway isway orkingway\n","Epoch:  23 | Train loss: 0.219 | Val loss: 0.678 | Gen: ethway airway onditioningcay isway orkingnway\n","Epoch:  24 | Train loss: 0.183 | Val loss: 0.495 | Gen: ethay airway onditioningcay isway orkingway\n","Epoch:  25 | Train loss: 0.149 | Val loss: 0.464 | Gen: ethay airway onditioningcay isway orkingway\n","Epoch:  26 | Train loss: 0.129 | Val loss: 0.462 | Gen: ethway airway onditioningcay isway orkingway\n","Epoch:  27 | Train loss: 0.124 | Val loss: 0.450 | Gen: ethay airway onditioningcay isway orkingway\n","Epoch:  28 | Train loss: 0.109 | Val loss: 0.499 | Gen: ethway ariway onditioningcay isway orkingway\n","Epoch:  29 | Train loss: 0.116 | Val loss: 0.647 | Gen: ethay arway onditioningcay isway orkingway\n","Epoch:  30 | Train loss: 0.137 | Val loss: 0.521 | Gen: ethay airway onditioningcay isway orkingway\n","Epoch:  31 | Train loss: 0.115 | Val loss: 0.460 | Gen: ethay airway onditioningcay isway orkingway\n","Epoch:  32 | Train loss: 0.093 | Val loss: 0.453 | Gen: ethay airway onditionningcay isway orkingway\n","Epoch:  33 | Train loss: 0.083 | Val loss: 0.440 | Gen: ethay airway onditioningcay isway orkingway\n","Epoch:  34 | Train loss: 0.106 | Val loss: 0.970 | Gen: ethay airway ondidoinginway isway orkingway\n","Epoch:  35 | Train loss: 0.265 | Val loss: 0.707 | Gen: ethay airway onditininginway isway orkingway\n","Epoch:  36 | Train loss: 0.185 | Val loss: 0.717 | Gen: ethay airway ondititingcay isway orkingway\n","Epoch:  37 | Train loss: 0.134 | Val loss: 0.533 | Gen: ethay airway onditionioncrway isway orkingway\n","Epoch:  38 | Train loss: 0.117 | Val loss: 0.599 | Gen: etway airway onditiongingcay isway orokingway\n","Epoch:  39 | Train loss: 0.138 | Val loss: 0.922 | Gen: ethay airway onoditoniongcay isw orogingway\n","Epoch:  40 | Train loss: 0.141 | Val loss: 0.631 | Gen: ethay airway onditioningcay isway orkingway\n","Epoch:  41 | Train loss: 0.144 | Val loss: 0.621 | Gen: ethay arirway onditingiongcay isway orkingway\n","Epoch:  42 | Train loss: 0.110 | Val loss: 0.493 | Gen: ethay iirway onditioningcay isway orokingway\n","Epoch:  43 | Train loss: 0.073 | Val loss: 0.342 | Gen: ethay airway onditioningcay isway orkingway\n","Epoch:  44 | Train loss: 0.055 | Val loss: 0.333 | Gen: ethay airway onditionniongcay isway orrkingway\n","Epoch:  45 | Train loss: 0.051 | Val loss: 0.290 | Gen: ethay airway onditionningcay isway orkingway\n","Epoch:  46 | Train loss: 0.042 | Val loss: 0.324 | Gen: ethay airway onditionningcay isway orrkingway\n","Epoch:  47 | Train loss: 0.039 | Val loss: 0.277 | Gen: ethay airway onditionningcay isway orkingway\n","Epoch:  48 | Train loss: 0.032 | Val loss: 0.313 | Gen: ethay airway onditionningcay isway orkingway\n","Epoch:  49 | Train loss: 0.029 | Val loss: 0.280 | Gen: ethay airway onditioningcay isway orkingway\n","Obtained lowest validation loss of: 0.27657217867149764\n","source:\t\tthe air conditioning is working \n","translated:\tethay airway onditioningcay isway orkingway\n"]}],"source":["TEST_SENTENCE = \"the air conditioning is working\"\n","\n","trans64_args_l = AttrDict()\n","args_dict = {\n","    \"data_file_name\": \"pig_latin_large\",  # Increased data set size\n","    \"cuda\": True,\n","    \"nepochs\": 50,\n","    \"checkpoint_dir\": \"checkpoints\",\n","    \"learning_rate\": 5e-4,\n","    \"early_stopping_patience\": 20,\n","    \"lr_decay\": 0.99,\n","    \"batch_size\": 512,\n","    \"hidden_size\": 64,  # Increased model size\n","    \"encoder_type\": \"transformer\",\n","    \"decoder_type\": \"transformer\",  # options: rnn / rnn_attention / transformer\n","    \"num_transformer_layers\": 3,\n","}\n","trans64_args_l.update(args_dict)\n","print_opts(trans64_args_l)\n","\n","trans64_encoder_l, trans64_decoder_l, trans64_losses_l = train(trans64_args_l)\n","\n","translated = translate_sentence(\n","    TEST_SENTENCE, trans64_encoder_l, trans64_decoder_l, None, trans64_args_l\n",")\n","print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"]},{"cell_type":"markdown","metadata":{"id":"pSSyiG39vVlN"},"source":["The following cell generates two loss plots. In the first plot, we compare the effects of increasing dataset size. In the second plot, we compare the effects of increasing model size. Include both plots in your report, and include your analysis of the results."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-Ql0pxrEvVP6","colab":{"base_uri":"https://localhost:8080/","height":53},"executionInfo":{"status":"ok","timestamp":1647209871149,"user_tz":240,"elapsed":1170,"user":{"displayName":"TONGFEI ZHOU","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15125844894166682257"}},"outputId":"eaf8b9b7-b074-4a03-9982-18a7e7ed8d3a"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 0 Axes>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 0 Axes>"]},"metadata":{}}],"source":["save_loss_comparison_by_dataset(\n","    trans32_losses_s,\n","    trans32_losses_l,\n","    trans64_losses_s,\n","    trans64_losses_l,\n","    trans32_args_s,\n","    trans32_args_l,\n","    trans64_args_s,\n","    trans64_args_l,\n","    \"trans_by_dataset\",\n",")\n","save_loss_comparison_by_hidden(\n","    trans32_losses_s,\n","    trans32_losses_l,\n","    trans64_losses_s,\n","    trans64_losses_l,\n","    trans32_args_s,\n","    trans32_args_l,\n","    trans64_args_s,\n","    trans64_args_l,\n","    \"trans_by_hidden\",\n",")"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["TjPTaRB4mpCd","9DaTdRNuUra7","4BIpGwANoQOg","pbvpn4MaV0I1","0yh08KhgnA30"],"name":"Copy of nmt.ipynb","provenance":[{"file_id":"https://github.com/uoft-csc413/2022/blob/master/assets/assignments/nmt.ipynb","timestamp":1646969667644}],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}